\documentclass[12pt,letterpaper]{report}

\usepackage{amsmath,amssymb,bm,bbm,upgreek,mathrsfs}
\usepackage{algorithmic,algorithm}
\usepackage{graphicx,caption,sidecap,subcaption}
\usepackage{setspace}
\usepackage{color}
\usepackage{multirow}
\usepackage{etoolbox}
\usepackage{fullpage}
\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

\usepackage[round,authoryear]{natbib}

% \doublespacing

% \captionsetup{subrefformat=parens}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Useful math macros %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% argmax and argmin
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%% Distributions
\newcommand{\Norm}[2]{\mathcal{N}(#1,#2)}
\newcommand{\Unif}{\mathcal{U}}
\newcommand{\Pois}[1]{\mathcal{P}\mathrm{ois}(#1)}
\newcommand{\Exp}[1]{\mathcal{E}\mathrm{xp}(#1)}
\newcommand{\Gamm}[2]{\mathcal{G}(#1,#2)}
\newcommand{\Bern}[1]{\mathcal{B}\mathrm{ern}\left(#1\right)}
\newcommand{\Binom}[2]{\mathcal{B}\mathrm{inom}(#1,#2)}
\newcommand{\Geom}[1]{\mathcal{G}\mathrm{eom}(#1)}
\newcommand{\Cat}{\mathcal{C}\mathrm{at}}
\newcommand{\Beta}[2]{\mathcal{B}\mathrm{eta}(#1,#2)}
\newcommand{\Lapl}{\mathcal{L}\mathrm{aplace}}
\newcommand{\GP}{\mathcal{GP}}
\newcommand{\DP}{\mathcal{DP}}
\newcommand{\CRP}{\mathsf{CRP}}

%% Probability
\newcommand{\E}[1]{\mathbb{E}[#1]}
\newcommand{\Cov}[2]{\mathbb{C}\mathrm{ov}(#1,#2)}
\newcommand{\given}{\, \vert \,}

%% General
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}
\newcommand{\norm}[1]{\left\vert \left \vert #1 \right\vert \right\vert}

%% Vectors
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bem}{\mathbf{m}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bth}{\bm\uptheta}
\newcommand{\bTh}{\bm\Theta}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bmu}{\boldsymbol{\mu}}

%% Sets
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cM}{\mathcal{M}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Modeling and Unsupervised Learning of Structured Similarity Among
  Source Contexts in Hierarchical Bayesian Infinite Mixture Models}
\subtitle{With Two Applications to Modeling Natural Language Semantics}
\author{Colin Reimer Dawson}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
In a classical mixture modeling, each data point is modeled as arising
i.i.d. (typically) from a weighted sum of probability distributions,
where both the weights and the parameters of the mixture components
are targets of inference.  When data arises from different sources
that may not give rise to the same mixture distribution, a
hierarchical model can allow the source contexts to share components while
assigning different weights across them (while perhaps coupling the
weights to ``borrow strength'' across contexts).  The Dirichlet
Process (DP) Mixture Model (e.g., Rasmussen, 2000) is a Bayesian
approach to mixture modeling which models the data as arising from a countably
infinite number of components: the Dirichlet Process provides a prior
on the mixture weights that guards against overfitting.  The Hierarchical 
Dirichlet Process (HDP) Mixture Model (Teh, et al. 2006) employs
a separate DP Mixture Model for each context, but
couples the weights across contexts by using a common base measure which is
itself drawn from a top-level DP.  This coupling is critical to ensure
that mixture components are reused across contexts.
For example, in natural language topic modeling, a common application
domain for mixture models, the components represent semantic topics,
and the contexts are documents, and it is critical that topics be reused
across documents.

These models have been widely adopted in Bayesian statistics and
machine learning.  However, a limitation of DPs is that the atoms
are {\it a priori} exchangeable, and in the case of HDPs, the
component weights are independent conditioned on the top-level
measure.  This is unrealistic in many applications, including topic modeling, where certain
components (e.g., topics) are expected to correlate across contexts
(e.g., documents).  In the case of topic modeling, the Discrete
Infinite Logistic Normal model (DILN; Paisley et al., 2011) addresses
this shortcoming by associating with each mixture component a latent
location in an abstract metric, and rescaling each context-specific
set of weights, initially drawn from an HDP, 
by an exponentiated draw from a Gaussian Process (GP), so that
components which are nearby in space tend to have their weights be
scaled up or down together.  However, inference in this model requires
the posterior distribution to be approximated by a variational family,
as MCMC sampling from the exact posterior was deemed intractable.
Thus, one goal of this dissertation is the development of simple MCMC
algorithms for HDP models with correlated components.

A second application of HDPs is to time series
models, in particular Hidden Markov Models (HMMs), where the HDP can be used
as a prior on a doubly infinite transition matrix for the latent Markov
chain, giving rise to the HDP-HMM (first developed, as the ``Infinite
HMM'', by Beal et al., 2001, and subsequently shown to be a case of an HDP by Teh et al, 2006).  
There, the hierarchy is over rows of the transition matrix,
and the distributions across rows are coupled through a top-level
Dirichlet Process.  The sequential nature of the problem introduces two
added wrinkles, namely that: the contexts themselves are random (since
the context when generating state $t$ is the state at time $t-1$),
and the set of contexts is the same as the set of components.  Hence,
not only might the components be correlated with each other via
locations in some latent space, but we might expect that contexts that
correspond to correlated components will overall have similar
distributions.  

In the first part of the dissertation, I will present a formal overview of
Dirichlet Processes and their various representations, as well as
associated schemes for tackling the problem of doing approximate inference over an
infinitely flexible model with finite computational resources.  I will
then turn to the Hierarchical Dirichlet Process, and review the literature on
modeling correlations between components.

Next, I will present a novel probabilistic model, which I call the
Hierarchical Dirichlet Process Hidden Markov Model With Local
Transitions, which achieves the goal of simultaneously modeling
correlations between contexts and components by assigning each
a location in a metric space and promoting transitions between states
that are near each other.  I present a Gibbs sampling scheme
for inference in this model, employing an augmented data
representation to simplify the relevant conditional distributions.  I
give a intuitive interpretation of the augmented representation by
casting the discrete time chain as a continuous time chain in which
durations are not observed, and in which some jump attempts fail and
are never observed.  By tying the success probability of a jump
between two states to the distance between them, the first successful
(and therefore observed) jump is more likely to be to a nearby state.
I refer to this representation as a Markov Process With Failed Jump
Attempts.  I test this model on both synthetic and real data,
including a natural language data set drawn from a corpus of
biological research articles, in which the goal is inferences about
the semantic scope of assertions about biological processes implicated
in cancer (to, e.g., species, organ sites, gene variants, etc.).
There, the latent states are sets of entities in the scope, and the
data is raw text.  It is presumed that succesive assertions in a paper
apply in similar scopes.

Finally, I present a generative model of natural language phrase
structure where the problem is estimation of context-dependent
distributions of parse tree symbols, and using that family of
distributions to infer, from novel sentences, (1) the best syntactic
parse tree, and (2) the semantic context that produced the sentence.  There, ``context''
consists of a combination of the surrounding linguistic elements as
well as the semantic context.  The chief challenge stems from the huge
number of possible contexts, and thus a principled method for tying
the contexts based on similarity is needed.  I present two approaches,
the first based on a multilevel HDP model, where contexts are
hierarchically nested based on shared features, and the second based
on an adaptation of a generalization of the Dirichlet Process known as the Distance
Dependent Chinese Restaurant Process (ddCRP; Blei et al., 2011) to the
problem.  Here, rather than nesting the contexts, which requires a
predetermined order of precedence among context features, 
I assign to each context an uncountably infinite mixture
of principal ``topic'' components, where the mixing weights are unique
per context, but are {\it a priori} similar across similar
contexts.  This is achieved by introducing latent cluster assignment
variables, where affiliation to a cluster is biased by similarity to
its other members.  I also present a prior, likelihood, and inference algorithm
to learn a sparse, cluster-specific similarity function.  I test the model and
inference algorithms on a corpus of captioned scenes, where the scenes
provide a space of possible semantic contexts for the captions.

\end{abstract}


\chapter{A Hierarchical Dirichlet Process Hidden Markov Model With ``Local'' Transitions (HDP-HMM-LT)}
 I describe a generalization of the Hierarchical Dirichlet Process
Hidden Markov Model (HDP-HMM; Teh, et al. 2006)
which introduces a notion of latent similarity between pairs of hidden
states, such that transitions are a priori more likely to occur between states
with similar emission distributions.  This is achieved by
placing a similarity kernel on the space of state parameters, and
scaling transition probabilities by the similarity between states.  
I refer to this model as the Hierarchical Dirichlet Process Hidden Markov Model with Local
Transitions (HDP-HMM-LT).  Although this achieves the goal of selectively increasing the
probability of transitions between similar states, inference is made
more complicated since the posterior measure over transition
distributions is no longer a Dirichlet Process, due to the
heterogenous scale parameters of the Gamma distributed unnormalized
weights.  I present an alternative representation of this process that facilitates
inference by casting the discrete time chain as a continuous time Markov Process
in which: (1) some jump attempts fail, (2) the probability of success is
proportional to the similarity between the source and destination
states, (3) only successful jumps are observed,
and (4) the time elapsed between jumps, as well as the number of
unsuccessful jump attempts, are latent variables that are sampled
during MCMC inference.  By introducing these auxiliary latent variables, all
conditional distributions in the model are members of an exponential
family, admitting exact Gibbs sampling,
with the exception of the parameters of the similarity kernel.
The choice of similarity kernel is application-specific, but I present
results for an exponential (a.k.a. Laplacian) similarity kernel with a single decay parameter
whose conditional posterior density is log-concave, and hence admits
Adaptive Rejection Sampling (Gilks and Wild, 1992).

The motivating domain for this model is natural language text, in
which sentences in a document are arranged in such a way that
the sets of relevant entities in successive sentences have a high
degree of overlap, even when they are not identical.  The goal is to model the
entity set in a sentence using a binary vector, indicating which
entities are present in the context, and to constrain the dynamics 
governing latent state transitions so that
transitions between similar entity sets are {\it a priori} more
likely, but where the presence or absence of an entity depends on
the state of multiple entities in the previous sentence.  The latter
property makes an ordinary factorial HMM undesireable.  

\section{Transition Dynamics in the HDP-HMM}
\label{sec:transition-dynamics}

The conventional HDP-HMM (Teh et al. 2006) is based on a 
Hierarchical Dirichlet Process defined as follows:

Each of a countably infinite set of states, indexed by $j$, receives a
location $\theta_j$ in emission parameter space, $\Omega$, according to 
base measure $H$.  A top-level
weight distribution, $\bbeta$, is drawn from a stick-breaking
process with parameter $\gamma > 0$, so that state $j$ has overall
weight $\beta_j$, and emission distribution parameterized by $\theta_j$.
\begin{align}
\theta_j &\stackrel{i.i.d.}{\sim} H \\
\bbeta &\sim GEM(\gamma)
\end{align}

The actual transition distribution from state $j$, denoted by $\bpi_j$
is then drawn from a DP with concentration $\alpha$ and base measure $\bbeta$:
\begin{equation}
  \label{eq:1}
  \bpi_j \stackrel{i.i.d}{\sim} DP(\alpha \bbeta) \qquad j = 1, 2, \dots
\end{equation}

The hidden state sequence is then generated according to the $\pi_j$.
Let $z_t$ be the index of the chain's state at time $t$.  Then we have
\begin{equation}
  \label{eq:4}
  z_t \given z_{t-1}, \bpi_{z_{t-1}} \sim \bpi_{z_{t-1}} \qquad t = 1, 2, \dots, T
\end{equation}
where $T$ is the length of the data sequence.

Finally, the emission distribution for state $j$ is a function of
$\theta_j$, so that we have
\begin{equation}
  \label{eq:5}
  y_t \given z_{t}, \theta_{z_t} \sim F(\theta_{z_t})
\end{equation}

A shortcoming of this model is that the generative process does not
take into account the fact that the set of source states is the same
as the set of destination states: that is, the distribution $\bpi_j$
has an element which corresponds to state $j$.  Put another way, there
is no special treatment of the diagonal of the transition matrix, so
that self-transitions are no more likely {\it a priori} than
transitions to any other state.  The Sticky HDP-HMM of Fox, et
al. (2008) addresses this issue by adding an extra mass of $\kappa$ at location $j$ to the base
measure of the DP that generates $\bpi_j$.  That is, they replace
\eqref{eq:1} with
\begin{equation}
  \label{eq:6}
  \bpi_j \sim DP(\alpha\bbeta + \kappa \delta_j).
\end{equation}
An alternative model is presented by Johnson et al. (2013), wherein 
state duration distributions are modeled
separately, and ordinary self-transitions are ruled out.  In both of
these models, auxiliary latent variables are introduced to simplify
conditional posterior distributions and facilitate Gibbs sampling.
However, while both of these models have the useful property that
self-transitions are treated as ``special'', they contain no notion of
similarity for pairs of states that are not identical: 
in both cases, when the transition matrix
is integrated out, the prior probability of
transitioning to state $j'$ depends only on the top-level stick
weight associated with state $j'$, and not on the identity or
parameters of the previous state $j$.

\section{An HDP-HMM With Local Transitions}

The goal is to add to the transition model the concept of a transition to
a ``nearby'' state, where nearness of $j$ and $j'$ is possibly a function of
$\theta_j$ and $\theta_{j'}$.  In order to accomplish this, we first
consider an alternative construction of the transition distributions,
based on the Normalized Gamma Process representation of the Dirichlet
Process (Ferguson, 1973).

\subsection{A Normalized Gamma Process representation of the HDP-HMM}
\label{sec:normalized-gamma}

Define a random measure, $\mu = \sum_{j=1}^{\infty} \pi_j \delta_{\theta_j}$, where 
\begin{align}
  \pi_j &\stackrel{ind}{\sim} \Gamm{w_j}{1} \label{eq:17}\\
  T &= \sum_{j=1}^{\infty} \pi_j \label{eq:18}\\
  \tilde{\pi}_j &= \frac{\pi_j}{T}   \label{eq:16}\\
  \theta_j &\stackrel{i.i.d}{\sim} H \label{eq:19}
\end{align}
and subject to the constraint that $\sum_{j\geq 1} w_j < \infty$,
which ensures that $T < \infty$ almost surely.  As
shown by Paisley et al. (2011), for fixed $\{w_j\}$ and $\{\theta_j\}$, $\mu$ is distributed as a Dirichlet
Process with base measure $\bw = \sum_{j=1}^{\infty} w_j \delta_{\theta_j}$.
If we draw $\bbeta$ from a stick-breaking process and then draw a
series $\{\mu_m\}_{m=1}^M$ of
i.i.d. random measures from the above process, setting $\bw =
\alpha\bbeta$ for some $\alpha > 0$, then
this defines a Hierarchical Dirichlet Process.  If, moreover, there is
one $\mu_m$ associated with every state $j$, then we obtain the
HDP-HMM.

We can thus write
\begin{align}
  \bbeta &\sim \mathsf{GEM}(\gamma)   \label{eq:20} \\
  \theta_j &\stackrel{i.i.d.}{\sim} H \label{eq:21}\\
  \pi_{jj'} &\stackrel{ind}{\sim} \Gamm{\alpha \beta_{j'}}{1} \label{eq:22}\\
  T_j &= \sum_{j'=1}^{\infty} \pi_{jj'} \\
  \tilde{\pi}_{jj'} &= \frac{\pi_{jj'}}{T_j} \label{eq:23},
\end{align}
where $\gamma$ and $\alpha$ are prior concentration hyperparameters
for the two DP levels, where
\begin{align}
  \label{eq:50}
  p(z_t \given z_{t-1}, \bpi) = \tilde{\pi}_{z_{t-1}z_t}
\end{align}
and the observed data
$\{y_t\}_{t\geq 1}$ distributed as
\begin{equation}
  \label{eq:24}
  y_t \given z_t \stackrel{ind}{\sim} F(\theta_{z_t})
\end{equation}
for some family, $F$ of probability measures indexed by values of $\theta$.

\subsection{Promoting ``Local" Transitions}
\label{sec:prom-local-trans}

In the preceding formulation, the $\theta_j$ and the $\pi_{jj'}$ are independent
conditioned on the top-level measure.  Our goal is to relax this
assumption, in order to allow for prior knowledge
that certain ``locations'', $\theta_j$, are more likely than others to
produce large weights.  This can be accomplished by letting the rate
parameter in the distribution of the $\pi_{jj'}$
be a function of $\theta_j$ and $\theta_{j'}$.  
Let $\Phi: \Omega \times \Omega \to [0,\infty)$ represent a
``similarity function'', and define a collection of random variables
$\{\phi_{jj'}\}_{j,j' \geq 1}$ according to
\begin{equation}
  \phi_{jj'} = \phi(\theta_j, \theta_j')
\end{equation}
We can then generalize \eqref{eq:20}-\eqref{eq:23} to
\begin{align}
  \bbeta &\sim \mathrm{GEM}(\gamma) \\
  \theta_j &\stackrel{i.i.d}{\sim} H \\
  \pi_{jj'} \given \bbeta, \btheta &\sim \Gamm{\alpha \beta_{j'}}{\phi_{jj'}^{-1}} \\
  T_j &= \sum_{j'=1}^{\infty} \pi_{jj'} \\
  \tilde{\pi}_{jj'} &= \frac{\pi_{jj'}}{T_j}
\end{align}
so that the expected value of $\pi_{jj'}$ is
$\alpha\beta_{j'}\phi_{jj'}$.  Since a similarity between one object
and another should not exceed the similarity between an object and
itself, we will assume that $\phi_{jj'} \leq B < \infty$ for all $j$
and $j'$, with equality holding iff $j = j'$.  Moreover, there 
is no loss of generality by taking $B = 1$, since a constant rescaling of
$\phi_{jj'}$ gets absorbed in the normalization.

The above model is equivalent to simply drawing the $\pi_{jj'}$ as in
\eqref{eq:20} and scaling each one by $\phi_{jj'}$ prior to
normalization.

Unfortunately, this formulation complicates inference significantly,
as the introduction of non-constant rate parameters to the prior on
$\bpi$ destroys the conjugacy between $\bpi$ and $\bz$, and worse, the
conditional likelihood function for $\bpi$ contains an infinite
sum of the elements in a row, rendering all entries within a row
mutually dependent.

\subsection{The HDP-HMM-LT as a continuous-time 
Markov Jump Process with ``failed'' jumps}
\label{sec:dist-based-filt}

We can gain stronger intuition, as well as simplify posterior
inference, by re-casting the HDP-HMM-LT described in the last section
as a continuous time Markov Jump Process where some of the attempts to jump
from one state to another fail, and where the failure probability
increases as a function of the ``distance'' between the states.

Let $\Phi$ be defined as in the last section, and let 
$\bbeta$, $\btheta$ and $\bpi$ be defined as in the Normalized Gamma
Process representation of the ordinary HDP-HMM.  That is,
\begin{align}
  \label{eq:beta} \bbeta &\sim \mathrm{GEM}(\gamma) \\
  \theta_j &\stackrel{i.i.d}{\sim} H \\
  \pi_{jj'} \given \bbeta, \btheta &\sim \Gamm{\alpha \beta_{j'}}{1}
\end{align}
Now suppose that when the process is in state $j$, jumps to state
$j'$ are made at rate $\pi_{jj'}$.  This defines a continuous-time
Markov Process where the off-diagonal elements of the transition rate
matrix are the off diagonal elements of $\bpi$.  In addition,
self-jumps are allowed, and occur with rate $\pi_{jj}$.   If we only
observe the jumps and not the durations between jumps, this is an
ordinary Markov chain, whose transition matrix is obtained by
appropriately normalizing $\bpi$.  If we do not observe the jumps themselves, but
instead an observation is generated once per jump from a distribution that depends
on the state being jumped to, then we have an ordinary HMM.

I modify this process as follows.  
Suppose that each jump attempt from state $j$ to state $j'$ has a
chance of failing, which is an increasing function of the ``distance''
between the states.  In particular, let the success probability be
$\phi_{jj'}$ (recall that we assumed above that $0 \leq \phi_{jj'}
\leq 1$ for all $j,j'$).  Then, the rate of successful jumps from $j$
to $j'$ is $\pi_{jj'}\phi_{jj'}$, and the corresponding rate of unsuccessful jump
attempts is $\pi_{jj'}(1-\phi_{jj'})$.  To see this, denote by
$N_{jj'}$ the total number of jump attempts to $j'$ in a unit
interval of time spent in state $j$.  Since we are assuming the
process is Markovian, the total number of attempts is $\Pois{\pi_{jj'}}$
distributed.  Conditioned on $N_{jj'}$, $n_{jj'}$ will be successful, where
\begin{equation}
  \label{eq:51}
  n_{jj'} \given N_{jj'} \sim \Binom{N_{jj'}}{\phi_{jj'}}
\end{equation}
It is easy to show (and well known) that the marginal distribution of
$n_{jj'}$ is $\Pois{\pi_{jj'}\phi_{jj'}}$, and the marginal
distribution of $\tilde{q}_{jj'} := N_{jj'} - n_{jj'}$ is
$\Pois{\pi_{jj'}(1-\phi_{jj'})}$.  The rate of successful jumps
from state $j$ overall is then $T_j := \sum_{j'} \pi_{jj'} \phi_{jj'}$.

Let $t$ index jumps, so that $z_t$ indicates the $t$th state visited
by the process (couting self-jumps as a new time step).  Given
that the process is in state $j$ at discretized time $t-1$ (that is,
$z_{t-1} = j$), it is a standard property of Markov Processes that 
the probability that the first successful jump is to state $j'$ (that is, $z_{t} = j'$) 
is proportional to the rate of successful attempts to 
$j'$, which is $\pi_{jj'}\phi_{jj'}$.  

Let $\tau_{t}$ indicate the time elapsed between the $t$th and 
and $t-1$th successful jump (where we assume that the first
observation occurs when the first successful jump from a distinguished initial
state is made).  We have
\begin{equation}
  \label{eq:52}
  \tau_t \given z_{t-1} \sim \Exp{T_{z_{t-1}}}
\end{equation}
where $\tau_t$ is independent of $z_{t}$.

During this period, there will be $\tilde{q}_{j't}$ unsuccessful attempts to
jump to state $j'$, where
\begin{equation}
  \label{eq:53}
  \tilde{q}_{j't} \given z_{t-1} \sim \Pois{\tau_t \pi_{z_{t-1}j'}(1-\phi_{z_{t-1}j'})}
\end{equation}

Define the following additional variables
\begin{align}
  \label{eq:56}
    \mathcal{T}_j &= \{t \given z_{t-1} = j\} \\
    q_{jj'} &= \sum_{t \in \mathcal{T}_j}
    \tilde{q}_{j't} \\
    u_j &= \sum_{t \in \mathcal{T}_j} \tau_t 
\end{align}
and let $\bQ = (q_{jj'})_{j,j' \geq 1}$ be the matrix of unsuccessful
jump attempt counts, and $\bu = (u_j)_{j \geq 1}$ be the vector of
the total times spent in each state.

Since each of the $\tau_t$ with $t \in \mathcal{T}_j$ are
i.i.d. $\Exp{T_j}$, we get the marginal distribution
\begin{equation}
u_j \given \bz, \bpi \btheta \stackrel{ind}{\sim} \Gamm{n_{j\cdot}}{T_j}
\end{equation}
by the standard property that sums of i.i.d. Exponential distributions
has a Gamma distribution with shape equal to the number of variates in
the sum, and rate equal to the rate of the individual exponentials.  
Moreover, since the $\tilde{q}_{j't}$ with $t \in \mathcal{T}_j$ 
are Poisson distributed, the total number of failed
attempts in the total duration $u_j$ is
\begin{equation}
  \label{eq:60}
  q_{jj'} \stackrel{ind}{\sim} \Pois{u_j\pi_{jj'}(1-\phi_{jj'})}.
\end{equation}

Thus if we marginalize out the individual $\tau_t$ and
$\tilde{q}_{j't}$, we have a joint distribution
over $\bz$, $\bu$, and $\bQ$, conditioned on the transition rate
matrix $\bpi$ and the success probability matrix $\bphi$, which is
\begin{align}
  \label{eq:54}
  p(\bz, \bu, \bQ \given \bpi, \btheta) &= \left(\prod_{t=1}^T p(z_{t} \given
    z_{t-1})\right) \prod_{j} p(u_j \given \bz, \bpi, \btheta)
  \prod_{j'} p(q_{jj'} \given u_j \pi_{jj'}, \phi_{jj'}) \\
  &= \left(\prod_{t} \frac{\pi_{z_{t-1}z_t}\phi_{z_{t-1}z_t}}{T_{z_{t-1}}}\right) \prod_{j}
  \frac{T_j^{n_{j\cdot}}}{\Gamma(n_{j\cdot})} u_j^{n_{j\cdot} - 1}
  e^{-T_j u_j} \\ &\qquad\qquad\times
  \prod_{j'} e^{-u_j\pi_{jj'}(1-\phi_{jj'})} u_j^{q_{jj'}}
  \pi_{jj'}^{q_{jj'}} (1-\phi_{jj'})^{q_{jj'}} (q_{jj'}!)^{-1} \\
  &= \prod_{j} \Gamma(n_{j\cdot})^{-1} u_j^{n_{j\cdot} + q_{j\cdot}-1}
  \\ &\qquad\qquad \times \prod_{j'}
  \pi_{jj'}^{n_{jj'} + q_{jj'}} \phi_{jj'}^{n_{jj'}}
  (1-\phi_{jj'})^{q_{jj'}} e^{-\pi_{jj'}\phi_{jj'}u_j}
  e^{-\pi_{jj'}(1-\phi_{jj'})u_j} (q_{jj'}!)^{-1} \\
  &\label{eq:joint-likelihood} = \prod_{j} \Gamma(n_{j\cdot})^{-1} u_j^{n_{j\cdot} + q_{j\cdot}-1} \prod_{j'}
  \pi_{jj'}^{n_{jj'} + q_{jj'}} \phi_{jj'}^{n_{jj'}}
  (1-\phi_{jj'})^{q_{jj'}} e^{-\pi_{jj'}u_j} (q_{jj'}!)^{-1}
\end{align}

\subsection{An HDP-HSMM-LT modification}
\label{sec:an-hsmm-modification}

Note that it is trivial to modify the HDP-HMM-LT to allow the
number of observations generated each time a state is visited to have
a distribution which is not Geometric, by simply fixing the diagonal
elements of $\bpi$ to be zero, and allowing $D_t$ observations to be
emitted $i.i.d.$ $F(\theta_{z_t})$ at jump $t$, where
\begin{equation}
  \label{eq:95}
  D_t \given \bz \stackrel{ind}{\sim} g(\omega_{z_t}) \qquad \omega_j
  \stackrel{i.i.d}{\sim} G
\end{equation}
The likelihood then includes the additional term for the $D_t$, and
the only inference step which is affected is that instead of sampling
$\bz$ alone, we sample $\bz$ and the $D_t$ jointly, by defining
\begin{equation}
  z^*_s = z_{\max\{T \given s \leq \sum_{t=1}^T D_t\}}
\end{equation}
where $s$ ranges over the number of observations, 
and associating a $\by_s$ with each $z^*_s$.
Inferences about $\bphi$ are not affected, since the diagonal
elements are assumed to be 1 anyway.

This is the same construction used in the Hierarchical Dirichlet
Process Hidden Semi-Markov Model (HDP-HSMM; Johnson et al.,
2013).  Unlike in the standard representation of the HDP-HSMM,
however, there is no need to introduce
additional auxiliary variables as a result of this modification, due
to the presence of the (continuous) durations, $\bu$, which were
already needed to account for the normalization of the $\bpi$.

\subsection{Summary}
\label{sec:model-summary}

I have defined the following augmented generative model for the
HDP-H(S)MM-LT:
\begin{align}
  \label{eq:96}
  \bbeta &\sim \mathrm{GEM}(\gamma) \\
  \theta_j &\stackrel{i.i.d}{\sim} H \\
  \pi_{jj'} \given \bbeta, \btheta &\sim \Gamm{\alpha \beta_{j'}}{1}
  \\
  z_{t} \given z_{t-1}, \bpi, \btheta &\sim \sum_{j}
  \left(\frac{\pi_{z_{t-1}j}\phi_{z_{t-1}j}}{\sum_{j'}
    \pi_{z_{t-1}j'}\phi_{z_{t-1}j'}}\right)\delta_j \\
  u_j \given \bz, \bpi, \btheta &\stackrel{ind}{\sim}
  \Gamm{n_{j\cdot}}{\sum_{j'} \pi_{jj'}\phi_{jj'}} \\
  q_{jj'} \given \bu, \bpi, \btheta &\stackrel{ind}{\sim}
  \Pois{u_j(1 - \phi_{jj'})\pi_{jj'}} \\
  \label{eq:likelihood} \by_t \given \bz, \btheta &\sim F(\theta_{z_t})
\end{align}

If we are using the HSMM variant, then we simply fix $\pi_{jj}$ to 0
for each $j$, draw
\begin{align}
  \label{eq:97}
  \omega_j &\stackrel{i.i.d}{\sim} G \\
  D_t \given \bz &\stackrel{ind}{\sim} g(\omega_{z_t}),
\end{align}
for chosen $G$ and $g$, set
\begin{equation}
  \label{eq:98}
  z^*_s = z_{\max\{T \given s \leq \sum_{t=1}^T D_t\}}
\end{equation}
and replace \eqref{eq:likelihood} with
\begin{equation}
  \label{eq:likelihood-hsmm} \by_s \given \bz, \btheta \sim F(\theta_{z^*_s})
\end{equation}

\section{Inference}
\label{sec:inference}

I develop a Gibbs sampling algorithm based on the Markov Process with
Failed Jumps representation, augmenting the data with the duration
variables $\bu$, the failed jump attempt count matrix, $\bQ$, as well
as additional auxiliary variables which we will define below.
In this representation the transition matrix is not modeled
directly, but is a function of the unscaled transition matrix $\pi$
and the similarity matrix $\bphi$.  The full set of variables is
partitioned into three blocks: $\{\gamma, \alpha, \bbeta, \bpi\}$,
$\{\bz, \bu, \bQ, \Lambda\}$, and $\{\btheta\}$, where $\Lambda$
represents a set of auxiliary variables that will be introduced
below.  The variables in each block are sampled jointly 
conditioned on the other two blocks.

Since we are representing the transition matrix of the Markov chain
explicitly, we approximate the stick-breaking process that produces
$\bbeta$ using a finite Dirichlet distribution with a number of 
components larger than we expect to need, forcing the remaining 
components to have zero weight.  
Let $J$ indicate the maximum number of states.  Then,
we approximate \eqref{eq:beta} with
\begin{equation}
  \label{eq:28}
  \bbeta \given \gamma \sim \mathrm{Dirichlet}(\gamma / J, \dots,
  \gamma / J)
\end{equation}
This distribution converges weakly to the Stick-Breaking Process as $J \to
\infty$.  In practice, $J$ is large enough when the vast majority of the
probability mass in $\bbeta$ is allocated to a strict subset of
components, or when the latent state sequence $\bz$ never uses all $J$
available states, indicating that the data is well described by a number of
states less than $J$.

\subsection{Sampling $\bpi$, $\bbeta$, $\alpha$ and $\gamma$}
\label{sec:sampling-pi}

The joint conditional over $\gamma$, $\alpha$, $\bbeta$ and $\bpi$
given $\bz$, $\bu$, $\bQ$, $\Lambda$ and $\btheta$ will factor as
\begin{equation}
  \label{eq:46}
  p(\gamma, \alpha, \bbeta, \bpi \given \bz, \bu, \bQ, \Lambda, \btheta) = p(\gamma \given
  \Lambda) p(\alpha \given \Lambda) p(\bbeta \given \gamma, \Lambda) p(\bpi
  \given \alpha, \bbeta, \btheta, \bz)
\end{equation}
I will derive these four factors in reverse order.

\subsubsection{Sampling $\bpi$}

The entries in $\bpi$ are conditionally independent given $\alpha$ and
$\bbeta$, so we have the prior
\begin{equation}
  \label{eq:47}
  p(\bpi \given \bbeta, \alpha) = \prod_{j} \prod_{j'}
  \Gamma(\alpha\beta_{j'})^{-1}
  \pi_{jj'}^{\alpha\beta_{j'} - 1} \exp(-\pi_{jj'}),
\end{equation}
and the likelihood given augmented data $\{\bz, \bu, \bQ\}$ given by
\eqref{eq:joint-likelihood}.  Combining these, we have
\begin{equation}
  \label{eq:61}
  p(\bpi, \bz, \bu, \bQ \given \bbeta, \alpha, \btheta) =
  \prod_{j} u_j^{n_{j\cdot} + q_{j\cdot}
  - 1}\prod_{j'} 
  \Gamma(\alpha\beta_{j'})^{-1} \pi_{jj'}^{\alpha\beta_{j'} + n_{jj'}
    + q_{jj'} - 1} e^{-(1 + u_j)
    \pi_{jj'}} \phi_{jj'}^{n_{jj'}} (1-\phi_{jj'})^{q_{jj'}} (q_{jj'}!)^{-1}
\end{equation}
Conditioning on everything except $\bpi$, we get
\begin{align}
  \label{eq:24}
  p(\bpi \given \bQ, \bu, \bZ, \bbeta, \alpha, \btheta) &\propto \prod_j
  \prod_{j'} \pi_{jj'}^{\alpha\beta_{j'} + n_{jj'} + q_{jj'} - 1}
  \exp(-(1 + u_j)\pi_{jj'})
\end{align}
and thus we see that the $\pi_{jj'}$ are conditionally independent
given $\bu$, $\bZ$ and $\bQ$, and distributed according to
\begin{align}
  \label{eq:25}
  \pi_{jj'} \given n_{jj'}, q_{jj'}, \beta_{j'}, \alpha \stackrel{ind}{\sim}
  \Gamm{\alpha\beta_{j'} + n_{jj'} + q_{jj'}}{1 + u_j}
\end{align}


\subsubsection{Sampling $\bbeta$}
\label{sec:sampling-bbeta}

Consider the conditional distribution of $\bbeta$ having
integrated out $\bpi$.  The prior density of $\bbeta$ from
\eqref{eq:28} is
\begin{equation}
  \label{eq:62}
  p(\bbeta \given \gamma) =
  \frac{\Gamma(\gamma)}{\Gamma(\frac{\gamma}{J})^J} \prod_{j}
  \beta_j^{\frac{\gamma}{J} - 1}
\end{equation}
After integrating out $\bpi$ in \eqref{eq:61}, we have
\begin{align}
  p(\bz, \bu, \bQ \given \bbeta, \alpha, \gamma, \btheta) &=
  \prod_{j=1}^J u_{j} ^{-1}
  \prod_{j'=1}^J u^{n_{jj'} + q_{jj'} - 1}(1 +
  u_j)^{-(\alpha\beta_{j'} + n_{jj'} + q_{jj'})}
  \\
  &\qquad \qquad \times \frac{\Gamma(\alpha\beta_{j'} + n_{jj'} +
    q_{jj'})}{\Gamma(\alpha\beta_{j'})} \phi_{jj'}^{n_{jj'}}(1-\phi_{jj'})^{q_{jj'}}
  (q_{jj'}!)^{-1} \\
  &= \prod_{j=1}^J \Gamma(n_{j\cdot})^{-1} u_j^{-1}(1+u_j)^{-\alpha}
  \left(\frac{u_j}{1+u_j}\right)^{n_{j\cdot} + q_{j\cdot}} \\ &\qquad
  \qquad \times \prod_{j' =
    1}^J \frac{\Gamma(\alpha\beta_{j'} + n_{jj'} +
    q_{jj'})}{\Gamma(\alpha\beta_{j'})} \phi_{jj'}^{n_{jj'}}(1-\phi_{jj'})^{q_{jj'}}
  (q_{jj'}!)^{-1}
\end{align}
 where we have used the fact that the $\beta_j$ sum to 1.  Therefore
\begin{align}
  p(\bbeta \given \bz, \bu, \bQ, \alpha, \gamma, \btheta) &\propto \prod_{j=1}^J
  \beta_j^{\frac{\gamma}{J} - 1} \prod_{j'=1}^J \frac{\Gamma(\alpha\beta_{j'} +
    n_{jj'} + q_{jj'})}{\Gamma(\alpha\beta_{j'})}.
\end{align}

Following Teh et al. (2006), we can write the ratios of Gamma functions
as polynomials in $\beta_j$, as
\begin{equation}
  \label{eq:31}
  p(\bbeta \given \bz, \bu, \bQ, \alpha, \gamma, \btheta) \propto \prod_{j=1}^J
  \beta_j^{\frac{\gamma}{J} - 1} \prod_{j'=1}^{J} \sum_{m_{jj'} = 1}^{n_{jj'}}
  s(n_{jj'} + q_{jj'}, m_{jj'}) (\alpha \beta_{j'})^{m_{jj'}}
\end{equation}
where $s(m,n)$ is an unsigned Stirling number of the first kind.
This admits an augmented data representation, where we introduce a
random matrix $\bM = (m_{jj'})_{1 \leq j,j' \leq J}$, whose
entries are conditionally independent given $\bbeta$, $\bQ$ and $\bz$, with
\begin{equation}
  \label{eq:32}
  p(m_{jj'} = m \given \beta_{j'}, \alpha, n_{jj'}, q_{jj'}) =
  \frac{s(n_{jj'} + q_{jj'}, m) \alpha^{m}
    \beta_{j'}^{m}}{\sum_{m'=0}^{n_{jj'} + q_{jj'}} s(n_{jj'} +
  q_{jj'}, m') \alpha^{m'} \beta_{j'}^{m'}}
\end{equation}
for integer $m$ ranging between $0$ and $n_{jj'} + q_{jj'}$.  Note
that $s(n,0) = 0$ if $n > 0$, $s(0,0) = 1$ and $s(0,m) = 0$ if $m > 0$.
Then, we have joint distribution
\begin{equation}
  \label{eq:33}
  p(\bbeta, \bM \given \bz, \bu, \bQ, \alpha, \gamma, \btheta) \propto \prod_{j=1}^J
  \beta_j^{\frac{\gamma}{J} - 1} \prod_{j'=1}^{J} s(n_{jj'} + q_{jj'}, m_{jj'}) \alpha^{m_{jj'}} \beta_{j'}^{m_{jj'}}
\end{equation}
which yields \eqref{eq:31} when marginalized over $\bM$.  Again discarding
constants in $\bbeta$ and regrouping yields
\begin{equation}
  \label{eq:34}
  p(\bbeta \given \bM, \bZ, \bu, \btheta, \alpha, \gamma) \propto \prod_{j=1}^J
  \beta_j^{\frac{\gamma}{J} + m_{\cdot j}- 1}
\end{equation}
which is Dirichlet:
\begin{equation}
  \label{eq:38}
  \bbeta \given \bM, \gamma \sim \mathrm{Dirichlet}(\frac{\gamma}{J} +
  m_{\cdot 1}, \dots, \frac{\gamma}{J} + m_{\cdot J})
\end{equation}

\subsubsection{Sampling $\alpha$ and $\gamma$}
\label{sec:sampling-alpha}
Assume that $\alpha$ and $\gamma$ have Gamma priors, with
\begin{align}
  \label{eq:42}
  p(\alpha) &= \frac{b_{\alpha}^{a_{\alpha}}}{\Gamma(a_{\alpha})}
  \alpha^{a_{\alpha} - 1} \exp(-b_{\alpha}\alpha) \\
  p(\gamma) &= \frac{b_{\gamma}^{a_\gamma}}{\Gamma(a_{\gamma})}
  \gamma^{a_{\gamma - 1}} \exp(-b_{\gamma}\gamma)
\end{align}

Having integrated out $\bpi$, we have
\begin{align}
  p(\bbeta, \bz, \bu, \bQ, \bM \given \alpha, \gamma, \btheta) &=
  \frac{\Gamma(\gamma)}{\Gamma(\frac{\gamma}{J})^J} \alpha^{m_{\cdot\cdot}} \prod_{j=1}^J \beta_j^{\frac{\gamma}{J} +
    m_{\cdot j} - 1}\Gamma(n_{j\cdot})^{-1} u_j^{-1}(1+u_j)^{-\alpha}
  \left(\frac{u_j}{1+u_j}\right)^{n_{j\cdot} + q_{j\cdot}} \\ &\qquad
  \qquad \times \prod_{j' =
    1}^J s(n_{jj'} + q_{jj'}, m_{jj'}) \phi_{jj'}^{n_{jj'}}(1-\phi_{jj'})^{q_{jj'}}
  (q_{jj'}!)^{-1}
\end{align}
We can also integrate out $\bbeta$, to yield
\begin{align}
  p(\bz, \bu, \bQ, \bM \given \alpha, \gamma, \btheta) &=
  \alpha^{m_{\cdot\cdot}} e^{-\sum_{j''} \log(1+u_{j''}) \alpha}
  \frac{\Gamma(\gamma)}{\Gamma(\gamma + m_{\cdot\cdot})} \\ &\qquad
  \qquad \times \prod_j
  \frac{\Gamma(\frac{\gamma}{J} + m_{\cdot
      j})}{\Gamma(\frac{\gamma}{J}) \Gamma(n_{j\cdot})} u_j^{-1}
  \left(\frac{u_j}{1+u_j}\right)^{n_{j\cdot} + q_{j\cdot}} \\ &\qquad
  \qquad \times \prod_{j' =
    1}^J s(n_{jj'} + q_{jj'}, m_{jj'}) \phi_{jj'}^{n_{jj'}}(1-\phi_{jj'})^{q_{jj'}}
  (q_{jj'}!)^{-1}
\end{align}
demonstrating that $\alpha$ and $\gamma$ are independent given $\btheta$
and the augmented data, with
\begin{equation}
  \label{eq:43}
  p(\alpha \given \bz, \bu, \bQ, \bM, \btheta) \propto
  \alpha^{a_{\alpha} + m_{\cdot\cdot}}\exp(-(b_\alpha + \sum_{j}\log(1+u_j))\alpha)
\end{equation}
and
\begin{align}
  \label{eq:8}
  p(\gamma \given \bz, \bu, \bQ, \bM, \btheta) &\propto \gamma^{a_{\gamma - 1}}
  \exp(-b_{\gamma}\gamma) \frac{\Gamma(\gamma)\prod_{j=1}^J
    \Gamma(\frac{\gamma}{J} + m_{\cdot j})}{\Gamma(\frac{\gamma}{J})^J\Gamma(\gamma + m_{\cdot\cdot})}
\end{align}
So we see that
\begin{equation}
  \label{eq:44}
  \alpha \given \bz, \bu, \bQ, \bM, \btheta \sim \Gamm{a_{\alpha}
    + m_{\cdot\cdot}}{b_\alpha + \sum_j\log(1+u_j)}
\end{equation}
To sample $\gamma$, we introduce a new set of auxiliary variables, $\br = (r_1, \dots,
r_j)$ and $t$ with the following distributions:
\begin{align}
  \label{eq:9}
  p(r_j = r \given m_{\cdot j}, \gamma) &=
  \frac{\Gamma(\frac{\gamma}{J})}{\Gamma(\frac{\gamma}{J}
    + m_{\cdot j})} s(m_{\cdot j}, r)
    \left(\frac{\gamma}{J}\right)^r \qquad r  = 1, \dots, m_{\cdot j} \\
  p(t \given m_{\cdot\cdot} \gamma) &= \frac{\Gamma(\gamma +
    m_{\cdot\cdot})}{\Gamma(\gamma) \Gamma(m_{\cdot\cdot})} t^{\gamma
    - 1} (1-t)^{m_{\cdot\cdot} - 1} \qquad t \in (0,1)
\end{align}
so that
\begin{align}
  \label{eq:10}
  p(\gamma, \br, t \given \bM) &\propto \gamma^{a_{\gamma - 1}}
  \exp(-b_{\gamma}\gamma) t^{\gamma - 1}(1-t)^{m_{\cdot\cdot} +
    q_{\cdot} - 1} \prod_{j=1}^J s(m_{\cdot j} + q_j, r_j)
  \left(\frac{\gamma}{J}\right)^{r_j}
\end{align}
and
\begin{align}
  \label{eq:11}
  p(\gamma \given \br, t) \propto \gamma^{a_\gamma +
    r_{\cdot} - 1} \exp(-(b_{\gamma} - \log(t)) \gamma),
\end{align}
which is to say
\begin{equation}
  \label{eq:18}
  \gamma \given \br, t, \bz, \bu, \bQ, \bM, \btheta \sim \Gamm{a_{\gamma} + r_{\cdot}}{b_{\gamma} - \log(t)}
\end{equation}

\subsubsection{Summary}

I have made the following additional assumptions about the generative
model in this section:
\begin{equation}
  \label{eq:100}
  \gamma \sim \Gamm{a_{\gamma}}{b_{\gamma}} \qquad \alpha \sim \Gamm{a_{\alpha}}{b_{\alpha}}
\end{equation}

The joint conditional over $\gamma$, $\alpha$, $\bbeta$ and $\bpi$
given $\bz$, $\bu$, $\bQ$, $\bM$, $\br$, $t$ and $\btheta$ factors as
\begin{equation}
  \label{eq:46}
  p(\gamma, \alpha, \bbeta, \bpi \given \bz, \bu, \bQ, \br, t,
  \btheta) = p(\gamma \given \br, t) p(\alpha \given \bu, \bM) p(\bbeta
  \given \gamma, \bM) p(\bpi \given \alpha, \bbeta, \bz, \bu, \bQ)
\end{equation}
where
\begin{align}
  \label{eq:64}
  \gamma \given \br, t &\sim \Gamm{a_{\gamma} + r_{\cdot}}{b_{\gamma} -
    \log(t)} \\
  \alpha \given \bu, \bM &\sim \Gamm{a_{\alpha} +
    m_{\cdot\cdot}}{b_{\alpha} + \sum_j \log(1 + u_j)} \\
  \bbeta \given \gamma, \bM &\sim \mathrm{Dirichlet}(\frac{\gamma}{J} + m_{\cdot 1},
  \dots, \frac{\gamma}{J} + m_{\cdot J}) \\
  \pi_{jj'} \given \alpha, \beta_{j'}, \bz, \bu, \bQ
  &\stackrel{ind}{\sim} \Gamm{\alpha\beta_{j'} + n_{jj'} + q_{jj'}}{1 +
  u_j}
\end{align}


\subsection{Sampling $\bz$ and the auxiliary variables}
\label{sec:sampling-z_t}

The hidden state sequence, $\bz$, is sampled jointly with the auxiliary
variables, which consist of $\bu$, $\bM$, $\bQ$, $\br$ and $t$.  The
joint conditional distribution of these variables is defined directly
by the generative model:
\begin{align}
  \label{eq:19}
  p(\bz, \bu, \bQ, \bM, \br, t \given \bpi, \bbeta, \alpha, \gamma,
  \btheta) &= p(\bz \given \bpi, \btheta) p(\bu \given \bz, \bpi, \btheta) p(\bQ \given
  \bu, \bpi, \btheta) p(\bM \given
  \bz, \bQ, \alpha, \bbeta) \\
  &\qquad \times p(\br \given
  \gamma, \bM) p(t \given \gamma, \bM)
\end{align}
Since we are representing the transition matrix explicitly, we can
sample the entire sequence $\bz$ at once with the forward-backward algorithm,
as in an ordinary HMM (or, if we are employing the HSMM variant
described in Sec. \ref{sec:an-hsmm-modification}, then we can use the
modified message passing scheme for HSMMs described by Johnson and
Wilsky, 2013).  Having done this, we can sample $\bu$, $\bQ$, $\bM$,
$\br$ and $t$ from their forward distributions.  To summarize,
we have
\begin{align}
  \label{eq:48}
  u_j \given \bZ, \bpi, \btheta &\stackrel{ind}{\sim}
  \Gamm{n_{j\cdot}}{\sum_{j'} \pi_{jj'}\phi_{jj'}} \\
  q_{jj'} \given u_j, \pi_{jj'}, \phi_{jj'} &\stackrel{ind}{\sim}
  \Pois{u_j(1 - \phi_{jj'})\pi_{jj'}} \\
  m_{jj'} \given n_{jj'}, q_{jj'}, \beta_{j'}, \alpha &\stackrel{ind}{\sim}
  \frac{\Gamma(\alpha\beta_j)}{\Gamma(\alpha\beta_j + n_{jj'} +
    q_{jj'})}\sum_{m=1}^{n_{jj'} + q_{jj'}} s(n_{jj'} + q_{jj'}, m) \alpha^m \beta_{j'}^m \delta_{m}
  \\
  r_j \given m_{\cdot j}, \gamma &\stackrel{ind}{\sim}
  \frac{\Gamma(\frac{\gamma}{J})}{\Gamma(\frac{\gamma}{J} + m_{\cdot
      j})} \sum_{r=1}^{m_{j\cdot}} s(m_{\cdot j}, r)
  \left(\frac{\gamma}{J}\right)^r \delta_r \\
  t \given \gamma, \bM &\sim \Beta{\gamma}{m_{\cdot\cdot}}
\end{align}

\subsection{Sampling state and emission parameters}
\label{sec:sampling-eta}

The state parameters, $\btheta$, influence the transition matrix,
$\bpi$ and the auxiliary vector $q$ through the similarity matrix
matrix $\bphi$, and also control the emission distributions.
We have likelihood factors
\begin{align}
  \label{eq:65}
  p(\bz, \bQ \given \btheta) &\propto \prod_{j}\prod_{j'}
  \phi_{jj'}^{n_{jj'}}(1-\phi_{jj'})^{q_{jj'}} \\
  p(\bY \given \bz, \btheta) &= \prod_{t=1}^T f(\by_t; \theta_{z_t})
\end{align}
where proportionality is with respect to variation in $\btheta$.

The parameter space for the hidden states, 
the associated prior $H$ on $\btheta$, and the similarity function
$\Phi$, is application-specific, but we consider here the case where a state,
$\theta_j$, consists of a finite-length binary vector, motivated by the
application of inferring the set of relevant entities in each sentence of
a text document.

Let $\theta_j = (\theta_{j1}, \dots, \theta_{jD})$, with $\theta_{jd} = 1$
indicating presence of feature $d$ in context state $j$, and
$\theta_{jd} = 0$ indicating absence.  Of course, in this case,
the set of possible states is finite, and so on its face it may
seem that a nonparametric model is unnecessary.  However, if $D$ is
reasonably large, it is likely that most of the $2^D$ possible states
are vanishingly unlikely (and, in fact, the number of observations may
well be less than $2^D$), and so we would like a model that encourages
the selection of a sparse set of states.  Moreover, there may be more
than one state with the same $\theta$, but with different transition dynamics.

\subsubsection{Sampling $\btheta$}
\label{sec:sampling-eta}

In principle, $H$ can be any distribution over binary vectors, but we
will suppose for simplicity that it can be factored into $D$
independent coordinate-wise Bernoulli variates.  Let $\mu_d$ be the
Bernoulli parameter for the $d$th coordinate.

We require a similarity function, $\Phi(\theta_j, \theta_{j'})$, which 
varies between 0 to 1, and is equal to 1 if and only if $\theta_j =
\theta_{j'}$.  A natural choice in this setting is the Laplacian kernel:
\begin{align}
  \label{eq:39}
  \phi_{jj'} &= \Phi(\theta_j, \theta_{j'}) = \exp(-\lambda \Delta_{jj'})
\end{align}
where $\Delta_{jj'd} = \abs{\theta_{jd} - \theta_{j'd}}$, $\Delta_{jj'} =
\sum_{d=1}^D \Delta_{jj'}$ is the Hamming
distance between $\theta_j$ and $\theta_{j'}$,
and $\lambda \geq 0$ (if $\lambda = 0$, the $\phi_{jj'}$
are identically 1, and so do not have any influence, reducing the
model to an ordinary HDP-HMM).

Let
\begin{align}
  \label{eq:68}
  \phi_{jj'-d} &= \exp(-\lambda(\Delta_{jj'} - \Delta_{jj'd}))
\end{align}
so that $\phi_{jj'} = \phi_{jj'-d} e^{-\lambda\Delta_{jj'd}}$.

Since the matrix $\bphi$ is assumed to be symmetric, we have
\begin{align}
  \label{eq:70}
  \frac{p(\bz, \bQ \given \theta_{jd}  = 1, \btheta\setminus\theta_{jd}
    )}{p(\bz, \bQ \given \theta_{jd}  = 0, \btheta\setminus\theta_{jd} )}
  &\propto \prod_{j' \neq j}
  \frac{e^{-\lambda(n_{jj'} + n_{j'j})\abs{1 - \theta_{j'd}}}(1 -
    \phi_{jj'-d} e^{-\lambda\abs{1 - \theta_{j'd}}})^{q_{jj'} +
      q_{j'j}}}{e^{-\lambda(n_{jj'} + n_{j'j})\abs{\theta_{j'd}}}
    (1-\phi_{jj'-d}e^{-\lambda\abs{\theta_{j'd}}})^{q_{jj'} +
      q_{j'j}}} \\
  &= \label{eq:71} e^{-\lambda(c_{jd0} - c_{jd1})}
  \prod_{j' \neq j} \left(\frac{1 - \phi_{jj'-d}e^{-\lambda}}{1-\phi_{jj'-d}}\right)^{(-1)^{\theta_{j'd}}(q_{jj'} + q_{j'j})}
\end{align}
where $c_{jd0}$ and $c_{jd1}$ are the number of successful jumps to or
from state $j$, to or from states with a 0 or 1, respectively, in
position $d$.  That is,
\begin{equation}
  \label{eq:72}
  c_{jd0} = \sum_{\{j' \given \theta_{j'd} = 0\}} n_{jj'} + n_{j'j}\qquad c_{jd1} = \sum_{\{j' \given \theta_{j'd} = 1\}} n_{jj'} + n_{j'j}
\end{equation}

Therefore, we can Gibbs sample $\theta_{jd}$ from its conditional
posterior Bernoulli distribution given the rest of $\btheta$, where
we compute the Bernoulli parameter via the log-odds
\begin{align}
  \label{eq:77}
  &\log\left(\frac{p(\theta_{jd} = 1 \given \bY, \bz, \bQ, \btheta \setminus
    \theta_{jd})}{p(\theta_{jd} = 0 \given \bY, \bz, \bQ, \btheta
    \setminus \theta_{jd})}\right) = \log\left(\frac{p(\theta_{jd} =
  1) p(\bz, \bQ \given \theta_{jd} = 1, \btheta \setminus
  \theta_{jd}) p(\bY \given \bz, \theta_{jd} = 1, \btheta \setminus \theta_{jd})}{p(\theta_{jd} = 0) p(\bz, \bQ \given \theta_{jd} = 0,
  \btheta \setminus \theta_{jd}) p(\bY \given \bz, \theta_{jd} = 0,
  \btheta \setminus \theta_{jd})}\right) \\ & \qquad = \log\left(\frac{\mu_d}{1 - \mu_d}\right)
  + (c_{jd1} - c_{jd0}) \lambda +
    \sum_{j' \neq j}
  (-1)^{\theta_{j'd}}(q_{jj'} + q_{j'j})\log\left(\frac{1 -
      \phi_{jj'}^{(-d)}e^{-\lambda}}{1-\phi_{jj'}^{(-d)}}\right) \\ &
  \qquad \qquad + \sum_{\{t \given z_t = j\}} \log\left(\frac{f(\by_t;
      \theta_{jd} = 1, \theta_j \setminus \theta_{jd})}{f(\by_t;
      \theta_{jd} = 0, \theta_j \setminus \theta_{jd})}\right)
\end{align}

Suppose also that the observed data $\bY$ consists of a $T \times K$
matrix, where the $t$th row $\by_t = (y_{t1}, \dots,
y_{tK})^{\mathsf{T}}$ is a $K$-dimensional feature vector associated
with time $t$, and let $\bW$ be a $D \times K$ weight matrix
with $k$th column $\bw_k$, such that 
\begin{equation}
  \label{eq:74}
  f(\by_t; \theta_j) = g(\by_t; \bW^{\mathsf{T}} \theta_j)
\end{equation}
for a suitable parametric function $g$.  I will assume for simplicity
that $g$ factors as
\begin{equation}
  \label{eq:73}
  g(\by_t; \bW^{\mathsf{T}} \theta_j) = \prod_{k=1}^K g_k(y_{tk}; \bw_k \cdot \theta_j)
\end{equation}
Define $x_{tk} = \bw_k \cdot \theta_{z_{t}}$, and
$x_{tk}^{(-d)} = \bw_k^{-d} \cdot \theta_{z_{t}}^{-d}$, where
$\theta_{j}^{-d}$ and $\bw_{k}^{-d}$ are $\theta_{j}$ and $\bw_k$, respectively, with
the $d$th coordinate removed.  Then
\begin{equation}
  \label{eq:76}
  \log\left(\frac{f(\by_t; \theta_{jd} = 1, \theta_j \setminus
    \theta_{jd})}{f(\by_t; \theta_{jd} = 0, \theta_j \setminus \theta_{jd})}\right) =
  \sum_{k=1}^K \log\left(\frac {g_k(y_{tk};
    x_{tk}^{(-d)} + w_{dk})}{g_k(y_{tk};
    x_{tk}^{(-d)})}\right)
\end{equation}
If $g_k(y; x)$ is a Normal density with mean $x$ and unit variance, then
\begin{equation}
  \label{eq:91}
  \log\left(\frac {g_k(y_{tk};
    x_{tk}^{(-d)} + w_{dk})}{g_k(y_{tk};
    x_{tk}^{(-d)})}\right) = -w_{dk}(y_{tk} - x_{tk}^{(-d)} + \frac{1}{2}w_{dk})
\end{equation}

\subsubsection{Sampling $\bmu$}
\label{sec:sampling-bmu}

Sampling the $\mu_d$ is straightforward with a Beta prior.  Suppose
\begin{equation}
  \label{eq:92}
  \mu_d \stackrel{ind}{\sim} \Beta{a_\mu}{b_\mu}
\end{equation}
Then, conditioned on $\btheta$ the $\mu_d$ are independent with
\begin{equation}
  \label{eq:93}
  \mu_d \given \btheta \sim \Beta{a_\mu + \sum_{j} \theta_{jd}}{b_\mu +
  \sum_{j} (1 - \theta_{jd})}
\end{equation}

\subsubsection{Sampling $\lambda$}
\label{sec:sampling-lambda}

The parameter $\lambda$ governs the connection between $\btheta$ and
$\bphi$.  Writing \eqref{eq:65} in terms of $\lambda$ and the difference matrix
$\boldsymbol{\Delta} = (\Delta_{jj'})_{1 \leq j,j' \leq J}$ gives
\begin{equation}
  \label{eq:88}
  p(\bz, \bQ \given \lambda, \btheta) \propto \prod_{j}\prod_{j'}
  e^{-\lambda \Delta_{jj'} n_{jj'}}(1-e^{-\lambda\Delta_{jj'}})^{q_{jj'}} 
\end{equation}
Put an $\Exp{b_{\lambda}}$ prior on $\lambda$, so that
\begin{equation}
  \label{eq:88}
  p(\lambda \given \bz, \bQ, \btheta) \propto
  e^{-(b_{\lambda} + \sum_{j}\sum_{j'} \Delta_{jj'} n_{jj'})\lambda} \prod_{j}\prod_{j'}
  (1-e^{-\lambda\Delta_{jj'}})^{q_{jj'}}
\end{equation}
This density is log-concave, with
\begin{equation}
  \label{eq:90}
  -\frac{d^2\log(p(\lambda \given \bz, \bQ,
    \btheta))}{d\lambda^2} = \sum_{\{(j,j') \given
    \Delta_{jj'} > 0\}}
  \frac{\Delta_{jj'}^2 q_{jj'}
    e^{\lambda\Delta_{jj'}}}{(e^{\lambda\Delta_{jj'}} - 1)^2} > 0
\end{equation}
and so we can use Adaptive Rejection Sampling (Gilks and Wild, 1992)
to sample from it.  The relevant $h$ and $h'$, representing the log
density and its first derivative, respectively, are
\begin{align}
  \label{eq:94}
  h(\lambda) &= 
  -(b_{\lambda} + \sum_{\{(j,j') \given \Delta_{jj'} > 0\}} \Delta_{jj'} n_{jj'})\lambda +
  \sum_{\{(j,j') \given \Delta_{jj'} > 0\}} q_{jj'} \log(1 - e^{-\lambda\Delta_{jj'}}) \\
  h'(\lambda) &= -(b_{\lambda} + \sum_{\{(j,j') \given \Delta_{jj'} > 0\}} \Delta_{jj'}
  n_{jj'}) + \sum_{\{(j,j') \given \Delta_{jj'} > 0\}}
  \frac{q_{jj'}\Delta_{jj'}}{e^{\lambda\Delta_{jj'}} - 1}
\end{align}

\subsubsection{Sampling $\bW$}

Conditioned on the state matrix $\btheta$ and the data matrix $\bY$, the weight matrix $\bW$ can be sampled as well using standard methods
for Bayesian regression problems.  For example, suppose that the
weights are {\em a priori} i.i.d. Normal:
\begin{equation}
  \label{eq:78}
  p(\bW) = \prod_{k=1}^K\prod_{d=1}^D \mathcal{N}(w_{dk} \given 0,\sigma^2_0)
\end{equation}
and the likelihood is
\begin{equation}
  \label{eq:79}
  g_k(y; x) = \mathcal{N}(y \given x,1)
\end{equation}
Then it is a standard result from Bayesian linear modeling that
\begin{equation}
  \label{eq:80}
  p(\bW \given \btheta, \bY) = \prod_{k=1}^K
  \mathcal{N}\left(\left(\sigma_0^2 \mathbf{I} + \btheta^{\mathsf{T}} \btheta
    \right)^{-1}\btheta^{\mathsf{T}}\by_k, \sigma_0^2 \mathbf{I} + \btheta^{\mathsf{T}} \btheta\right)
\end{equation}

If one or more output features, say $\by_k$, is binary, we can adopt a
probit model where we introduce a latent data vector $\by^*_k$ for
each such $k$, and assume
\begin{equation}
  \label{eq:83}
  p(\by^*_{k} \given \bx_k) = \prod_{t} \mathcal{N}(y^*_{tk} \given x_{tk}, 1)
\end{equation}
and 
\begin{equation}
  \label{eq:84}
  y_{tk} = \begin{cases}
    0,& y^*_{tk} \leq 0 \\
    1,& y^*_{tk} > 0
  \end{cases}
\end{equation}
And so, after marginalizing over $\by^*_k$
\begin{equation}
  \label{eq:81}
  p(\by_k \given \bx_k) = \prod_{t=1}^T F(x_{tk})^{y_{tk}} (1 - F(x_{tk}))^{1 - y_{tk}}
\end{equation}
where $F$ is the standard Normal CDF, since
\begin{equation}
  \label{eq:85}
  \int_{0}^{\infty} dy^*_{tk} \mathcal{N}(y^*_{tk} \given x_{tk}, 1) =
  \int_{-x_{tk}}^{\infty} dy^*_{tk} \mathcal{N}(y^*_{tk} \given 0, 1)
  = 1 - F(-x_{tk}) = F(x_{tk})
\end{equation}
Then, conditioned on $x_{tk}$ and $y_{tk}$, we can sample $y^*_{tk}$ 
from a Normal distribution left- or right-truncated at 0:
\begin{equation}
  \label{eq:82}
  p(y^*_{tk} \given x_{tk}, y_{tk}) = \begin{cases}
    \mathcal{N}(x_{tk}, 1) I(y^{*}_{tk} \leq 0), & y_{tk} = 0 \\
    \mathcal{N}(x_{tk}, 1) I(y^*_{tk} > 0), & y_{tk} = 1
  \end{cases}
\end{equation}
Conditioned on the $y^*_{tk}$ and $\btheta$, the weights are
distributed as in \eqref{eq:80}.

\subsubsection{Summary}
\label{sec:summary}

I have made the following assumptions about the representation of the
hidden states and observed data in this subsection:
(1) $\btheta$ consists of $D$ binary features (2) the similarity function $\Phi$
is the Laplacian kernel with respect to Hamming distance with
decay parameter $\lambda$, and (3) $\bY$ consists of $K$ continuous or 
binary features associated with each time step $t$.  In addition, we
make the following distributional assumptions:
\begin{align}
\mu_d &\stackrel{i.i.d}{\sim} \Beta{a_{\mu}}{b_{\mu}} \\
\lambda &\sim \Exp{b_{\lambda}} \\
\theta_{jd} \given \bmu &\stackrel{ind}{\sim} \Bern{\mu_d} \\
\bW \sim \Norm{0}{\sigma^2_0 \mathbf{I}}
y^*_{tk} \given \bW, \bz, \btheta &\stackrel{ind}{\sim} \Norm{x_{tk}}{1} \\
y_{tk} &= \begin{cases}
  y^*_{tk}, & \text{if $k$ is a continuous feature} \\
  \mathbb{I}(y^*_{tk} > 0) & \text{if $k$ is a binary feature}
\end{cases}
\end{align}
where we have defined
\begin{align}
  \label{eq:102}
  x_{tk} = \bw_k \cdot \theta_{z_t}
\end{align}

I introduce Gibbs blocks corresponding to (1) each $\theta_{jd}$
individually, (2) the vector $\bmu$, (3) the decay parameter
$\lambda$, (4) the weight matrix $\bW$, and (5) the latent data
$\bY^*$ associated with binary features.  We have
\begin{align}
  \label{eq:101}
  \theta_{jd} \given \btheta \setminus \theta_{jd}, \bz, \bQ, \bmu,
  \lambda, \bW, \bY^* &\sim
  \Bern{\frac{e^{\zeta_{jd}}}{1 + e^{\zeta_{jd}}}} \\
  \mu_d \given \btheta, \dots &\stackrel{ind}{\sim} \Beta{a_\mu + \sum_{j} \theta_{jd}}{b_\mu +
  \sum_{j} (1 - \theta_{jd})} \\
p(\lambda \given \bz, \bQ, \btheta, \dots) &\propto e^{-(b_{\lambda} + \sum_{j}\sum_{j'} \Delta_{jj'} n_{jj'})\lambda} \prod_{j}\prod_{j'}
  (1-e^{-\lambda\Delta_{jj'}})^{q_{jj'}} \\
  \bw_k \given \btheta, \bY^{*}, \dots &\stackrel{ind}{\sim}
  \Norm{(\sigma_0^2 \mathbf{I} + \btheta^{\mathsf{T}}
    \btheta)^{-1}\btheta^{\mathsf{T}}\by^*_k}{\sigma_0^2 \mathbf{I} +
    \btheta^{\mathsf{T}} \btheta} \\
  \by^*_{tk} \given \bX, \bY, \dots &\stackrel{ind}{\sim} \begin{cases}
    \Norm{x_{tk}}{1} \mathbb{I}(y^*_{tk} \leq 0), & y_{tk} = 0 \\
    \Norm{x_{tk}}{1} \mathbb{I}(y^*_{tk} > 0), & y_{tk}= 1
  \end{cases}
\end{align}
where $\Delta_{jj'} = \norm{\theta_j - \theta_j'}_{L_1}$ and
\begin{align}
\zeta_{jd} &= \log\left(\frac{\mu_d}{1 - \mu_d}\right)
  + (c_{jd1} - c_{jd0}) \lambda +
    \sum_{j' \neq j}
  (-1)^{\theta_{j'd}}(q_{jj'} + q_{j'j})\log\left(\frac{1 -
      \phi_{jj'}^{(-d)}e^{-\lambda}}{1-\phi_{jj'}^{(-d)}}\right)
  \notag \\ & \qquad - \sum_{\{t \given z_t = j\}} \sum_{k=1}^K
  w_{dk}(y^*_{tk} - x_{tk}^{(-d)} + \frac{1}{2}w_{dk})
\end{align}
All distributions can be sampled from directly except for $\lambda$, which
requires Adaptive Rejection Sampling, with the equations
\begin{align}
  h(\lambda) &= 
  -(b_{\lambda} + \sum_{\{(j,j') \given \Delta_{jj'} > 0\}} \Delta_{jj'} n_{jj'})\lambda +
  \sum_{\{(j,j') \given \Delta_{jj'} > 0\}} q_{jj'} \log(1 - e^{-\lambda\Delta_{jj'}}) \\
  h'(\lambda) &= -(b_{\lambda} + \sum_{\{(j,j') \given \Delta_{jj'} > 0\}} \Delta_{jj'}
  n_{jj'}) + \sum_{\{(j,j') \given \Delta_{jj'} > 0\}}
  \frac{q_{jj'}\Delta_{jj'}}{e^{\lambda\Delta_{jj'}} - 1}
\end{align}

\chapter{Two Dirichlet Process Models of Context-Rich Probabilistic Grammars}

A caption has two layers of latent 
representation in the model, in addition to the observed word
sequence, $S$.  The first is an
{\em elaboration tree}, $\Psi$, in which each node represents a semantic
entity or relation to be expressed in the text.  The second
representation is a lexicalized constituency tree, $\Lambda$, adapted from the
representation of \citet{collins2003head}, which combines features of
a traditional probabilistic context-free grammar (PCFG) tree and a dependency tree
\cite{mcdm08b}.  We describe these two representations in detail next.

\paragraph{Elaboration Trees: $\Psi$}

We assume that the communicative goal of the speaker producing a
caption is to describe to a listener a three-dimensional scene such
that the listener may pick out the same objects and relations in the
scene that are the speaker's intended referents of the terms in the
caption.  

Each object in $\mathcal{O}$ is a candidate to be the focus of a
description.  We assume the speaker will {\em not} refer to the camera
itself, so the camera is not a member of $\mathcal{O}$; however the
room itself is.  A focal, or
{\em target} object, $t \in \mathcal{O}$ can be elaborated by
including information about its intrinsic features, 
such as color and size, or about its spatial relationship to 
other objects in the scene.  For each new object introduced into 
the description via a spatial relation, it may in turn be elaborated.
These elaborations define a set of recursive grammatical rewrite rules
which instantiate an {\em elaboration tree}, which we denote by $\Psi$.

Each elaboration tree is assumed to have a unary predicate, {\sc
  Focus}($t$), at its root.  This predicate has two children: the {\em
  relation}, $\rho = \textsc{Focus}$, and the {\em target},
$t$, which
indicates the object type.  The node $t$ may be rewritten with either
a spatial relation subtree, or an attribute subtree.  A {\em spatial
  relation} elaboration is a rewrite rule that 
transforms the target object, $t$, into a
predicate node, $\rho(t, b_1, \dots, b_k)$, with $k+2$ children: one
for the relation itself, one for the target object, and one for each
of the reference objects used in the description\footnote{Currently
  the relations we model are all binary --- the $k = 1$ case --- 
but the modeling framework is easily extensible to 
relations of greater arities.}.  An {\em attribute} elaboration is a
rewrite rule that replaces $t$ with a unary predicate node,
$\alpha(t)$, with two children: the attribute, $\alpha$ and $t$ itself.

[SHOW AN EXAMPLE]

\paragraph{Syntactic Trees: $\Lambda$}

The nodes in the elaboration tree consist solely of semantic concepts
and predicates.  All linguistic information is contained in a
{\em syntactic tree}, $\Lambda$.  We first give a brief description of
the representation used by \cite{collins2003head}, and then describe
how we augment this representation to incorporate information from the
elaboration tree.

The trees in \cite{collins2003head} and related work consist of nodes
representing syntactic constituents, labeled by Penn Treebank symbols
\citep{marcus1993building}.  Each node has a special child, called the
{\em head}, which is assumed to define the main content of the phrase.
For example, verb phrases (VPs) typically have a verb as their head; noun
phrases have nouns or perhaps other noun phrases; etc.  A path from
any constituent to the word level, called a {\em spike}, 
can always be defined by iteratively descending to the head child 
until a single word is reached.  Each node in the tree carries with it
the identities of the word and part-of-speech tag at the end of its
respective spike.  The non-head children of a phrase node are called
{\em modifiers}, and represent the top of a new spike.  Proceeding
from the root to the leaves, there are three types of productions, or
{\em events}:
(1) {\em root events}, in which the top level constituent label,
along with its associated head word and tag, is generated, (2) {\em
  unary events}, in which a head child acquires a label (inheriting
the word and tag from the parent), and (3) {\em modifier events}, in
which children are generated to the left and right of the head,
receiving a label, tag and word (the latter two of which are passed to
its own head child).  Eventually, each spike of unary events generates
a part-of-speech tag as the label, which terminates the spike.

[SHOW AN EXAMPLE]

We extend this basic model by adding semantic content from the
elaboration tree to each node, in addition to its label, tag, word
triple.  We assume each node is associated with zero or one node from
the elaboration tree; that is, that there is a function from the set
of nodes in $\Lambda$ to the set of nodes in $\Psi$ plus a {\em null
  elaboration} node.  Further, we make a continuity assumption, which
requires that every subtree in $\Lambda$ maps to a subtree
in $\Psi$ (or to the null elaboration), 
and we assume that the root of $\Lambda$ maps to the root of $\Psi$.
As a result of these assumptions, $\Lambda$ together with the set of
associations can be generated by specifying, for each child node in
$\Lambda$, whether its associated node in $\Psi$ will be (1) the null
node, (2) the same as its parent, or (3) a child of the semantic
node associated with its parent.  We call these decisions {\em
semantic step events}, and distinguish two types: {\em head semantic
steps}, which determine the semantic association of head children,
and {\em modifier semantic steps}, which determine the association of
non-head children.

\end{document}

