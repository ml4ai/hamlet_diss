\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{natbib}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage[font=footnotesize]{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes}
\usepackage{qtree}

\newcommand{\abs}[1]{\lvert #1 \rvert}

\DeclareMathOperator{\Gamm}{\mathcal{G}}
\DeclareMathOperator{\Stick}{Stick}
\DeclareMathOperator{\Dir}{Dir}
\DeclareMathOperator{\Mult}{Multinomial}

\title{Unsupervised Bayesian Induction of Spatial Relations from Captioned Scenes: A Dissertation Proposal (in progress)}
\author{Colin Reimer Dawson}
\date{\today}

\begin{document}
\maketitle

\tikzstyle{altblock} = [rectangle, draw, fill=gray!15, 
    text width=5em, text centered, rounded corners=6pt, minimum height=2em]
\tikzstyle{line} = [draw]
\tikzstyle{arrow} = [draw, -latex']
\tikzstyle{lambdabox} = [rectangle, draw, fill=gray!0, text width=11em, text centered]

\section{Overview}
\label{sec:overview}

The purpose of this dissertation is the development of a probabilistic generative model, and associated inference and learning algorithms, to model the recovery of semantic scene representations from the conjunction of natural language text (captions or narratives) and visual data (images or videos).  This enterprise can be seen as lying in the realm of {\it grounded semantics}: given some text, the goal is to recover a semantic representation which can be connected to the physical environment.  However, the connection between textual and visual information is not one-directional: the presence of the visual scene constrains the meaning of the text, but the text also constrains the interpretation of the raw visual signal.

The specific focus of the project is {\it relations} among {\it objects}.  Starting with an image that is equipped with a human-generated caption, the inferential goals are to determine (1) what entities are in the scene (e.g., people, vehicles, furniture), (2) the features of those entities (size, color, category), (3) what entities are parts of/attached to/supported by/contained in what other entities, and (4) what/who is located where in relation to what/whom?  I will adopt the approach that the representations that support answering these what/where questions are neither intrinsically visual nor intrinsically linguistic, but are abstract semantic propositions whose meanings are determined by (a) their (probabilistic) grounding in physical scenes, and (b) the kinds of utterances they (probabilistically) produce.  For example, in a scene containing a blue couch and an end table, a possible semantic representation is that there is a {\sc couch} object that can be represented as a rectangular prism with major and minor axes, by virtue of which it has two {\sc end} features, and a small table, which is {\sc Near-To} one of the {\sc ends} of the {\sc couch}.

The formal semantic representation is discussed in \ref{sec:sem-repr}, along with the form of a prior distribution over semantics.

Conditioned on this semantic representation, a 3D (``minds eye'') scene representation is generated by sampling physical parameters for the objects (e.g., length, width, height, average RGB value), as well as $(x,z)$ coordinates on the ground plane.  The minds-eye representation is projected onto an image plane to produce the 2D image.  These representations and their corresponding priors and likelihoods are discussed in Sec. \ref{sec:vis-repr}.  Independently, a caption is generated by sampling a syntactic structure, which is then filled in with words.  A candidate source of syntactic structure comes from the collapsed typed dependency structure of the Stanford parser \citep{mcdm08b}.  Here, the syntactic representation of a sentence consists of labeled binary relations among words in the sentence.  This representation, along with priors and likelihoods, are discussed in Sec. \ref{sec:ling-repr}.

By performing simultaneous inference over several scene-caption pairs, the model will be able to learn a ``vocabulary'' of relations by parsimoniously explaining patterns of cooccurrence between features of the ``minds eye'' representation of the scene (e.g., distances between objects, relative orientations with respect to the perspective axis, etc.), and features of the syntactic structures.

\section{Related Work}
\label{sec:related-work}

The proposed model is informed by two hitherto largely separate literatures: one on language understanding, and the other on scene understanding.  To my knowledge, few published papers exist that attempt to take advantage of images and text as synergistic and complementary sources of information about ``meaning'', and those that do \citep{barnard2001learning, barnard2003matching} mostly use individual word tags rather than natural language captions.

On the language understanding side, \cite{Tellex2011approaching} employ a graphical model 
to understand grounded spatial relationships from text, and 
\cite{Makalic2008probabilistic} present a probabilistic model
to infer ``instantiated concept graphs'' (ICGs) from speech by
performing ASR, parsing, relation induction, and finally grounding of
semantic arguments.  However, in both cases, the relational structure
is assumed to be available deterministically from a parse,
whereas here, relations are inferred by learning a probabilistic
mapping from training sentences that are grounded by images.  Moreover, whereas the
model in \cite{Tellex2011approaching} is trained on images annotated with relations, the present model will learn from training data that consists only of text and unannotated images.

On the scene understanding side, \cite{delpero2011sampling, delpero2012bayesian} develop probabilistic generative models that populate a room with furniture which is modeled with connected parallelepipeds \citep{schlecht2009learning}.  Given a 2D image, a 3D room and furniture configuration is proposed from the posterior distribution by MCMC sampling.

The work in this dissertation builds on previous work in both computer vision and grounded semantics.  \cite{dawson2013spatial} developed a ``toy'' model for learning to interpret spatial utterances in the context of a simple tabletop scene on which a small nubmer of objects are arranged.  This work was limited, however, by several simplifying assumptions: first, the representation of the scene itself was assumed known, and utterances were assumed to be ``about'' a single object in view or a single location in space.  Second, utterances were assumed to have a single, fixed parse, which was coerced into a specific ``relation-landmark'' form.  Finally, a fixed ``vocabulary'' of spatial relations was defined {\it a priori}, with fixed groundings in space.

\section{Summary of Novelty}

In the present work, I will extend the spatial language model of \cite{dawson2013spatial} in several ways, and synthesize it with the scene understanding model of \cite{delpero2012bayesian}, resulting in a generative model resembling (at a high level) the one depicted in Fig. \ref{fig:generative-model}.
\begin{itemize}
\item I will draw on computer vision work by \cite{delpero2011sampling,delpero2012bayesian} by allowing uncertainty about what is in view in the first place, and where it is, to be integrated with uncertainty about the meaning of the language.  Together the uncertainty should be lower than that obtained using either side alone.
\item I will relax the previous assumption of \cite{dawson2013spatial} that sentences come pre-equipped with a parse tree, and instead incorporate the parse as a random variable in the generative model.  In the model in \ref{fig:generative-model}, the probability of a parse is determined by the semantic representation of the scene, as well as a scene-independent grammatical model such as that used by ``off-the-shelf'' parsers trained on large annotated corpora that probabilistically parse ``ungrounded'' utterances.
\item I will allow the model to learn sparse, but not one-to-one, dependencies between semantic attributes (e.g., the existence of a couch) and syntactic/lexical features (e.g. the {\tt subject} argument of the verb {\tt is}).  In generative terms, given a semantic representation, $\Psi$, a syntactic representation, $\Upsilon$, is generated, where $P(\Upsilon \vert \Psi)$ factors into terms corresponding to the nodes and edges of typed dependency trees (Fig. \ref{fig:typed-dependency-tree}).  A number of conditional independence properties will be assumed in this factorization.  First, conditioned on $\Psi$, subtrees of the dependency tree are assumed to be conditionally independent when all common ancestors are known.  Second, the distribution for a subtree is assumed to depend on only a small number of semantic properties.  Which properties are needed, however, is not assumed to be known {\it a priori}, but will be learned by clustering.  The details of this factorization are given in Sec. \ref{sec:ling-repr}.
\item Finally, I will attempt to allow the model to learn a language-specific relational ontology in an unsupervised manner by attempting to parsimoniously explain regularities in the relationship between visual and text data.  A simple example might be the distinction between tight and loose containment, which are distinguished lexically in Korean as 'kkita' and 'nehta', but which are both expressed using the preposition 'in' in English.  \cite{mcdonough2003understanding} found that English-speaking adults did not systematically distinguish visual examples of tight vs. loose containment in a categorization task, whereas Korean-speaking adults distinguished both.  A model that attempts to explain the words used to describe a scene should infer the existence of two relational categories when the distinction is expressed in the language, but should prefer one category if no linguistic distinction is made (assuming the visual distributions are the same in either case).
\end{itemize}

\section{Model and Representations}
\label{sec:model-overview}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{../img/GenerativeModel}
  \caption{A sketch of a generative model, expressed as a Bayes net, giving rise to scene-caption pairs.  Nodes inside the box correspond to scene-caption-specific variables, and are objects of inference from a particular scene-caption pair.  Nodes outside the box represent ``general knowledge'' about the language and/or scene domain, and are either specified {\it a priori} or are learned over many scene-caption pairs.}
  \label{fig:generative-model}
\end{figure}

\subsection{(Amodal) Semantic Representation}
\label{sec:sem-repr}

A sketch of a generative model for scene-caption pairs is shown in Fig. \ref{fig:generative-model}.  A scene has a ``topology'', $\Phi$, which contains a set of {\bf objects} with associated ontological categories (tables, chairs, people, clothing, cars, etc.), associated unary object {\bf methods} that return {\bf attributes} (e.g., category, subcategory, color, parent object), and binary propositional {\bf relations} between objects (e.g., proximity, relative orientation, containment).  Instances of binary location relations are {\sc left-of}, {\sc behind}, {\sc near}, and {\sc in}, though this vocabulary could be built up from primitive features.  Each relation has a focal object and a reference entity (in the case that a set of objects is the reference, the set is instantiated as a separate object which is the parent of its members).

For the example scene containing a room with an end table near a blue couch, the semantic representation might be as in Fig. \ref{fig:semantic-representation}.

\begin{figure}[t]
\centering

\begin{subfigure}{0.8\textwidth}
\begin{tikzpicture}
\node [lambdabox] (room) {
  \begin{minipage}{0.3\textwidth}
  \vspace{-0.2in}
  \begin{align*}
    \lambda_0 & \\
    \textsf{class:}& \textsc{room} \\
    \textsf{subclass:}& \textsc{living-room} \\
    \textsf{color:}&\textsc{null}
  \end{align*}
\end{minipage}
};
\node [lambdabox, below of=room, right of=room, node distance=3.5cm] (table) {
  \begin{minipage}{0.3\textwidth}
  \vspace{-0.2in}
  \begin{align*}
    \lambda_2 & \\
    \textsf{class:}& \textsc{table} \\
    \textsf{subclass:}& \textsc{small-square} \\
    \textsf{color:}&\textsc{null}
  \end{align*}
\end{minipage}
};
\node [lambdabox, below of=room, left of=room, node distance=3.5cm] (couch) {
  \begin{minipage}{0.3\textwidth}
  \vspace{-0.2in}
  \begin{align*}
    \lambda_1 & \\
    \textsf{class:}& \textsc{couch} \\
    \textsf{subclass:}& \textsc{standard} \\
    \textsf{color:}&\textsc{blue}
  \end{align*}
\end{minipage}
};
\path [arrow] (couch) -- node[auto,above,left] {$\rho_2$} (room);
\path [arrow] (table) -- node[auto,above,left] {$\rho_3$} (room);
\path [arrow] (table) -- node[auto,above] {$\rho_1$} (couch);
\end{tikzpicture}
\caption{Example of object and attribute representation}
\end{subfigure}

\vspace{0.3in}

\begin{subfigure}{0.8\textwidth}
\begin{tikzpicture}
\node [lambdabox] (near-to) {
  \begin{minipage}{0.3\textwidth}
  \vspace{-0.2in}
  \begin{align*}
    \rho_1 & \\
    \textsf{name:}& \textsc{near-to} \\
    \textsf{focus:}& \lambda_2 \\
    \textsf{referent:} & \lambda_1
  \end{align*}
\end{minipage}
};

\node [lambdabox, right of=near-to, node distance=5cm] (contained-by-1) {
  \begin{minipage}{0.3\textwidth}
  \vspace{-0.2in}
  \begin{align*}
    \rho_2 & \\
    \textsf{name:}& \textsc{contained-by} \\
    \textsf{focus:}& \lambda_1 \\
    \textsf{reference:}& \lambda_0
  \end{align*}
\end{minipage}
};

\node [lambdabox, right of=contained-by-1, node distance=5cm] (contained-by-2) {
  \begin{minipage}{0.3\textwidth}
  \vspace{-0.2in}
  \begin{align*}
    \rho_3 & \\
    \textsf{name:}& \textsc{contained-by} \\
    \textsf{focus:}& \lambda_2 \\
    \textsf{reference:}& \lambda_0
  \end{align*}
\end{minipage}
};
\end{tikzpicture}
\caption{Relation representation}
\end{subfigure}

\caption{Example semantic representation \label{fig:semantic-representation}}
\end{figure}

Constructing a prior over such representations is fairly straightforward.  I will attempt to define here a general model that assumes no prior knowledge about object or scene categories, but in practice the set of objects and scenes will most likely be assumed known for purposes of easy compatibility with the existing room model of \cite{delpero2012bayesian}.

First, each scene is populated with an initial object, $\lambda_0$, constituting the scene itself, and it is assigned a context class (in the example, this is a room).  To incorporate the ability to discover new scene types, the distribution $\boldsymbol{\pi}^{ctx}$ over context classes is represented by the stick-breaking process \citep{sethuraman1994constructive}.  We write $\boldsymbol{\pi}^{ctx} \sim \Stick(\alpha^{ctx})$, where the concentration parameter $\alpha^{ctx}$ represents the tendency to lump or split contexts.  If the set of contexts is instead pre-defined, a symmetric Dirichlet prior can be used.  Given the context class, $c$, a subclass $c_i$ may be sampled from another stick-breaking process, where again the concentration parameter represents the tendency to make fine-grained distinctions; however, I will henceforth assume that there is only one context level.

Then, given the scene context, a number of objects, $\lambda_1, \dots, \lambda_n$ is generated, where $n$ is drawn from a context-sensitive Poisson distribution with a parameter $\mu^{obj}_c$ drawn from a Gamma prior whose parameters are fixed.  (Alternatively, if the set of contexts is fixed, the $\mu^{obj}_c$ could simply be calibrated from annotated data)

Each object is then assigned a category.  I will define two models of object category: one where the set of objects is unknown, the other where the set is known in advance.

If the object categories are unknown, then the context-dependent distribution $\boldsymbol{\pi}^{cat}_c$ over object categories is a draw from a hierarchical Dirichlet Process (HDP) \citep{teh2006hierarchical} with concentration $\alpha^{cat}$ and base measure $\boldsymbol{\pi}^{cat}$.  In an HDP, $\boldsymbol{\pi}^{cat}$ is itself a draw from a DP with concentration $\alpha_0^{cat}$ and base measure $\boldsymbol{\pi}_0^{cat}$.  In this case, $\boldsymbol{\pi}_0^{cat}$ is drawn from a stick breaking process, $\Stick(\alpha_0^{cat})$.  This model has the desirable property that the number of object categories need not be defined {\it a priori}, but different contexts tend to include certain categories.  Here, $\alpha_0^{cat}$ governs the degree of lumping or splitting of object categories, whereas $\alpha^{cat}$ governs the extent to which the same objects appear across scene types: if $\alpha^{cat}$ is small, different contexts will tend to have distinct sets of objects; if it is large, contexts have similar object distributions.

Although the above description in principle allows for object types to be discovered, in practice we will, at least at first, assume a fixed set of $K$ object categories.  In this case, the hierarchy of Dirichlet Processes can be replaced by a hierarchy of Dirichlet distributions (i.e., $\boldsymbol{\pi}^{cat}_c \stackrel{i.i.d.}{\sim} \Dir(\alpha^{cat} \boldsymbol{\pi}^{cat})$ and $\boldsymbol{\pi}^{cat} \sim \Dir(\alpha_0^{cat} \boldsymbol{1} / K)$), but the qualitative properties are similar: $\alpha_0^{cat}$ governs the overall variability in object types, whereas $\alpha^{cat}$ represents the tendency of scene types to share object distributions.

For each object of category $o$, its attributes (subcategory, color label, parent object, etc.) are sampled from a category-specific distribution (e.g., $\lambda.\textsf{color} \sim \boldsymbol{\pi}^{col}_o$).  In the case of unknown object categories, these distributions (other than subcategory) are sampled from a parent measure, e.g., $\boldsymbol{\pi}^{col}$, which is drawn from a stick-breaking process, $\Stick(\alpha^{col})$; for known categories, the distributions might have independent {\it a priori} Dirichlet priors.

Finally, for each pair of objects, a location relation is generated by sampling from a distribution over relations, $\boldsymbol{\pi}^{rel}$, which in the case of a fixed vocabulary of relations has a Dirichlet prior, and in the case of unspecified relations is a sample from another stick-breaking process, $\Stick(\alpha^{rel})$.

\subsection{Visual Representation}
\label{sec:vis-repr}
Given the scene topology, $\Phi$, a 3D ``geometric'' representation, $Z$, of the objects in the scene is generated.  This includes specific numeric dimensions and locations of objects, as well as properties such as ``intrinsic'' color values in numeric color space.  This step can be thought of as the ``mind's eye'' representation of the scene.  Object- and relation-specific ``grounded semantics'' are represented by $\theta$, which contains information about, for example, typical dimensions of tables; how near is ${\sc near}$ when the arguments are cups on a table vs. furniture in a room, where are the continuous boundaries between ``tight'' and ``loose containment'' (if such a distinction is made at all), etc.

Some features of $Z$ are plausibly modeled as independent given $\Phi$ and $\theta$, such as the dimensions of individual tables, but the joint distribution over locations of objects in the scene is likely better handled by a Markov Random Field (MRF) than a directed representation, as multiple soft constraints must be satisfied simultaneously.  This seems more conducive to an ``energy minimization'' approach than to sequential generation.  If generation is needed, a stochastic simulator whose dynamics reflect the energy functions in the MRF could be used.  However, at least at first, much of this component of the model will be drawn directly from \cite{delpero2012bayesian}.

Given the camera, $C$, the 3D representation is projected onto the image plane, together with perturbations and noise arising from lighting, etc., to produce an image (or video) $X$.  In principle, aspects of the camera perspective (e.g. occlusion), and specifically visual features (e.g. lighting) are likely to influence the choice of semantic representation, $\Psi$, to be expressed in the caption; however, as a first approximation, $\psi$ and $Z$ are assumed to be conditionally independent given $\Phi$.

\begin{verbatim}
blue 
\end{verbatim}

\subsection{Linguistic Representation}
\label{sec:ling-repr}

Given a scene topology $\Phi$, a subset, $\Psi \subset \Phi$, of the available features are chosen to be expressed.  First a set of relations is chosen, and then for each relation argument, a subset of the features of each object argument is selected.  The distribution $p(\Psi \vert \Phi)$ is governed by considerations of salience (which arises, for example, by departures from prior expectations, as well as the behavioral significance of a given relation), descriptiveness (how well could the full scene be recovered from $\Psi$), and sparsity (a desire for descriptions to be concise).  More will be said about this in a future iteration.

Each sentence is assumed to be associated with a single semantic relation, represented by $\psi_0$.  A semantic feature-vector, $(\psi_0, \psi_1, \dots, \psi_M)$, can be constructed for the sentence by systematically stepping through the attributes of the arguments of $\psi_0$, some of which have the value {\tt omitted}, and others of which have the value {\tt null}.  By including these gaps, the semantic feature vector has fixed length, and for each fixed $m$, $\psi_m$ is the same feature across sentences\footnote{Eventually recursive semantics should be represented, where the argument of a relation can be another relation.  In this case the number of semantic features associated with a given sentence is potentially unbounded, but systematic ordering of the features should still be possible by, for example, leaving a series of gaps whenever an argument is not an object.}.  In the example, $\psi_0$ has the value {\sc Near-To}.  The next two features are the categories of the focus and referent; in this case $\psi_1 = \textsc{Table}$ and $\psi_2 = \textsc{Couch}$.  The next two features are the subcategory and color of the table; etc.

Given a semantic feature vector (henceforth labeled $\Psi$ by a slight abuse of notation), the speaker generates a syntactic tree, $\Upsilon$.  The tree containins a hierarchically organized set of syntactic relations among lexical content.  In the Stanford typed dependency system \citep{mcdm08b}, syntactic relations roughly correspond to syntactic roles (subject-of, direct object-of) and function words (prepositions, conjunctions), whereas their arguments are ``content words'' (nouns, verbs, adjectives and adverbs).  For example, one parse of the sentence ``A small end table is near a blue couch'' contains the syntactic relations:
\begin{itemize}
\item {\tt root(ROOT-0, is-5)}
\item {\tt nsubj(is-5, table-4)}
\item {\tt prep\_near(is-5, couch-9)}
\item {\tt amod(table-4, small-2)}
\item {\tt nn(table-4, end-3)}
\item {\tt amod(couch-13, blue-12)}
\end{itemize}
which can be represented as a tree as in Fig. \ref{fig:typed-dependency-tree}.
\begin{figure}[t]
\begin{center}
\begin{tikzpicture}[node distance = 3cm, auto,scale=0.75,transform shape]
\node [altblock] (is) {N1: is};
\node [altblock, left of=is, below of=is, node distance=3.5cm] (table) {N2: table};
\node [altblock, right of=is, below of=is, node distance=3.5cm] (couch) {N3: couch};
\node [altblock, below of=table, right of=table, node distance=2cm] (end) {N5: end};
\node [altblock, below of=table, left of=table, node distance=2cm] (small) {N4: small};
\node [altblock, below of=couch, node distance = 2cm] (blue) {N8: blue};
\path [arrow] (is) -- node[auto,above,left] {{\sf subj}} (table);
\path [arrow] (is) -- node[auto,above,right] {{\sf prep\_near}} (couch);
\path [arrow] (table) -- node[auto,right] {{\sf nn}} (end);
\path [arrow] (table) -- node[auto,left] {{\sf amod}} (small);
\path [arrow] (couch) -- node[auto,right] {{\sf amod}} (blue);
\end{tikzpicture}
\end{center}
\caption{Tree representation of collapsed typed dependencies induced by the Stanford parse of the sentence ``A small end table is near the left end of a blue couch.''} \label{fig:typed-dependency-tree}
\end{figure}

A tree with $J$ nodes can be decomposed into $arc$ features, $\Upsilon^{a} = (\upsilon_{1,1}^{a}, \dots, \upsilon_{1,n_1}^{a}, \dots \upsilon_{J,1}^{a}, \dots, \upsilon_{J,n_j}^{a})$, corresponding to the dependency labels between words, and $text$ features $\Upsilon^{t} = (\upsilon_1^{t}, \dots, \upsilon_J^{t})$ whose value is the word associated with each node.  In the example, we have
\begin{itemize}
\item $\upsilon_{1,\cdot}^{a} = (\textsf{subj}, \textsf{prep\_near})$
\item $\upsilon_1^{t} = \text{is}$
\item $\upsilon_{2,\cdot}^{a} = (\textsf{amod},\textsf{nn})$
\item $\upsilon_2^{t} = \text{table}$

$\dots$
\end{itemize}

For each dependency sequence, one dependency is identified as the {\it head}, an adaptation of  \cite{collins1997three,collins2003head} to dependency trees.  This arc is generated first.  Then, adjacent dependency arcs are generated outward in both directions until a value of {\tt stop} is reached.  The {\it syntactic history} for an arc is its parent word ({\tt ROOT} for the root), whether it is the head (0), left of the head (-1) or right of the head (1), and its preceding (toward the head) sibling arc.  The syntactic history for a text feature is its incoming arc, its parent word, the adjacent siblings of its parent arc, and its sibling word in the direction of the head (equal to {\tt NULL} if it is in head position).  The syntactic history for the text feature associated with the $j$th node is denoted by $\mathbf{h}^t_{j}$; similarly the syntactic history of the $\ell$th arc descending from the $j$th node is denoted by $\mathbf{h}^a_{j\ell}$.

% In the example, the $text$ and $dependency$ features of the root node presumably depend on $\psi_0$ --- in this case that the root relation is an instance of {\sc Near-To}; the $text$ and $children$ features of N1 (``table'' and {\sf amod nn}, respectively) likely depend on the the {\sf subj} {\sf prep\_near} structure, as well as the category of the focus of $\psi_0$ (in this case a {\sc Table}); the text of N3 depends on the features of the {\sc Table} and the fact that the incoming dependency is an {\sf amod}; etc.

In terms of the generative model, the distribution $P(\Upsilon_i \vert \Psi_i)$, corresponding to the $i$th observation, factors into the probability of each feature $\upsilon_{ij\ell}^{a}$ and $\upsilon_{ij}^{t}$ conditioned on the {\it semantic context}, $\Psi$, and the {\it syntactic history}, $h^a_{ij\ell}$ and $h^t_{ij}$, respectively.  That is,
\begin{equation}
  \label{eq:1}
  P(\Upsilon_i \vert \Psi_i) = \prod_{j=1}^J \pi^{t}(\upsilon^{t}_{ij} \vert h^t_{ij}, \Psi_i) \prod_{\ell=1}^{n_j} \pi^{a}(\upsilon^{a}_{ij\ell} \vert h^a_{ij\ell}, \Psi_{i}) 
\end{equation}

Due to the high dimensionality of both semantic context and syntactic history, there will be very little data available for any particular combination.  This is a familiar problem for parsing models that attempt to condition production probabilities on more than very local context (even very local context can result in data sparsity problems when individual words are involved).  A standard approach is to start with maximum likelihood estimates (MLEs) of full conditional probabilities and then incorporate some form of smoothing, where probability mass is borrowed from similar contexts (for a review of canonical smoothing methods, see \cite{chen1999empirical}).  The parser of \cite{collins2003head} uses successive {\it back-off interpolation} steps.  Abstractly, if $f$ is a syntactic feature to be generated in a context $\mathbf{c} = (c_1, c_2, \dots, c_n)$, then back-off smoothing estimates the probability $p(f \vert c_1, \dots, c_n)$ by first ordering the context features in perceived decreasing order of importance, and recursively interpolating.  Let $\hat{p}$ the target estimate, let $\tilde{p}$ represent the MLE, and let $c_0$ be the trivial context.  Collins recursively defines
\begin{equation}
  \label{eq:3}
  \hat{p}(f \vert c_1, \dots, c_{n-k}) = \lambda_k \tilde{p}(f \vert c_1, \dots, c_{n-k}) + (1 - \lambda_k) \hat{p}(f \vert c_1, \dots, c_{n-k-1}), \quad k = 0, \dots, n-2
\end{equation}
where $\hat{p}(f \vert c_1) = \lambda_{n-1} \tilde{p}(f) + (1 - \lambda_{n-1}) \varepsilon$ for a small constant $\varepsilon$.  The smoothing weights, $\lambda_1, \dots, \lambda_{n-1}$ are context-dependent, giving greater weight to contexts that have been observed frequently in training, but giving lower weight to contexts in which many values of $f$ were observed with low frequency (intuitively, this is the situation that requires more smoothing, since there are likely many more rare values of $f$ that were not observed).

The back-off interpolation method of \cite{collins2003head} has many desireable properties, and could be easily ``made Bayesian'' through the use of a multilevel HDP model. Let $p_{\mathbf{c}_{k}} \stackrel{i.i.d.}{\sim} DP(\alpha_{\mathbf{c}_{k-1}}, p_{\mathbf{c}_{k-1}})$ for $k=1,\dots,n$, where $p_{\mathbf{c}_n}$ represents the fully contextualized distribution, $p_{\mathbf{c}_{n-1}}$ represents the distribution with the last context feature deleted, $p_{\mathbf{c}_{0}} \sim \Stick(\alpha')$, and the concentrations have Gamma hyperpriors.  In this model, context distributions are pulled toward a parent distribution with one fewer context feature, with the degree of smoothing at each level governed by a context-dependent concentration parameter.  The $\alpha$s play an analogous role to $1 - \lambda$ in \cite{collins2003head}, and by placing Gamma priors on their values, contexts with high diversity of productions yield higher posterior $\alpha$s, and hence more smoothing.

However, a problem with this model is that it requires context features to be ordered {\it a priori}, which is not easily done when the context includes a mix of semantic and syntactic features.  Moreover, the precedence order of semantic features is different depending on the particular syntactic history.  For example, at the root of the tree, relation features matter most, and color features matter hardly at all; whereas the reverse might be true following an {\tt amod} arc.  Hence, if relation features take precedence, there is no mechanism to enforce similarity of productions that share landmark features if their relations differ; and vice-versa.  Ultimately, backing off of semantic features in a fixed order is not reasonable.

Instead of a hierarchical context model, I will assume that each syntactic feature is generated from an infinite mixture of discrete measures, where the mixing weights are informed by the context.  Let $\pi^{f}$ be the distribution over syntactic feature $f$ (where $f \in \{a,t\}$), and for generality, index instances by $j \in 1, \dots, J$, and define the {\em covariate vector}, $x_{j} = (h_{j}, \Psi_j)$.  Then assume
\begin{equation}
  \label{eq:4}
  \boldsymbol{\pi}^{f} \vert x_{j} = \sum_{k=1}^\infty w_{k}(x_j) \pi^{f}_k
\end{equation}
where the $\pi^{f}_k$ are multinomial distributions over the values of syntactic feature $f$, and $w(x_j)) = (w_1(x_j), w_2(x_j), \dots)$ represents the covariate-dependent mixing weights.

By defining
\begin{align}
  \label{eq:10}
  \pi^f_k &\sim Dir(\alpha^f\pi^f_0) \\
  \pi^f_0 &\sim Dir(\gamma^f W^f) \\
  \alpha^f &\sim \Gamm(a^f_\alpha, b^f_\alpha) \\
  \gamma^f &\sim \Gamm(a^f_\gamma, b^f_\gamma) \\
\end{align}
where $\alpha^f$ is a concentration parameter governing the similarity of the $\pi_k$ to each other (small values result in peaked components), $\pi^f_0$ represents the marginal distribution of syntactic features (aggregating across mixture components), and $\gamma^f$ represents the prior degree of uniformity in the distribution of feature $f$, and $W_f$ is a prior distribution over values of feature $f$.  The $a$ and $b$ hyperparameters are fixed.

By introducing indicator variables, $\mathbf{z} = \{z_{j} \in \mathbb{N}\}$, one for each feature instance the weight vector $w$ can be thought of as a covariate-dependent prior on partitions of the $j$s in the data set.  The goal is to define the prior on partitions in such a way that similar covariate vectors are grouped together with high probability; that is, if $x_{j}$ and $x_{j'}$ are ``close'', then $w(x_{j})$ and $w(x_{j'})$ should be ``close'' (as distributions) as well.

One attractive possibility comes from the PPMx model of \cite{muller2011product}, who introduce a probability model for covariates conditional on cluster membership.  The distribution of partitions conditioned on the covariates is then proportional to the product of a prior {\it cohesion function}, which depends only on cluster sizes, via a Dirichlet Process, and a {\it similarity function}, which gives a likelihood over partitions given the within-cluster distribution of covariates.  \cite{quintana2012cluster} introduce a method for variable-selection within this model, allowing different clusters to depend on different covariates.

While this model works well when the covariates are observed, in the present case some of them are latent.  This presents a problem for the PPMx model, because the normalization factor for the covariate-conditioned partition distribution is no longer constant, as it depends on the overall distribution of covariates in the entire sample.  Moreover, although the normalization is a finite sum, it is a sum over all possible partitions of the data, the number of which for $n$ observations is given by the $n$th Bell number, which grows combinatorially in $n$.

Hence, it is desirable to model $p(\mathbf{z} \vert \mathbf{x})$ directly, rather than as the inversion of $p(\mathbf{x} \vert \mathbf{z})$.  One possibility comes from \cite{dahl2008distance}.  Like the PPMx model, his approach begins with a Dirichlet Process prior over partitions, which can be modeled as the stationary distribution of the Polya urn model, in which observations are successively assigned to clusters in proportion to the number already there, and to a new cluster in proportion to a concentration parameter, $\alpha$.  Dahl's approach modifies the Polya urn to take covariates into account by incorporating a {\em distance function} between observations in covariate space.  Let $d_{ii'} = d(x_i, x_{i'})$ represent a distance between two covariate vectors, and let $d^{\max} = \max_{i'} d_{ii'}$.  The similarity measure between an observation and a cluster is then given by $g_i(R_k) = c_i \sum_{j \in R_k} (d^{\max}_{i,k} - d_{ii'})$, with the constant chosen so that $\sum_{k} g(x_i, R_k)$ equals the number of observations.  

For a given concentration parameter $\alpha$, this normalization renders the probability of assigning an observation to a new cluster independent of the covariates.  This is undesirable if a new observation's dissimilarity on covariates suggests {\em a priori} that it likely belongs to a new cluster.  \cite{blei2011distance} relax the normalization assumption and instead define the similarity measure on an absolute scale relative to $\alpha$, giving rise to the Distance-Dependent Chinese Restaurant Process ($ddCRP$).  Unlike in the traditional CRP, where customers are assigned to tables, in the ddCRP, customers are linked to other customers (in proportion to their similarity), resulting in indirect assignment of tables by identifying connected components of the customer graph.

Here I use a hybrid between these two approaches, retaining the ``table-assignment'' structure of \cite{dahl2008distance}'s model, but using the absolute similarity scale of \cite{blei2011distance}.

Consider first the case of a single categorical covariate, and let $d_{ii'} = 1 - \delta_{x_i,x_{i'}}$, where $\delta$ is the Kronecker delta function.  Then, $g_i(R_k) = \sum_{i' \in R_k} \left(1 - (1 - \delta_{x_i,x_{i'}})\right) = \sum_{i' \in R_k} \delta_{x_i,x_{i'}}$.  That is, $g_i$ is proportional to the number of observations already in cluster $k$ such that $x_{i'} = x_i$.

This is easily extended to multiple categorical covariates, $x_i = (x_{i1},\dots,x_{iM})$, by letting $d_{ii'}$ be a function of the $L^p$ norm of the binary vector of position-wise differences.  For example, if $d_{ii'} = \frac{1}{M} \sum_{m=1}^M (1 - \delta_{x_{im},x_{i'm}})$, then the distance is the proportion of the covariate features that differ, and $g_i(R_k) = \frac{1}{M} \sum_{i' \in R_k} \sum_{m=1}^M \delta_{x_{im},x_{i'm}}$ is the total number of matching feature values in a cluster normalized by the number of features per observation.  This corresponds to the $L_0$ (Hamming) distance between categorical vectors, normalized by dimension.  

The above is easily generalized to other types of covariates by replacing $\delta$ with an arbitrary similarity measure ranging from 0 to 1 (for example, an exponentially decaying function of an arbitrary distance metric).  In the present application, it is important that different clusters be allowed to depend on different covariates, as discussed above.  Hence it is necessary that the similarity measure be allowed to depend on cluster-specific parameters.  This can be accomplished by computing coordinate-wise similarities as desired (ranging from 0 to 1), and defining overall similarity as a weighted average of the coordinate-wise similarities, where the weights are allowed to vary across clusters.  Note that the assumption that similarities range from 0 to 1 is not restrictive, since any desired rescaling can be incorporated into the weights.

Let $\pi^{sim}_k = (\pi^{sim}_{k1},\dots,\pi^{sim}_{km})$ be a probability vector, and define
\begin{equation}
g_{ik}(R_k) = \sum_{i' \in R_k} \sum_{m=1}^M w_{km}^{sim} \delta_{x_{im},x_{i'm}}
\end{equation}

Then, for cluster $k$, similarity of the $m$th covariate contributes to the overall similarity in proportion to $\pi^{sim}_{km}$.  The $\pi^{sim}_k$ have a common Dirichlet prior with concentration $\alpha^{sim}$ and base measure $w_0^{sim}$, which in turn has a Dirichlet prior with concentration $\gamma^{sim}$ and symmetric base measure\footnote{There is no additional difficulty in using a non-symmetric base measure here if there is prior information about the relative importance of the covariates}.  The interpretation of $w_0^{sim}$ is as the vector of mean weights for each covariate across clusters, with $\alpha^{sim}$ determining the similarity of weights across clusters and $\gamma^{sim}$ determining the similarity of mean weights across features.

To summarize, given a configuration with $K$ existing components, we have
\begin{align}
  \label{eq:2}
  p(z_{ij} \vert z_{-ij}, x_{i}, \mathbf{w}) &\propto
  \begin{cases}
    \sum_{i' \in R_k} \sum_{m=1}^M w_{km}^{sim} \delta_{x_{im},x_{i'm}} & z_{ij} = k \\
    \alpha^{cl} & z_{ij} = K+1
  \end{cases} \\
  \pi^{sim}_k \vert \alpha^{sim}, w_0^{sim} &\sim Dir(\alpha^{sim}w_0^{sim}) \\
  w_0^{sim} \vert \gamma^{sim} &\sim Dir(\alpha^{sim} \mathbf{1}/M) \\
  \alpha^{sim} &\sim \Gamm(a_{\alpha^{sim}}, b_{\alpha^{sim}}) \\
  \gamma^{sim} &\sim \Gamm(a_{\gamma^{sim}},b_{\gamma^{sim}})
\end{align}

In order to encourage sparsity of the weight vectors, $\alpha^{sim}$ should be less than 1 with high prior probability.  This yields weight vectors with mass concentrated in a few components.  The prior on $\gamma^{sim}$ is determined by the extent to which covariates are believed to be equally important {\em a priori}.

Letting $\upsilon^{f*}_k = \{\upsilon^f_j: j \in R_k\}$, where $f \in \{a,t\}$, letting $s = 1, \dots, S^f$ index distinct values of syntactic feature $f$, and writing $n^f_{k,s}$ for the number of times feature $f$ takes the value $s$ in cluster $k$, we have the likelihood
\begin{align}
  \label{eq:11}
  p(\boldsymbol{\upsilon}^f \vert \mathbf{z}, \pi^f_0, \dots, \pi^f_K) &= \prod_{k=1}^K p(\upsilon^*_k \vert \pi^{f}_k) \\
  &= \prod_{k=1}^K \prod_{i \in R_k} \pi^f_k(\upsilon^*_{ki}) \\
  &= \prod_{k=1}^K \prod_{s=1}^{S^f} (\pi^f_{k,s})^{n^f_{k,s}} \\
  \pi^f_k &\sim Dir(\alpha^f\pi^f_0) \\
  \pi^f_0 &\sim Dir(\gamma\mathbf{1}/S^f)
\end{align}

\section{Posterior Inference}
\label{sec:inference}

Given a scene-caption pair, $(S, X)$ % (or, a triple, $(S, X, Y)$ if a bottom-up parser is being used to generate a score)
, the goal of inference is to recover the scene topology, $\Phi$, and perhaps additionally the ``highlighted'' aspects of the scene, $\Psi$.  Since the model yields a posterior distribution over these variables and not a single value, it is necessary to define an objective function, an optimum of which is the desired output of ``perception''.  A sensible solution is to define a loss function, $L(\hat{\Phi}, \hat{\Psi}, \Phi, \Psi)$, where estimates are penalized according to their propositional deviations (some version of ``edit distance'') from the truth: for example, there would be some cost associated with failing to represent an object, another cost associated with hallucinating an object, and other (presumably smaller) costs associated with misrepresenting the features or (propositional) locations of objects.  Misrepresenting objects and relations highlighted by $\Psi$ could have a higher cost, depending on the inference objective.\footnote{As an alternative to this ``edit distance''-based loss function, if the semantic interpretation of the scene leads to some action (perhaps based on ``upstream'' higher-level inference), then the specification of the loss function could be deferred to a more abstract variable on which a decision is based.}

The optimal solution would then minimize the posterior risk, 
\begin{equation*}
(\Phi^*, \Psi^*) = \arg\min_{(\hat{\Phi}, \hat{\Psi})} r(\hat{\Phi}, \hat{\psi} \vert S,X,Y) = \mathbb{E}_{\hat{\phi},\hat{\psi}}[L(\hat{\phi}, \hat{\psi}, \phi, \psi)]
\end{equation*}
where the expectation is taken with respect to the posterior distribution $P(\Phi, \Psi \vert S,X,Y)$, itself approximated with a set of representative hypotheses sampled using Markov Chain Monte Carlo.

The overall MCMC algorithm is Gibbs sampling, with MH acceptance-rejection steps in some blocks.  Apart from an additional likelihood term corresponding to the prior on relations given the mind's eye, the sampling scheme from \cite{delpero2012bayesian} can be used largely unchanged.

Next I give a Gibbs sampling algorithm for the caption/semantics side of the model.

\subsection{Sampling Cluster Indicators}

Recall that we model the distribution of syntactic feature $\upsilon_{ij}$ in the context of semantic representation $\Psi_i = (\psi_{i1}, \dots, \psi_{iM})$ and syntactic history $h_{ij} = (h_{i,j,1}, \dots, h_{i,j,H_f})$ as an infinite mixture of multinomials:
\begin{equation}
  \label{eq:13}
  p(\upsilon^f_{ij} \vert \Psi_i, h_{ij}) = \sum_{k=1}^\infty w_{k}(\Psi_i, h_{ij}) \pi_k(\upsilon_{ij})
\end{equation}
Inference is simplified by introducing an indicator variable $z^{cl}_{ij}$ to represent which mixture component generated $\upsilon_{ij}$, yielding
\begin{equation}
  \label{eq:13}
  \pi(\upsilon^f_{ij} \vert \Psi_i, h_{ij}) = \sum_{k=1}^\infty p(z^{cl}_{ij} = k \vert \Psi_i, h_{ij}) \pi_k(\upsilon_{ij})
\end{equation} By renumbering, let $j$ range over all $i,j$ combinations, so that $z^{cl}_{ij}$ becomes $z^{cl}_j$.  Let $x_{j} = (x_{j,1}, \dots, x_{j,M})$ represent the combined context (semantic context and syntactic history features together).  Then the assignment of observations to mixture components (``clusters'') is modeled using a modified Polya urn:
\begin{align}
p(z^{cl}_{j} \vert z^{cl}_{-j}, x_{j}, \mathbf{w}) &\propto
  \begin{cases}
    \sum_{j' \in R_k} \sum_{m=1}^M w_{km}^{sim} \delta_{x_{jm},x_{j'm}} & z^{cl}_{ij} = k \\
    \alpha^{cl} & z^{cl}_{ij} = K+1
  \end{cases} \\
\end{align}
where $\delta$ is a similarity measure between $x_{i,m}$ and $x_{i',m}$ assumed to range between 0 and 1 (e.g., the Kronecker delta), and $\pi^{sim}_k$ is the weight vector governing which context features matter for determining membership in cluster $k$.  The $\pi^{sim}_k$ have a two-level hierarchical Dirichlet prior with a fixed, symmetric base measure at the top level, and base measure $\pi^{sim}_0$ at the lower level, and respective concentrations $\alpha^{sim}$ and $\gamma^{sim}$.  By introducing an additional set of indicator variables, $\mathbf{z}^{sim}$ ranging over features $1, \dots, M$ with $z^{sim}_j \vert z^{cl}_j \sim \pi^{sim}_{z_j}$, the above simplifies to
\begin{align}
  \label{eq:5}
  p(z^{cl}_{j} \vert z^{cl}_{-j}, \mathbf{z}^{sim}_{-j}, x_{j}) &\propto
  \begin{cases}
    \sum_{j' \in R_k} \delta_{x_{jz^{sim}_{j'}},x_{j'z^{sim}_{j'}}} & z^{cl}_{j} = k \\
    \alpha^{cl} & z^{cl}_{j} = K+1
  \end{cases}
\end{align}
The distribution of a single $z^{sim}_j$ given only a cluster assignment $z^{cl}_j$ and the other $z^{sim}_{j'}$ within the cluster, marginalizing out $\pi^{sim}_k$ and $\pi^{sim}_0$, can be computed using the Dirichlet integral.  First, integrating out $\pi^{sim}_k$, writing $n^{sim}_{mk} = \sum_{j' in R_k} I(z^{sim}_{j'} = m)$ for the number of times feature $m$ is selected in cluster $k$, $n^{sim}_{\cdot k} = \sum_{m=1}^M n^{sim}_{mk}$ and $n^{sim}_{m\cdot} = \sum_{k=1}^K n^{sim}_{mk}$,
\begin{align}
  \label{eq:6}
  p(z^{sim}_{j}  = m \vert z^{cl}_{j}, \mathbf{z}^{sim}_{-j}, x_{j}, \alpha^{sim}, \pi^{sim}_0) = \frac{n^{sim}_{mz^{cl}_j} + \alpha^{sim} \pi^{sim}_{0,m}}{n^{sim}_{\cdot z^{cl}_j} + \alpha^{sim}}.
\end{align}
This expression can be understood as the sum of two components using the Chinese Restaurant Franchise metaphor \cite{teh2006hierarchical}.  Here, observations are ``customers'', $z^{sim}$ values are ``dishes'', and mixture components are ``restaurants''.  Given that there are $n^{sim}_{mk}$ customers in restaurant $k$ already assigned to tables eating dish $m$, the next customer sits at one of those tables with probability proportional to $n^{sim}_{mk}$, and at a new table with probability proportional to $\alpha^{sim}$.  If the customer starts a new table, she orders a dish from the global menu with probabilities drawn from $\pi_0$, and orders dish $m$ with probability $\pi_{0,m}$.  Hence there are two ways to get dish $m$: either by joining an existing table eating that dish, or by ordering it anew from the global menu.  Combining these two probabilities gives the expression in \ref{eq:6}.

To integrate out $\pi^{sim}_0$, it is necessary to distinguish which ``path'' was taken to each dish: that is, to keep track of how many distinct tables (and not just customers) there are eating each dish.  To see why, note that $\pi^{sim}_0$ only generates a new observation when a new table is created, and so only tables are evidence for mass at a particular location of $\pi^{sim}_0$ (see \cite{wallach2009rethinking} for a detailed derivation).

Hence, we introduce another set of indicators, $\tau^{sim}_j$, indexing the table number of customer $j$, where tables are indexed globally by $t = 1, \dots T$.  Given $\mathbf{z}^{sim}_{-j}$, $z^{cl}_j$, and $\boldsymbol{\tau}^{sim}_{-j}$, $\tau^{sim}$ is assigned to an existing value, $t$, in cluster $z^{cl}_j$ in proportion to the number of observations in that cluster already assigned to that value, and to a new value with probability in proportion to $\alpha^{sim}$.  That is,
\begin{equation}
  \label{eq:7}
  p(\tau^{sim}_j \vert z^{cl}_{j}, \mathbf{z}^{sim}_{-j}, \boldsymbol{\tau}^{sim}_{-j}, \alpha^{sim}) = 
  \begin{cases}
    \frac{\#(\tau^{sim} = t \cap z^{cl}_j = k)}{n^{sim}_{\cdot z^{cl}_j} + \alpha^{sim}} & t \leq T \\
    \frac{\alpha^{sim}}{n^{sim}_{\cdot z^{cl}_j} + \alpha^{sim}} & t = T+1
  \end{cases}
\end{equation}

It is then possible to integrate out $\pi^{sim}_0$.  Let $\hat{n}^{sim}_{km}$ be the number of tables in cluster $k$ with value $m$, let $\hat{n}^{sim}_{m} = \sum_{k=1}^K \hat{n}^{sim}_{km}$ be the global number of tables with value $m$, and let $\hat{n}^{sim}_{\cdot} = \sum_{t=1}^T \hat{n}^{sim}_{m}$.  Then we have (see \cite{wallach2009rethinking})
\begin{align}
  \label{eq:6}
  p(z^{sim}_{j} = m \vert z^{cl}_{j}, \mathbf{z}^{sim}_{-j}, \boldsymbol{\tau}^{sim}_{-j}, x_{j}, \alpha^{sim}, \gamma^{sim}) &= \frac{n^{sim}_{mz^{cl}_j} + \alpha^{sim} \left(\frac{\hat{n}^{sim}_{m} + \frac{\gamma^{sim}}{M}}{\hat{n}^{sim}_{\cdot} + \gamma^{sim}}\right)}{n^{sim}_{\cdot z^{cl}_j} + \alpha_0^{sim}}
\end{align}

Finally, we need to take the observed $\upsilon$ into account.  The distribution of $\upsilon$ does not depend on $z^{sim}$, and so sampling $z^{sim}$ is just as in \eqref{eq:6}.  The conditional posterior of the $z^{cl}$ depends both on the prior in \eqref{eq:6} and the likelihood
\begin{equation}
  \label{eq:8}
  p(\upsilon_{j} \vert z^{cl}_j) = \pi_{z^{cl}_j}(\upsilon_j)
\end{equation}

Hence, conditioned on the $\pi_k$, we have
\begin{align}
p(z^{cl}_{j} \vert \upsilon_j, z^{cl}_{-j}, \mathbf{z}^{sim}_{-j}, x_{j}, \pi_0,\dots,\pi_K) &\propto
  \begin{cases}
    \pi_k(\upsilon_j) \sum_{j' \in R_k} \delta_{x_{jz^{sim}_{j'}},x_{j'z^{sim}_{j'}}} & z^{cl}_{j} = k \\
    \pi_0(\upsilon_j) \alpha^{cl} & z^{cl}_{j} = K+1
  \end{cases}
\end{align}

However, as before, we can integrate out the random measures.  Let $n^{f}_{sk} = \sum_{j \in R_k} I(\upsilon_j = s)$ be the number of times that feature $f$ takes the value $s$ in cluster $k$.  Let $n^{f}_{\cdot k} = \sum_{s=1}^{S^f} n^f_{sk}$, let $n^{f}_{s\cdot} = \sum_{k=1}^K n^f_{sk}$.  As before, let $\hat{n}^{f}_{ks}$ be the number of tables in cluster $k$ with value $s$, let $\hat{n}^{f}_{s} = \sum_{k=1}^K \hat{n}^{f}_{ks}$ be the global number of tables with value $m$, and let $\hat{n}^{f}_{\cdot} = \sum_{t=1}^T \hat{n}^{f}_{s}$.  Then

\begin{equation}
  \label{eq:9}
  p(\tau^{f}_j \vert z^{cl}_{j}, \mathbf{z}^{f}_{-j}, \boldsymbol{\tau}^{f}_{-j}, \alpha^{f}) = 
  \begin{cases}
    \frac{\#(\tau^{f} = t \cap z^{cl}_j = k)}{n^{f}_{\cdot z^{cl}_j} + \alpha^{f}} & t \leq T \\
    \frac{\alpha^{f}}{n^{f}_{\cdot z^{cl}_j} + \alpha^{f}} & t = T+1
  \end{cases}
\end{equation}

and
\begin{align}
p(z^{cl}_{j} \vert \upsilon_j, z^{cl}_{-j}, \boldsymbol{\tau}^f, \mathbf{z}^{sim}_{-j}, x_{j}) &\propto
  \begin{cases}
    \frac{n^f_{\upsilon_j,z^{cl}_j} + \alpha^f \frac{\hat{n}^f_{\cdot,\upsilon_j} + \frac{\gamma^f}{S^f}}{\hat{n}^f_{\cdot,\cdot} + \gamma^f}}{n^f_{\cdot,z^{cl}_j} + \alpha^f} \sum_{j' \in R_{z^{cl}_j}} \delta_{x_{jz^{sim}_{j'}},x_{j'z^{sim}_{j'}}} & z^{cl}_{j} \leq K \\
    \frac{n^f_{\upsilon_j,\cdot} + \frac{\gamma^f}{S^f}}{n^f_{\cdot,\cdot} + \gamma^f} \alpha^{cov} & z^{cl}_{j} = K+1
  \end{cases}
\end{align}

\subsection{Sampling Concentration Parameters}
There are several parameters representing concentrations of Dirichlet distributions or Dirichlet Processes.  The conditional posteriors for these parameters have simple forms, but are not exponential family distributions, and so exact sampling is not possible.  The following factors involve concentration parameters
\begin{align}
  \label{eq:12}
  \pi^f_k \vert \pi^f_0, \alpha^f &\sim Dir(\alpha^f \pi^f_0) \\
  \alpha^f &\sim \Gamm(a_{\alpha^f}, b_{\alpha^f}) \\
  \pi^f_0 \vert \gamma^f &\sim Dir(\gamma^f \mathbf{u}) \\
  \gamma^f &\sim \Gamm(a_{\gamma^f}, b_{\gamma^f}) \\
  \pi^{sim}_k \vert \pi^{sim}_0, \alpha^{sim} &\sim Dir(\alpha^{sim} \pi^{sim}_0) \\
  \alpha^{sim} &\sim \Gamm(a_{\alpha^{sim}}, b_{\alpha^{sim}}) \\
  \pi^{sim}_0 \vert \gamma^{sim} &\sim Dir(\gamma^{sim} \mathbf{u}) \\
  \gamma^{sim} &\sim \Gamm(a_{\gamma^{sim}}, b_{\gamma^{sim}}) \\
  \mathbf{z}^{cl} \vert \mathbf{x} &\sim CRP(\alpha^{cl})
\end{align}

\subsection{Sampling $\Psi$ and Auxiliary Parameters}
\label{sec:gibbs-sampler-psi}

The propositional semantics represented by $\Psi$ has in its Markov blanket (1) the parameters, $\theta$ governing the grounding of relations, (2) the mind's eye representation, $Z$, (3) the parse tree, $\Upsilon$ and (4) the set of auxiliary parameters governing the PPMx model.  $Z$ together with $\theta$ provides the prior on $\Psi$, while $\Upsilon$ and the auxiliary parameters (such as the indicators various $\alpha$s and their hyperparameters) determine the likelihood.  Let 
\begin{equation*}
\Psi = \left(\begin{array}{ccc} \psi_{10} & \dots & \psi_{1,M} \\ \vdots & \ddots & \vdots \\ \psi_{I,1} & \dots & \psi_{I,M} \end{array}\right)
\end{equation*}
be the matrix of semantic features, where $i$ indexes sentences and $m$ indexes features (with $m=0$ denoting the relation label, $m=1$ and $2$ denoting the category and color of the focus object, $m=3$ and $m=4$ denoting the category and color of the reference object).  Let
\begin{equation*}
\Upsilon^t_i = \left(\begin{array}{cccc} \upsilon^t_{i,1} & h^t_{i,1,1} & \dots & h^t_{i,1,H_t} \\ \vdots & \vdots & \ddots & \vdots \\ \upsilon^t_{i,J_i} & h^t_{i,J_i,1} & \dots & h^t_{i,J_i,H_t} \end{array} \right)
\end{equation*}
be the representation of the words features of sentence $i$, where $\nu^t_{i,j}$ is the $j$th word, and $h^t_{i,j,s}$ is the $s$th history feature of that word.  Let $\mathbf{z}^t_i = \left(z^t_{i,1}, \dots, z^t_{i,J}\right)$ be the vector of cluster indicators associated the word features of sentence $i$.  Similarly, let
\begin{equation*}
\Upsilon^a_i = \left(\begin{array}{cccc} \upsilon^a_{i,1,1} & h^a_{i,1,1,1} & \dots & h^a_{i,1,1,H_a} \\ \vdots & \vdots & \ddots & \vdots \\ \upsilon^a_{i,1,L_{i,1}} & h^a_{i,1,L_{i,1},1} & \dots & h^a_{i,1,L_{i,1},H_a} \\ \vdots & \vdots & \ddots & \vdots \\ \upsilon^a_{i,J_i,1} & h^a_{i,J_i,1,1} & \dots & h^a_{i,J_i,1,H_a} \\ \vdots & \vdots & \ddots & \vdots \\ \upsilon^a_{i,J_i, L_{i,J_i}} & h^a_{i,J_i,L_{i,J_i},1} & \dots & h^a_{i,J_i,L_{i,J_i},H_a} \end{array} \right)
\end{equation*}
be the representation of the arc features of sentence $i$, where $\nu^t_{i,j,\ell}$ is the $\ell$th arc emitted by the $j$th word, and $h^a_{i,j,\ell,s}$ is the $s$th history feature for that arc.  Finally, let $\mathbf{z}^a_i = \left(z^a_{i,1,1}, \dots, z^a_{i,1,n_{i,1}}, \dots, z^a_{i,J_i,1}, \dots, z^a_{i,J_i,L_{i,J_i}}\right)$ be the vector of cluster indicators associated with the arc features of sentence $i$.

Then, $p(\psi_{i,m} \vert \Psi_{-(i,m)}, \Upsilon, \mathbf{z}, \theta, Z_i)$ is proportional to
\begin{align}
 & p(z^t_{i\cdot} \vert \psi_{i,m}, \Psi_{-(i,m)}, \mathbf{z}^t_{-i}) p(z^a_{i\cdot} \vert \psi_{i,m}, \Psi_{-(i,m)}, \mathbf{z}^a_{-i}) p(\psi_{i,m} \vert \theta, Z_i)
\end{align}

% \section{Learning}
% \label{sec:learning}

% A final goal involves learning what relational distinctions are made in a particular (language and scene) context, as well as how particular relations are ground out in the 3D world, and what features of the arguments are relevant in modulating those distributions.  Here there are interesting ``model selection'' problems, as the choice is among theories with differing numbers of moving parts.  However, Bayesian inference offers a principled criterion for deciding among models of different dimensions, via estimation of marginal likelihoods (i.e., integrating out all mediating parameters between the theory and the data).  % This is easier said than done, of course, and presents a fruitful area of research.

\bibliographystyle{apalike}
\bibliography{../../bib/dissertation}

\end{document} 
