\relax 
\@writefile{toc}{\contentsline {chapter}{List of Tables}{9}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{kolter2011redd}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{10}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{rasmussen2000infinite}
\citation{teh2006hierarchical}
\citation{paisley2011discrete}
\@writefile{toc}{\contentsline {chapter}{Abstract}{11}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\citation{rasmussen2000infinite}
\citation{teh2006hierarchical}
\citation{paisley2011discrete}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\citation{box1987empirical}
\citation{rosenblatt1956remarks,parzen1962estimation}
\citation{wasserman2007nonparametric}
\@writefile{toc}{\contentsline {chapter}{\chapterline {1} \MakeUppercase  {Statistical Models and Model Selection} }{14}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:intro}{{1}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{14}}
\citation{lindsay1995mixture,mclachlan2004finite}
\citation{geman1984stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}An overview of the dissertation}{16}}
\newlabel{sec:an-overv-diss}{{1.1.1}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Model Selection}{17}}
\newlabel{sec:model-selection}{{1.2}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}The Likelihood Function and the Conservation of Explanatory Power}{17}}
\newlabel{sec:bayes-occams-razor}{{1.2.1}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}The Bias-Variance Tradeoff}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Example: Polynomial Regression}{20}}
\newlabel{sec:exampl-polyn-regr}{{1.2.3}{20}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:poly-linear}{{1.1a}{21}}
\newlabel{sub@fig:poly-linear}{{a}{21}}
\newlabel{fig:poly-quad}{{1.1b}{21}}
\newlabel{sub@fig:poly-quad}{{b}{21}}
\newlabel{fig:poly10}{{1.1c}{21}}
\newlabel{sub@fig:poly10}{{c}{21}}
\newlabel{fig:poly-errors}{{1.1d}{21}}
\newlabel{sub@fig:poly-errors}{{d}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Polynomials of varying degrees fit using ordinary least squares (equivalently, maximum likelihood estimation) to a set of eleven points generated from a fifth order polynomial with independent Normal residuals. 1.1a\hbox {}--1.1c\hbox {}: fits of polynomial order 1, 2 and 10. Filled circles are training data, unfilled circles are data not used during fitting. 1.1d\hbox {} squared prediction error in and out of the training sample for polynomial orders 1 through 10.\relax }}{21}}
\newlabel{fig:polynomials}{{1.1}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Balancing Fit and Complexity: Bayesian Occam's Razor}{22}}
\newlabel{sec:balanc-fit-compl}{{1.2.4}{22}}
\newlabel{eq:2}{{1.3}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Clustering via a Mixture of Gaussians Model}{24}}
\newlabel{sec:clustering}{{1.3}{24}}
\newlabel{eq:gmm}{{1.4}{24}}
\citation{geman1984stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Clustering Sequential Data: The Hidden Markov Model}{25}}
\citation{baum1966statistical}
\citation{rabiner1986introduction}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Model Selection in HMMs and the Mixture of Gaussians Model}{27}}
\newlabel{sec:model-select-mixt}{{1.5}{27}}
\citation{rosenblatt1956remarks,parzen1962estimation}
\@writefile{toc}{\contentsline {chapter}{\chapterline {2} \MakeUppercase  {Bayesian Nonparametric Models and the Infinite Hidden Markov Model} }{29}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:HMM-NPBayes}{{2}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Parametric Vs. Nonparametric Models}{29}}
\newlabel{sec:param-vs.-nonp}{{2.1}{29}}
\citation{epanechnikov1969non}
\citation{ferguson1973bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Dirichlet Process}{30}}
\newlabel{sec:dirichlet-process}{{2.2}{30}}
\citation{ferguson1973bayesian}
\newlabel{eq:4}{{2.2}{31}}
\citation{ferguson1973bayesian}
\citation{sethuraman1994constructive}
\citation{sethuraman1994constructive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Normalized Gamma Process representation of the DP}{32}}
\newlabel{sec:norm-gamma-proc}{{2.2.1}{32}}
\newlabel{eq:8}{{2.3}{32}}
\newlabel{eq:3}{{2.4}{32}}
\newlabel{eq:9}{{2.5}{32}}
\newlabel{eq:1}{{2.6}{32}}
\citation{sethuraman1994constructive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The Stick-Breaking Process construction of the DP}{33}}
\newlabel{sec:stick-break-proc}{{2.2.2}{33}}
\newlabel{stick-step-1}{{1}{33}}
\newlabel{stick-step-2}{{2}{33}}
\newlabel{stick-step-3}{{3}{33}}
\newlabel{eq:11}{{2.7}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}The Chinese Restaurant Process}{33}}
\newlabel{sec:chin-rest-proc}{{2.2.3}{33}}
\citation{teh2011dirichlet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}The Dirichlet Process Mixture Model}{34}}
\citation{fox2012tutorial}
\citation{blei2006variational}
\citation{kurihara2007collapsed}
\newlabel{eq:gmm-2}{{2.9}{35}}
\newlabel{eq:DP-prior}{{2.10}{35}}
\newlabel{eq:5}{{2.11}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Two Gibbs Samplers for DP Mixture Models}{35}}
\newlabel{sec:gibbs-sampler-dp}{{2.2.5}{35}}
\@writefile{toc}{\contentsline {paragraph}{Method 1: A Collapsed Sampler Based on the CRP}{36}}
\newlabel{eq:1}{{2.16}{37}}
\@writefile{toc}{\contentsline {paragraph}{Method 2: An uncollapsed sampler based on the Stick-Breaking Process}{37}}
\newlabel{eq:2}{{2.17}{38}}
\newlabel{eq:6}{{2.19}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}An Infinite State HMM}{38}}
\newlabel{sec:an-infinite-state}{{2.3}{38}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The Hierarchical Dirichlet Process}{39}}
\newlabel{sec:hier-dirichl-proc}{{2.3.1}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}The HDP-HMM}{39}}
\newlabel{sec:hdp-hmm}{{2.3.2}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}A Stick-Breaking Representation of the Aggregate Weights in an HDP}{41}}
\newlabel{stick-step-1}{{1}{41}}
\newlabel{stick-step-2}{{2}{41}}
\citation{teh2006hierarchical}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\citation{beal2001infinite}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}A Normalized Gamma Process Representation of the Weights in the HDP}{42}}
\newlabel{sec:norm-gamma-proc-1}{{2.3.4}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Adapting the HDP for an Infinite State HMM}{42}}
\citation{beal2003variational}
\citation{johnson2014stochastic}
\citation{fearnhead2003line,tripuraneni2015particle}
\citation{teh2006hierarchical,vangael2008beam,fox2008hdp,johnson2013bayesian}
\citation{teh2006hierarchical}
\citation{vangael2008beam}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Inference in Finite and Infinite State HMMs}{43}}
\newlabel{sec:inference-hdp-hmm}{{2.4}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}The Forward Backward Algorithm}{44}}
\newlabel{sec:forw-backw-algor-1}{{2.4.1}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}A Gibbs Sampler for the HDP-HMM}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Beam Sampling in the HDP-HMM}{44}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {chapter}{\chapterline {3}\MakeUppercase  {HaMMLeT: An infinite Hidden Markov Model with Local Transitions}}{45}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:HaMMLeT}{{3}{45}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Transition Dynamics in the HDP-HMM}{46}}
\newlabel{sec:transition-dynamics}{{3.1}{46}}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\newlabel{eq:1}{{3.3}{47}}
\newlabel{eq:4}{{3.4}{47}}
\newlabel{eq:5}{{3.5}{47}}
\newlabel{eq:6}{{3.6}{47}}
\citation{ferguson1973bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}An HDP-HMM With Local Transitions}{48}}
\@writefile{toc}{\contentsline {paragraph}{Notational Conventions}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}A Normalized Gamma Process representation of the HDP-HMM}{49}}
\newlabel{sec:normalized-gamma}{{3.2.1}{49}}
\newlabel{eq:17}{{3.7}{49}}
\newlabel{eq:18}{{3.8}{49}}
\newlabel{eq:16}{{3.9}{49}}
\newlabel{eq:19}{{3.10}{49}}
\newlabel{eq:20}{{3.11}{49}}
\newlabel{eq:21}{{3.12}{49}}
\newlabel{eq:22}{{3.13}{49}}
\newlabel{eq:23}{{3.15}{49}}
\newlabel{eq:50}{{3.16}{49}}
\newlabel{eq:24}{{3.17}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Promoting ``Local'' Transitions}{50}}
\newlabel{sec:prom-local-trans}{{3.2.2}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The HDP-HMM-LT as a continuous-time Markov Jump Process with ``failed'' jumps}{52}}
\newlabel{sec:dist-based-filt}{{3.3}{52}}
\newlabel{eq:beta}{{3.24}{52}}
\newlabel{eq:51}{{3.27}{53}}
\newlabel{eq:52}{{3.28}{53}}
\newlabel{eq:53}{{3.29}{53}}
\newlabel{eq:56}{{3.30}{54}}
\newlabel{eq:60}{{3.34}{54}}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\newlabel{eq:54}{{3.35}{55}}
\newlabel{eq:joint-likelihood}{{3.40}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}An HDP-HSMM-LT modification}{55}}
\newlabel{sec:an-hsmm-modification}{{3.3.1}{55}}
\citation{johnson2013bayesian}
\citation{johnson2013bayesian}
\newlabel{eq:95}{{3.41}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Summary}{57}}
\newlabel{sec:model-summary}{{3.3.2}{57}}
\newlabel{eq:96}{{3.43}{57}}
\newlabel{eq:likelihood}{{3.49}{57}}
\newlabel{eq:97}{{3.50}{57}}
\newlabel{eq:98}{{3.52}{57}}
\newlabel{eq:likelihood-hsmm}{{3.53}{57}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}MCMC Inference in the ``Failed Jumps'' Representation}{57}}
\newlabel{sec:inference}{{3.4}{57}}
\citation{ishwaran2000markov}
\newlabel{eq:28}{{3.54}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Sampling $\pi $, $\beta $, $\alpha $ and $\gamma $}{58}}
\newlabel{sec:sampling-pi}{{3.4.1}{58}}
\newlabel{eq:46}{{3.55}{58}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\pi $}{59}}
\newlabel{eq:47}{{3.56}{59}}
\newlabel{eq:61}{{3.57}{59}}
\newlabel{eq:24}{{3.59}{59}}
\newlabel{eq:25}{{3.60}{59}}
\newlabel{sec:sampling-bbeta}{{3.4.1}{59}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\beta $}{59}}
\newlabel{eq:62}{{3.61}{59}}
\citation{teh2006hierarchical}
\newlabel{eq:31}{{3.67}{60}}
\newlabel{eq:32}{{3.68}{60}}
\newlabel{eq:33}{{3.69}{61}}
\newlabel{eq:34}{{3.70}{61}}
\newlabel{eq:38}{{3.71}{61}}
\newlabel{sec:sampling-alpha}{{3.4.1}{61}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\alpha $ and $\gamma $}{61}}
\newlabel{eq:42}{{3.72}{61}}
\newlabel{eq:43}{{3.79}{62}}
\newlabel{eq:8}{{3.80}{62}}
\newlabel{eq:44}{{3.81}{62}}
\newlabel{eq:9}{{3.82}{62}}
\citation{johnson2013bayesian}
\newlabel{eq:10}{{3.84}{63}}
\newlabel{eq:11}{{3.85}{63}}
\newlabel{eq:18}{{3.86}{63}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{63}}
\newlabel{eq:100}{{3.87}{63}}
\newlabel{eq:46}{{3.88}{63}}
\newlabel{eq:64}{{3.89}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Sampling $z$ and the auxiliary variables}{63}}
\newlabel{sec:sampling-z_t}{{3.4.2}{63}}
\newlabel{eq:19}{{3.93}{64}}
\newlabel{eq:48}{{3.95}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Sampling state and emission parameters}{64}}
\newlabel{sec:sampling-eta}{{3.4.3}{64}}
\newlabel{eq:65}{{3.100}{64}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Use Cases}{65}}
\@writefile{toc}{\contentsline {chapter}{\chapterline {4}\MakeUppercase  {Binary Vector States: Speaker Diarization}}{67}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:cocktail-party}{{4}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Binary State Vectors}{67}}
\newlabel{eq:39}{{4.1}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Additional Inference Steps}{68}}
\newlabel{sec:sampling-eta}{{4.1.1}{68}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\theta $}{68}}
\newlabel{eq:1}{{4.2}{68}}
\newlabel{eq:68}{{4.3}{68}}
\newlabel{eq:70}{{4.4}{68}}
\newlabel{eq:71}{{4.5}{68}}
\newlabel{eq:72}{{4.6}{69}}
\newlabel{eq:74}{{4.7}{69}}
\newlabel{eq:73}{{4.8}{69}}
\newlabel{eq:91}{{4.9}{69}}
\newlabel{eq:77}{{4.10}{70}}
\newlabel{sec:sampling-bmu}{{4.1.1}{70}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\mu $}{70}}
\newlabel{eq:92}{{4.15}{70}}
\newlabel{eq:93}{{4.16}{70}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $W$ and $\Sigma $}{70}}
\newlabel{eq:78}{{4.17}{70}}
\newlabel{eq:79}{{4.18}{70}}
\citation{gilks1992adaptive}
\newlabel{eq:80}{{4.19}{71}}
\newlabel{sec:sampling-lambda}{{4.1.1}{71}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\lambda $}{71}}
\newlabel{eq:88}{{4.20}{71}}
\newlabel{eq:88}{{4.21}{71}}
\newlabel{eq:90}{{4.22}{71}}
\newlabel{eq:94}{{4.23}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Summary}{71}}
\newlabel{sec:summary}{{4.1.2}{71}}
\newlabel{eq:101}{{4.30}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}``Cocktail Party'' Data}{73}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Synthetic Data Without Local Transitions}{75}}
\newlabel{sec:synth-data-without}{{4.3}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces (a-b) Accuracy and F1 scores for the HDP-HMM-LT, standard HDP-HMM, and Binary Factorial HMM on the Cocktail Party Data. Metrics are averaged over 10 Gibbs runs on each model, with error bars representing a 99\% confidence interval for the mean per iteration. The first 100 iterations are not shown. (c) Learned similarity parameter, $\lambda $, for the LT model, (d) Number of distinct states used by HDP-HMM and HDP-HMM-LT. The first 100 iterations are excluded.\relax }}{76}}
\newlabel{fig:cocktail-results}{{4.1}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces (a-b) Accuracy and F1 for the three models on data generated from an HDP-HMM without local transitions, (c) Learned similarity parameter, $\lambda $, for the LT model, (d) Number of states used by the HDP-HMM and HDP-HMM-LT. The first 100 iterations are omitted.\relax }}{77}}
\newlabel{fig:synthetic-results}{{4.2}{77}}
\@writefile{toc}{\contentsline {chapter}{\chapterline {5}\MakeUppercase  {Categorical Vector States: Power Disaggregation}}{78}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:REDD}{{5}{78}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Generalizing to Categorical-Valued $\theta $}{78}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Priors and Representations in the Categorical State Variant}{79}}
\newlabel{sec:priors-repr-categ}{{5.2}{79}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Adapting Posterior Inference for Categorical State Vectors}{80}}
\newlabel{sec:adapt-post-infer}{{5.3}{80}}
\@writefile{toc}{\contentsline {subsubsection}{Sampling $\theta $}{80}}
\citation{kolter2011redd}
\@writefile{toc}{\contentsline {paragraph}{Sampling $W$}{81}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\lambda $}{81}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Power Disaggregation}{81}}
\newlabel{sec:power-disaggregation}{{5.4}{81}}
\citation{kolter2011redd}
\citation{kolter2011redd}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A sample data interval from the REDD dataset \cite  {kolter2011redd}. The top channel contains the total measured power in watts consumed by a home during a period of approximately 24 hours. Each timestep represents a 20 second intervals, during which the amplitude recorded is a median of the amplitudes in the original higher resolution data.\relax }}{83}}
\newlabel{fig:redd-data-example}{{5.1}{83}}
\citation{duane1987hybrid}
\citation{neal2011mcmc}
\@writefile{toc}{\contentsline {chapter}{\chapterline {6}\MakeUppercase  {Separate Similarities and Emissions: Learning Tonal Grammar in Music}}{84}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:music}{{6}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Separable Similarity and Emissions}{84}}
\newlabel{sec:separ-simil-emiss}{{6.1}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}A Hamlitonian Monte Carlo step to sample $\eta $}{84}}
\newlabel{sec:haml-monte-carlo}{{6.2}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Synthetic Data from an HMM with a Nearly Block Diagonal Transition Matrix}{87}}
\newlabel{sec:synthetic-data-from}{{6.3}{87}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Discovering Chord Equivalence Classes in Tonal Music}{88}}
\newlabel{sec:disc-chord-equiv}{{6.4}{88}}
\bibstyle{apalike}
\bibdata{../bib/supplement}
\@writefile{toc}{\contentsline {chapter}{\chapterline {7}\MakeUppercase  {Conclusions and Future Work}}{89}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:discussion}{{7}{89}}
\bibcite{baum1966statistical}{{1}{1966}{{Baum and Petrie}}{{}}}
\bibcite{beal2003variational}{{2}{2003}{{Beal}}{{}}}
\bibcite{beal2001infinite}{{3}{2001}{{Beal et~al.}}{{}}}
\bibcite{blei2006variational}{{4}{2006}{{Blei and Jordan}}{{}}}
\bibcite{box1987empirical}{{5}{1987}{{Box et~al.}}{{}}}
\bibcite{duane1987hybrid}{{6}{1987}{{Duane et~al.}}{{}}}
\bibcite{epanechnikov1969non}{{7}{1969}{{Epanechnikov}}{{}}}
\bibcite{fearnhead2003line}{{8}{2003}{{Fearnhead and Clifford}}{{}}}
\bibcite{ferguson1973bayesian}{{9}{1973}{{Ferguson}}{{}}}
\bibcite{fox2012tutorial}{{10}{2012}{{Fox and Roberts}}{{}}}
\bibcite{fox2008hdp}{{11}{2008}{{Fox et~al.}}{{}}}
\bibcite{geman1984stochastic}{{12}{1984}{{Geman and Geman}}{{}}}
\bibcite{gilks1992adaptive}{{13}{1992}{{Gilks and Wild}}{{}}}
\bibcite{ishwaran2000markov}{{14}{2000}{{Ishwaran and Zarepour}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{90}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\bibcite{johnson2014stochastic}{{15}{2014}{{Johnson and Willsky}}{{}}}
\bibcite{johnson2013bayesian}{{16}{2013}{{Johnson and Willsky}}{{}}}
\bibcite{kolter2011redd}{{17}{2011}{{Kolter and Johnson}}{{}}}
\bibcite{kurihara2007collapsed}{{18}{2007}{{Kurihara et~al.}}{{}}}
\bibcite{lindsay1995mixture}{{19}{1995}{{Lindsay}}{{}}}
\bibcite{mclachlan2004finite}{{20}{2004}{{McLachlan and Peel}}{{}}}
\bibcite{neal2011mcmc}{{21}{2011}{{Neal et~al.}}{{}}}
\bibcite{paisley2011discrete}{{22}{2011}{{Paisley et~al.}}{{}}}
\bibcite{parzen1962estimation}{{23}{1962}{{Parzen}}{{}}}
\bibcite{rabiner1986introduction}{{24}{1986}{{Rabiner and Juang}}{{}}}
\bibcite{rasmussen2000infinite}{{25}{2000}{{Rasmussen}}{{}}}
\bibcite{rosenblatt1956remarks}{{26}{1956}{{Rosenblatt et~al.}}{{}}}
\bibcite{sethuraman1994constructive}{{27}{1994}{{Sethuraman}}{{}}}
\bibcite{teh2011dirichlet}{{28}{2011}{{Teh}}{{}}}
\bibcite{teh2006hierarchical}{{29}{2006}{{Teh et~al.}}{{}}}
\bibcite{tripuraneni2015particle}{{30}{2015}{{Tripuraneni et~al.}}{{}}}
\bibcite{vangael2008beam}{{31}{2008}{{Van~Gael et~al.}}{{}}}
\bibcite{wasserman2007nonparametric}{{32}{2007}{{Wasserman}}{{}}}
