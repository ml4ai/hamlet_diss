\relax 
\@writefile{toc}{\contentsline {chapter}{List of Tables}{7}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{kolter2011redd}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{8}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{rasmussen2000infinite}
\citation{teh2006hierarchical}
\citation{paisley2011discrete}
\@writefile{toc}{\contentsline {chapter}{Abstract}{9}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\citation{rasmussen2000infinite}
\citation{teh2006hierarchical}
\citation{paisley2011discrete}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\citation{box1987empirical}
\@writefile{toc}{\contentsline {chapter}{\chapterline {1} \MakeUppercase  {Statistical Models and Model Selection} }{12}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:intro}{{1}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Statistical Models}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Model Selection}{12}}
\newlabel{sec:model-selection}{{1.2}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}The Likelihood Function and the Conservation of Explanatory Power}{12}}
\newlabel{sec:bayes-occams-razor}{{1.2.1}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}The Bias-Variance Tradeoff}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Example: Polynomial Regression}{16}}
\newlabel{sec:exampl-polyn-regr}{{1.2.3}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Balancing Fit and Complexity: Bayesian Occam's Razor}{16}}
\newlabel{sec:balanc-fit-compl}{{1.2.4}{16}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:poly-linear}{{1.1a}{17}}
\newlabel{sub@fig:poly-linear}{{a}{17}}
\newlabel{fig:poly-quad}{{1.1b}{17}}
\newlabel{sub@fig:poly-quad}{{b}{17}}
\newlabel{fig:poly10}{{1.1c}{17}}
\newlabel{sub@fig:poly10}{{c}{17}}
\newlabel{fig:poly-errors}{{1.1d}{17}}
\newlabel{sub@fig:poly-errors}{{d}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Polynomials of varying degrees fit using ordinary least squares (equivalently, maximum likelihood estimation) to a set of eleven points generated from a fifth order polynomial with independent Normal residuals. 1.1a\hbox {}--1.1c\hbox {}: fits of polynomial order 1, 2 and 10. Filled circles are training data, unfilled circles are data not used during fitting. 1.1d\hbox {} squared prediction error in and out of the training sample for polynomial orders 1 through 10.\relax }}{17}}
\newlabel{fig:polynomials}{{1.1}{17}}
\newlabel{eq:2}{{1.3}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Clustering via a Mixture of Gaussians Model}{19}}
\newlabel{sec:clustering}{{1.3}{19}}
\newlabel{eq:gmm}{{1.4}{19}}
\citation{geman1984stochastic}
\citation{baum1966statistical}
\citation{rabiner1986introduction}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Clustering Sequential Data: The Hidden Markov Model}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Model Selection in HMMs and the Mixture of Gaussians Model}{23}}
\newlabel{sec:model-select-mixt}{{1.5}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}The Structure of This Dissertation}{23}}
\citation{rosenblatt1956remarks,parzen1962estimation}
\@writefile{toc}{\contentsline {chapter}{\chapterline {2} \MakeUppercase  {Bayesian Nonparametric Models and the Infinite Hidden Markov Model} }{25}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:HMM-NPBayes}{{2}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Parametric Vs. Nonparametric Models}{25}}
\newlabel{sec:param-vs.-nonp}{{2.1}{25}}
\citation{epanechnikov1969non}
\citation{ferguson1973bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Dirichlet Process}{26}}
\newlabel{sec:dirichlet-process}{{2.2}{26}}
\citation{ferguson1973bayesian}
\newlabel{eq:4}{{2.2}{27}}
\citation{ferguson1973bayesian}
\citation{sethuraman1994constructive}
\citation{sethuraman1994constructive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Normalized Gamma Process representation of the DP}{28}}
\newlabel{sec:norm-gamma-proc}{{2.2.1}{28}}
\newlabel{eq:8}{{2.3}{28}}
\newlabel{eq:3}{{2.4}{28}}
\newlabel{eq:9}{{2.5}{28}}
\newlabel{eq:1}{{2.6}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The Stick-Breaking Process construction of the DP}{29}}
\newlabel{sec:stick-break-proc}{{2.2.2}{29}}
\newlabel{stick-step-1}{{1}{29}}
\newlabel{stick-step-2}{{2}{29}}
\newlabel{stick-step-3}{{3}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}The Chinese Restaurant Process}{29}}
\newlabel{sec:chin-rest-proc}{{2.2.3}{29}}
\citation{teh2011dirichlet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}The Dirichlet Process Mixture Model}{30}}
\newlabel{eq:gmm-2}{{2.8}{30}}
\newlabel{eq:DP-prior}{{2.9}{30}}
\citation{fox2012tutorial}
\citation{blei2006variational}
\citation{kurihara2007collapsed}
\newlabel{eq:5}{{2.10}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}A Gibbs Sampler for DP Mixture Models}{31}}
\newlabel{sec:gibbs-sampler-dp}{{2.2.5}{31}}
\newlabel{sec:dirichl-proc-gauss}{{2.2.5}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}An Infinite State HMM}{32}}
\newlabel{sec:an-infinite-state}{{2.3}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The Hierarchical Dirichlet Process}{32}}
\newlabel{sec:hier-dirichl-proc}{{2.3.1}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}The HDP-HMM}{32}}
\newlabel{sec:hdp-hmm}{{2.3.2}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Inference in Finite and Infinite State HMMs}{32}}
\newlabel{sec:inference-hdp-hmm}{{2.4}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}The Forward Backward Algorithm}{32}}
\newlabel{sec:forw-backw-algor-1}{{2.4.1}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Two Gibbs Samplers for the HDP-HMM}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Beam Sampling in the HDP-HMM}{32}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {chapter}{\chapterline {3}\MakeUppercase  {HaMMLeT: An infinite Hidden Markov Model with Local Transitions}}{33}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:HaMMLeT}{{3}{33}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Transition Dynamics in the HDP-HMM}{34}}
\newlabel{sec:transition-dynamics}{{3.1}{34}}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\newlabel{eq:1}{{3.3}{35}}
\newlabel{eq:4}{{3.4}{35}}
\newlabel{eq:5}{{3.5}{35}}
\newlabel{eq:6}{{3.6}{35}}
\citation{ferguson1973bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}An HDP-HMM With Local Transitions}{36}}
\@writefile{toc}{\contentsline {paragraph}{Notational Conventions}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}A Normalized Gamma Process representation of the HDP-HMM}{37}}
\newlabel{sec:normalized-gamma}{{3.2.1}{37}}
\newlabel{eq:17}{{3.7}{37}}
\newlabel{eq:18}{{3.8}{37}}
\newlabel{eq:16}{{3.9}{37}}
\newlabel{eq:19}{{3.10}{37}}
\newlabel{eq:20}{{3.11}{37}}
\newlabel{eq:21}{{3.12}{37}}
\newlabel{eq:22}{{3.13}{37}}
\newlabel{eq:23}{{3.15}{37}}
\newlabel{eq:50}{{3.16}{37}}
\newlabel{eq:24}{{3.17}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Promoting ``Local'' Transitions}{38}}
\newlabel{sec:prom-local-trans}{{3.2.2}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The HDP-HMM-LT as a continuous-time Markov Jump Process with ``failed'' jumps}{40}}
\newlabel{sec:dist-based-filt}{{3.3}{40}}
\newlabel{eq:beta}{{3.24}{40}}
\newlabel{eq:51}{{3.27}{41}}
\newlabel{eq:52}{{3.28}{41}}
\newlabel{eq:53}{{3.29}{41}}
\newlabel{eq:56}{{3.30}{42}}
\newlabel{eq:60}{{3.34}{42}}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\newlabel{eq:54}{{3.35}{43}}
\newlabel{eq:joint-likelihood}{{3.40}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}An HDP-HSMM-LT modification}{43}}
\newlabel{sec:an-hsmm-modification}{{3.3.1}{43}}
\citation{johnson2013bayesian}
\citation{johnson2013bayesian}
\newlabel{eq:95}{{3.41}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Summary}{45}}
\newlabel{sec:model-summary}{{3.3.2}{45}}
\newlabel{eq:96}{{3.43}{45}}
\newlabel{eq:likelihood}{{3.49}{45}}
\newlabel{eq:97}{{3.50}{45}}
\newlabel{eq:98}{{3.52}{45}}
\newlabel{eq:likelihood-hsmm}{{3.53}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}MCMC Inference in the ``Failed Jumps'' Representation}{45}}
\newlabel{sec:inference}{{3.4}{45}}
\citation{ishwaran2000markov}
\newlabel{eq:28}{{3.54}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Sampling $\pi $, $\beta $, $\alpha $ and $\gamma $}{46}}
\newlabel{sec:sampling-pi}{{3.4.1}{46}}
\newlabel{eq:46}{{3.55}{46}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\pi $}{47}}
\newlabel{eq:47}{{3.56}{47}}
\newlabel{eq:61}{{3.57}{47}}
\newlabel{eq:24}{{3.59}{47}}
\newlabel{eq:25}{{3.60}{47}}
\newlabel{sec:sampling-bbeta}{{3.4.1}{47}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\beta $}{47}}
\newlabel{eq:62}{{3.61}{47}}
\citation{teh2006hierarchical}
\newlabel{eq:31}{{3.67}{48}}
\newlabel{eq:32}{{3.68}{48}}
\newlabel{eq:33}{{3.69}{49}}
\newlabel{eq:34}{{3.70}{49}}
\newlabel{eq:38}{{3.71}{49}}
\newlabel{sec:sampling-alpha}{{3.4.1}{49}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\alpha $ and $\gamma $}{49}}
\newlabel{eq:42}{{3.72}{49}}
\newlabel{eq:43}{{3.79}{50}}
\newlabel{eq:8}{{3.80}{50}}
\newlabel{eq:44}{{3.81}{50}}
\newlabel{eq:9}{{3.82}{50}}
\citation{johnson2013bayesian}
\newlabel{eq:10}{{3.84}{51}}
\newlabel{eq:11}{{3.85}{51}}
\newlabel{eq:18}{{3.86}{51}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{51}}
\newlabel{eq:100}{{3.87}{51}}
\newlabel{eq:46}{{3.88}{51}}
\newlabel{eq:64}{{3.89}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Sampling $z$ and the auxiliary variables}{51}}
\newlabel{sec:sampling-z_t}{{3.4.2}{51}}
\newlabel{eq:19}{{3.93}{52}}
\newlabel{eq:48}{{3.95}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Sampling state and emission parameters}{52}}
\newlabel{sec:sampling-eta}{{3.4.3}{52}}
\newlabel{eq:65}{{3.100}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Use Cases}{53}}
\@writefile{toc}{\contentsline {chapter}{\chapterline {4}\MakeUppercase  {Binary Vector States: Speaker Diarization}}{55}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:cocktail-party}{{4}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Binary State Vectors}{55}}
\newlabel{eq:39}{{4.1}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Additional Inference Steps}{56}}
\newlabel{sec:sampling-eta}{{4.1.1}{56}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\theta $}{56}}
\newlabel{eq:1}{{4.2}{56}}
\newlabel{eq:68}{{4.3}{56}}
\newlabel{eq:70}{{4.4}{56}}
\newlabel{eq:71}{{4.5}{56}}
\newlabel{eq:72}{{4.6}{57}}
\newlabel{eq:74}{{4.7}{57}}
\newlabel{eq:73}{{4.8}{57}}
\newlabel{eq:91}{{4.9}{57}}
\newlabel{eq:77}{{4.10}{58}}
\newlabel{sec:sampling-bmu}{{4.1.1}{58}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\mu $}{58}}
\newlabel{eq:92}{{4.15}{58}}
\newlabel{eq:93}{{4.16}{58}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $W$ and $\Sigma $}{58}}
\newlabel{eq:78}{{4.17}{58}}
\newlabel{eq:79}{{4.18}{58}}
\citation{gilks1992adaptive}
\newlabel{eq:80}{{4.19}{59}}
\newlabel{sec:sampling-lambda}{{4.1.1}{59}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\lambda $}{59}}
\newlabel{eq:88}{{4.20}{59}}
\newlabel{eq:88}{{4.21}{59}}
\newlabel{eq:90}{{4.22}{59}}
\newlabel{eq:94}{{4.23}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Summary}{59}}
\newlabel{sec:summary}{{4.1.2}{59}}
\newlabel{eq:101}{{4.30}{60}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}``Cocktail Party'' Data}{61}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Synthetic Data Without Local Transitions}{63}}
\newlabel{sec:synth-data-without}{{4.3}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces (a-b) Accuracy and F1 scores for the HDP-HMM-LT, standard HDP-HMM, and Binary Factorial HMM on the Cocktail Party Data. Metrics are averaged over 10 Gibbs runs on each model, with error bars representing a 99\% confidence interval for the mean per iteration. The first 100 iterations are not shown. (c) Learned similarity parameter, $\lambda $, for the LT model, (d) Number of distinct states used by HDP-HMM and HDP-HMM-LT. The first 100 iterations are excluded.\relax }}{64}}
\newlabel{fig:cocktail-results}{{4.1}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces (a-b) Accuracy and F1 for the three models on data generated from an HDP-HMM without local transitions, (c) Learned similarity parameter, $\lambda $, for the LT model, (d) Number of states used by the HDP-HMM and HDP-HMM-LT. The first 100 iterations are omitted.\relax }}{65}}
\newlabel{fig:synthetic-results}{{4.2}{65}}
\@writefile{toc}{\contentsline {chapter}{\chapterline {5}\MakeUppercase  {Categorical Vector States: Power Disaggregation}}{66}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:REDD}{{5}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Generalizing to Categorical-Valued $\theta $}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Priors and Representations in the Categorical State Variant}{67}}
\newlabel{sec:priors-repr-categ}{{5.2}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Adapting Posterior Inference for Categorical State Vectors}{68}}
\newlabel{sec:adapt-post-infer}{{5.3}{68}}
\@writefile{toc}{\contentsline {subsubsection}{Sampling $\theta $}{68}}
\citation{kolter2011redd}
\@writefile{toc}{\contentsline {paragraph}{Sampling $W$}{69}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\lambda $}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Power Disaggregation}{69}}
\newlabel{sec:power-disaggregation}{{5.4}{69}}
\citation{kolter2011redd}
\citation{kolter2011redd}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A sample data interval from the REDD dataset \cite  {kolter2011redd}. The top channel contains the total measured power in watts consumed by a home during a period of approximately 24 hours. Each timestep represents a 20 second intervals, during which the amplitude recorded is a median of the amplitudes in the original higher resolution data.\relax }}{71}}
\newlabel{fig:redd-data-example}{{5.1}{71}}
\citation{duane1987hybrid}
\citation{neal2011mcmc}
\@writefile{toc}{\contentsline {chapter}{\chapterline {6}\MakeUppercase  {Separate Similarities and Emissions: Learning Tonal Grammar in Music}}{72}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:music}{{6}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Separable Similarity and Emissions}{72}}
\newlabel{sec:separ-simil-emiss}{{6.1}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}A Hamlitonian Monte Carlo step to sample $\eta $}{72}}
\newlabel{sec:haml-monte-carlo}{{6.2}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Synthetic Data from an HMM with a Nearly Block Diagonal Transition Matrix}{75}}
\newlabel{sec:synthetic-data-from}{{6.3}{75}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Discovering Chord Equivalence Classes in Tonal Music}{76}}
\newlabel{sec:disc-chord-equiv}{{6.4}{76}}
\bibstyle{apalike}
\bibdata{../bib/supplement}
\@writefile{toc}{\contentsline {chapter}{\chapterline {7}\MakeUppercase  {Conclusions and Future Work}}{77}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:discussion}{{7}{77}}
\bibcite{baum1966statistical}{{1}{1966}{{Baum and Petrie}}{{}}}
\bibcite{beal2001infinite}{{2}{2001}{{Beal et~al.}}{{}}}
\bibcite{blei2006variational}{{3}{2006}{{Blei and Jordan}}{{}}}
\bibcite{box1987empirical}{{4}{1987}{{Box et~al.}}{{}}}
\bibcite{duane1987hybrid}{{5}{1987}{{Duane et~al.}}{{}}}
\bibcite{epanechnikov1969non}{{6}{1969}{{Epanechnikov}}{{}}}
\bibcite{ferguson1973bayesian}{{7}{1973}{{Ferguson}}{{}}}
\bibcite{fox2012tutorial}{{8}{2012}{{Fox and Roberts}}{{}}}
\bibcite{fox2008hdp}{{9}{2008}{{Fox et~al.}}{{}}}
\bibcite{geman1984stochastic}{{10}{1984}{{Geman and Geman}}{{}}}
\bibcite{gilks1992adaptive}{{11}{1992}{{Gilks and Wild}}{{}}}
\bibcite{ishwaran2000markov}{{12}{2000}{{Ishwaran and Zarepour}}{{}}}
\bibcite{johnson2013bayesian}{{13}{2013}{{Johnson and Willsky}}{{}}}
\bibcite{kolter2011redd}{{14}{2011}{{Kolter and Johnson}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{78}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\bibcite{kurihara2007collapsed}{{15}{2007}{{Kurihara et~al.}}{{}}}
\bibcite{neal2011mcmc}{{16}{2011}{{Neal et~al.}}{{}}}
\bibcite{paisley2011discrete}{{17}{2011}{{Paisley et~al.}}{{}}}
\bibcite{parzen1962estimation}{{18}{1962}{{Parzen}}{{}}}
\bibcite{rabiner1986introduction}{{19}{1986}{{Rabiner and Juang}}{{}}}
\bibcite{rasmussen2000infinite}{{20}{2000}{{Rasmussen}}{{}}}
\bibcite{rosenblatt1956remarks}{{21}{1956}{{Rosenblatt et~al.}}{{}}}
\bibcite{sethuraman1994constructive}{{22}{1994}{{Sethuraman}}{{}}}
\bibcite{teh2011dirichlet}{{23}{2011}{{Teh}}{{}}}
\bibcite{teh2006hierarchical}{{24}{2006}{{Teh et~al.}}{{}}}
