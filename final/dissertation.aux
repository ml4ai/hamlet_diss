\relax 
\@writefile{toc}{\contentsline {chapter}{List of Tables}{9}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{kolter2011redd}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{10}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{rasmussen2000infinite}
\citation{teh2006hierarchical}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {chapter}{Abstract}{11}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{rasmussen2000infinite}
\citation{teh2006hierarchical}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\citation{box1987empirical}
\citation{rosenblatt1956remarks,parzen1962estimation}
\citation{wasserman2007nonparametric}
\@writefile{toc}{\contentsline {chapter}{\chapterline {1} \MakeUppercase  {Statistical Models and Model Selection} }{13}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:intro}{{1}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{13}}
\citation{lindsay1995mixture,mclachlan2004finite}
\citation{geman1984stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}An overview of the dissertation}{15}}
\newlabel{sec:an-overv-diss}{{1.1.1}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Model Selection}{16}}
\newlabel{sec:model-selection}{{1.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}The Likelihood Function and the Conservation of Explanatory Power}{16}}
\newlabel{sec:bayes-occams-razor}{{1.2.1}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}The Bias-Variance Tradeoff}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Example: Polynomial Regression}{19}}
\newlabel{sec:exampl-polyn-regr}{{1.2.3}{19}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:poly-linear}{{1.1a}{20}}
\newlabel{sub@fig:poly-linear}{{a}{20}}
\newlabel{fig:poly-quad}{{1.1b}{20}}
\newlabel{sub@fig:poly-quad}{{b}{20}}
\newlabel{fig:poly10}{{1.1c}{20}}
\newlabel{sub@fig:poly10}{{c}{20}}
\newlabel{fig:poly-errors}{{1.1d}{20}}
\newlabel{sub@fig:poly-errors}{{d}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Polynomials of varying degrees fit using ordinary least squares (equivalently, maximum likelihood estimation) to a set of eleven points generated from a fifth order polynomial with independent Normal residuals. 1.1a\hbox {}--1.1c\hbox {}: fits of polynomial order 1, 2 and 10. Filled circles are training data, unfilled circles are data not used during fitting. 1.1d\hbox {} squared prediction error in and out of the training sample for polynomial orders 1 through 10.\relax }}{20}}
\newlabel{fig:polynomials}{{1.1}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Balancing Fit and Complexity: Bayesian Occam's Razor}{21}}
\newlabel{sec:balanc-fit-compl}{{1.2.4}{21}}
\newlabel{eq:2}{{1.3}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Clustering via a Mixture of Gaussians Model}{23}}
\newlabel{sec:clustering}{{1.3}{23}}
\newlabel{eq:gmm}{{1.4}{23}}
\citation{geman1984stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Clustering Sequential Data: The Hidden Markov Model}{24}}
\citation{baum1966statistical}
\citation{rabiner1986introduction}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Model Selection in HMMs and the Mixture of Gaussians Model}{26}}
\newlabel{sec:model-select-mixt}{{1.5}{26}}
\citation{rosenblatt1956remarks,parzen1962estimation}
\@writefile{toc}{\contentsline {chapter}{\chapterline {2} \MakeUppercase  {Bayesian Nonparametric Models and the Infinite Hidden Markov Model} }{28}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:HMM-NPBayes}{{2}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Parametric Vs. Nonparametric Models}{28}}
\newlabel{sec:param-vs.-nonp}{{2.1}{28}}
\citation{epanechnikov1969non}
\citation{ferguson1973bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Dirichlet Process}{29}}
\newlabel{sec:dirichlet-process}{{2.2}{29}}
\citation{ferguson1973bayesian}
\newlabel{eq:4}{{2.2}{30}}
\citation{ferguson1973bayesian}
\citation{sethuraman1994constructive}
\citation{sethuraman1994constructive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Normalized Gamma Process representation of the DP}{31}}
\newlabel{sec:norm-gamma-proc}{{2.2.1}{31}}
\newlabel{eq:8}{{2.3}{31}}
\newlabel{eq:3}{{2.4}{31}}
\newlabel{eq:9}{{2.5}{31}}
\newlabel{eq:1}{{2.6}{31}}
\citation{sethuraman1994constructive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The Stick-Breaking Process construction of the DP}{32}}
\newlabel{sec:stick-break-proc}{{2.2.2}{32}}
\newlabel{stick-step-1}{{1}{32}}
\newlabel{stick-step-2}{{2}{32}}
\newlabel{stick-step-3}{{3}{32}}
\newlabel{eq:11}{{2.7}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}The Chinese Restaurant Process}{32}}
\newlabel{sec:chin-rest-proc}{{2.2.3}{32}}
\citation{teh2011dirichlet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}The Dirichlet Process Mixture Model}{33}}
\citation{fox2012tutorial}
\citation{blei2006variational}
\citation{kurihara2007collapsed}
\newlabel{eq:gmm-2}{{2.9}{34}}
\newlabel{eq:DP-prior}{{2.10}{34}}
\newlabel{eq:5}{{2.11}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Two Gibbs Samplers for DP Mixture Models}{34}}
\newlabel{sec:gibbs-sampler-dp}{{2.2.5}{34}}
\@writefile{toc}{\contentsline {paragraph}{Method 1: A Collapsed Sampler Based on the CRP}{35}}
\newlabel{eq:1}{{2.16}{36}}
\@writefile{toc}{\contentsline {paragraph}{Method 2: An uncollapsed sampler based on the Stick-Breaking Process}{36}}
\newlabel{eq:2}{{2.17}{37}}
\newlabel{eq:6}{{2.19}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}An Infinite State HMM}{37}}
\newlabel{sec:an-infinite-state}{{2.3}{37}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The Hierarchical Dirichlet Process}{38}}
\newlabel{sec:hier-dirichl-proc}{{2.3.1}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}The HDP-HMM}{38}}
\newlabel{sec:hdp-hmm}{{2.3.2}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}A Stick-Breaking Representation of the Aggregate Weights in an HDP}{40}}
\newlabel{stick-step-1}{{1}{40}}
\newlabel{stick-step-2}{{2}{40}}
\citation{teh2006hierarchical}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\citation{beal2001infinite}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}A Normalized Gamma Process Representation of the Weights in the HDP}{41}}
\newlabel{sec:norm-gamma-proc-1}{{2.3.4}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Adapting the HDP for an Infinite State HMM}{41}}
\citation{beal2003variational}
\citation{johnson2014stochastic}
\citation{fearnhead2003line,tripuraneni2015particle}
\citation{teh2006hierarchical,vangael2008beam,fox2008hdp,johnson2013bayesian}
\citation{teh2006hierarchical}
\citation{vangael2008beam}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Inference in Finite and Infinite State HMMs}{42}}
\newlabel{sec:inference-hdp-hmm}{{2.4}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}The Forward Backward Algorithm}{43}}
\newlabel{sec:forw-backw-algor-1}{{2.4.1}{43}}
\@writefile{toc}{\contentsline {paragraph}{The Backward Step}{44}}
\newlabel{sec:forward-step}{{2.4.1}{44}}
\@writefile{toc}{\contentsline {paragraph}{The Forward Step}{44}}
\citation{teh2006hierarchical}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Gibbs Sampling in the HDP-HMM}{45}}
\citation{vangael2008beam}
\citation{neal2003slice}
\citation{vangael2008beam}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Beam Sampling in the HDP-HMM}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Local Transitions}{49}}
\newlabel{sec:local-transitions}{{2.5}{49}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {chapter}{\chapterline {3}\MakeUppercase  {HaMMLeT: An infinite Hidden Markov Model with Local Transitions}}{51}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:HaMMLeT}{{3}{51}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Transition Dynamics in the HDP-HMM}{52}}
\newlabel{sec:transition-dynamics}{{3.1}{52}}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\newlabel{eq:1}{{3.3}{53}}
\newlabel{eq:4}{{3.4}{53}}
\newlabel{eq:5}{{3.5}{53}}
\newlabel{eq:6}{{3.6}{53}}
\citation{ferguson1973bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}An HDP-HMM With Local Transitions}{54}}
\@writefile{toc}{\contentsline {paragraph}{Notational Conventions}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}A Normalized Gamma Process representation of the HDP-HMM}{55}}
\newlabel{sec:normalized-gamma}{{3.2.1}{55}}
\newlabel{eq:17}{{3.7}{55}}
\newlabel{eq:18}{{3.8}{55}}
\newlabel{eq:16}{{3.9}{55}}
\newlabel{eq:19}{{3.10}{55}}
\newlabel{eq:20}{{3.11}{55}}
\newlabel{eq:21}{{3.12}{55}}
\newlabel{eq:22}{{3.13}{55}}
\newlabel{eq:23}{{3.15}{55}}
\newlabel{eq:50}{{3.16}{55}}
\newlabel{eq:24}{{3.17}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Promoting ``Local'' Transitions}{56}}
\newlabel{sec:prom-local-trans}{{3.2.2}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The HDP-HMM-LT as a continuous-time Markov Jump Process with ``failed'' jumps}{58}}
\newlabel{sec:dist-based-filt}{{3.3}{58}}
\newlabel{eq:beta}{{3.24}{58}}
\newlabel{eq:51}{{3.27}{59}}
\newlabel{eq:52}{{3.28}{59}}
\newlabel{eq:53}{{3.29}{59}}
\newlabel{eq:56}{{3.30}{60}}
\newlabel{eq:60}{{3.34}{60}}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\newlabel{eq:54}{{3.35}{61}}
\newlabel{eq:joint-likelihood}{{3.40}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}An HDP-HSMM-LT modification}{61}}
\newlabel{sec:an-hsmm-modification}{{3.3.1}{61}}
\citation{johnson2013bayesian}
\citation{johnson2013bayesian}
\newlabel{eq:95}{{3.41}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Summary}{63}}
\newlabel{sec:model-summary}{{3.3.2}{63}}
\newlabel{eq:96}{{3.43}{63}}
\newlabel{eq:likelihood}{{3.49}{63}}
\newlabel{eq:97}{{3.50}{63}}
\newlabel{eq:98}{{3.52}{63}}
\newlabel{eq:likelihood-hsmm}{{3.53}{63}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}MCMC Inference in the ``Failed Jumps'' Representation}{63}}
\newlabel{sec:inference}{{3.4}{63}}
\citation{ishwaran2000markov}
\newlabel{eq:28}{{3.54}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Sampling $\pi $, $\beta $, $\alpha $ and $\gamma $}{64}}
\newlabel{sec:sampling-pi}{{3.4.1}{64}}
\newlabel{eq:46}{{3.55}{64}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\pi $}{65}}
\newlabel{eq:47}{{3.56}{65}}
\newlabel{eq:61}{{3.57}{65}}
\newlabel{eq:24}{{3.59}{65}}
\newlabel{eq:25}{{3.60}{65}}
\newlabel{sec:sampling-bbeta}{{3.4.1}{65}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\beta $}{65}}
\newlabel{eq:62}{{3.61}{65}}
\citation{teh2006hierarchical}
\newlabel{eq:31}{{3.67}{66}}
\newlabel{eq:32}{{3.68}{66}}
\newlabel{eq:33}{{3.69}{67}}
\newlabel{eq:34}{{3.70}{67}}
\newlabel{eq:38}{{3.71}{67}}
\newlabel{sec:sampling-alpha}{{3.4.1}{67}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\alpha $ and $\gamma $}{67}}
\newlabel{eq:42}{{3.72}{67}}
\newlabel{eq:43}{{3.79}{68}}
\newlabel{eq:8}{{3.80}{68}}
\newlabel{eq:44}{{3.81}{68}}
\newlabel{eq:9}{{3.82}{68}}
\citation{johnson2013bayesian}
\newlabel{eq:10}{{3.84}{69}}
\newlabel{eq:11}{{3.85}{69}}
\newlabel{eq:18}{{3.86}{69}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{69}}
\newlabel{eq:100}{{3.87}{69}}
\newlabel{eq:46}{{3.88}{69}}
\newlabel{eq:64}{{3.89}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Sampling $z$ and the auxiliary variables}{69}}
\newlabel{sec:sampling-z_t}{{3.4.2}{69}}
\newlabel{eq:19}{{3.93}{70}}
\newlabel{eq:48}{{3.95}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Sampling state and emission parameters}{70}}
\newlabel{sec:sampling-eta}{{3.4.3}{70}}
\newlabel{eq:65}{{3.100}{70}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Use Cases}{71}}
\@writefile{toc}{\contentsline {chapter}{\chapterline {4}\MakeUppercase  {Binary Vector States: Speaker Diarization}}{73}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:cocktail-party}{{4}{73}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Binary State Vectors}{73}}
\newlabel{eq:39}{{4.1}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Additional Inference Steps}{74}}
\newlabel{sec:sampling-eta}{{4.1.1}{74}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\theta $}{74}}
\newlabel{eq:1}{{4.2}{74}}
\newlabel{eq:68}{{4.3}{74}}
\newlabel{eq:70}{{4.4}{74}}
\newlabel{eq:71}{{4.5}{74}}
\newlabel{eq:72}{{4.6}{75}}
\newlabel{eq:74}{{4.7}{75}}
\newlabel{eq:73}{{4.8}{75}}
\newlabel{eq:91}{{4.9}{75}}
\newlabel{eq:77}{{4.10}{76}}
\newlabel{sec:sampling-bmu}{{4.1.1}{76}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\mu $}{76}}
\newlabel{eq:92}{{4.15}{76}}
\newlabel{eq:93}{{4.16}{76}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $W$ and $\Sigma $}{76}}
\newlabel{eq:78}{{4.17}{76}}
\newlabel{eq:79}{{4.18}{76}}
\citation{gilks1992adaptive}
\newlabel{eq:80}{{4.19}{77}}
\newlabel{sec:sampling-lambda}{{4.1.1}{77}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\lambda $}{77}}
\newlabel{eq:88}{{4.20}{77}}
\newlabel{eq:88}{{4.21}{77}}
\newlabel{eq:90}{{4.22}{77}}
\newlabel{eq:94}{{4.23}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Summary}{77}}
\newlabel{sec:summary}{{4.1.2}{77}}
\newlabel{eq:101}{{4.30}{78}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}``Cocktail Party'' Data}{79}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Synthetic Data Without Local Transitions}{81}}
\newlabel{sec:synth-data-without}{{4.3}{81}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces (a-b) Accuracy and F1 scores for the HDP-HMM-LT, standard HDP-HMM, and Binary Factorial HMM on the Cocktail Party Data. Metrics are averaged over 10 Gibbs runs on each model, with error bars representing a 99\% confidence interval for the mean per iteration. The first 100 iterations are not shown. (c) Learned similarity parameter, $\lambda $, for the LT model, (d) Number of distinct states used by HDP-HMM and HDP-HMM-LT. The first 100 iterations are excluded.\relax }}{82}}
\newlabel{fig:cocktail-results}{{4.1}{82}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces (a-b) Accuracy and F1 for the three models on data generated from an HDP-HMM without local transitions, (c) Learned similarity parameter, $\lambda $, for the LT model, (d) Number of states used by the HDP-HMM and HDP-HMM-LT. The first 100 iterations are omitted.\relax }}{83}}
\newlabel{fig:synthetic-results}{{4.2}{83}}
\@writefile{toc}{\contentsline {chapter}{\chapterline {5}\MakeUppercase  {Categorical Vector States: Power Disaggregation}}{84}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:REDD}{{5}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Generalizing to Categorical-Valued $\theta $}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Priors and Representations in the Categorical State Variant}{85}}
\newlabel{sec:priors-repr-categ}{{5.2}{85}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Adapting Posterior Inference for Categorical State Vectors}{86}}
\newlabel{sec:adapt-post-infer}{{5.3}{86}}
\@writefile{toc}{\contentsline {subsubsection}{Sampling $\theta $}{86}}
\citation{kolter2011redd}
\@writefile{toc}{\contentsline {paragraph}{Sampling $W$}{87}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\lambda $}{87}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Power Disaggregation}{87}}
\newlabel{sec:power-disaggregation}{{5.4}{87}}
\citation{kolter2011redd}
\citation{kolter2011redd}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A sample data interval from the REDD dataset \cite  {kolter2011redd}. The top channel contains the total measured power in watts consumed by a home during a period of approximately 24 hours. Each timestep represents a 20 second intervals, during which the amplitude recorded is a median of the amplitudes in the original higher resolution data.\relax }}{89}}
\newlabel{fig:redd-data-example}{{5.1}{89}}
\citation{duane1987hybrid}
\citation{neal2011mcmc}
\@writefile{toc}{\contentsline {chapter}{\chapterline {6}\MakeUppercase  {Separate Similarities and Emissions: Learning Tonal Grammar in Music}}{90}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:music}{{6}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Separable Similarity and Emissions}{90}}
\newlabel{sec:separ-simil-emiss}{{6.1}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}A Hamlitonian Monte Carlo step to sample $\eta $}{90}}
\newlabel{sec:haml-monte-carlo}{{6.2}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Synthetic Data from an HMM with a Nearly Block Diagonal Transition Matrix}{93}}
\newlabel{sec:synthetic-data-from}{{6.3}{93}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Data-generation}{93}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Left: Log likelihood on the training set by Gibbs iteration (marginalizing out state sequence) for LT and no LT (HDP-HMM) models. Right: Log likelihood on a held out test set.\relax }}{94}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Results}{94}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Discovering Chord Equivalence Classes in Tonal Music}{94}}
\newlabel{sec:disc-chord-equiv}{{6.4}{94}}
\bibstyle{apalike}
\bibdata{../bib/supplement}
\@writefile{toc}{\contentsline {chapter}{\chapterline {7}\MakeUppercase  {Conclusions and Future Work}}{95}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:discussion}{{7}{95}}
\bibcite{baum1966statistical}{{1}{1966}{{Baum and Petrie}}{{}}}
\bibcite{beal2003variational}{{2}{2003}{{Beal}}{{}}}
\bibcite{beal2001infinite}{{3}{2001}{{Beal et~al.}}{{}}}
\bibcite{blei2006variational}{{4}{2006}{{Blei and Jordan}}{{}}}
\bibcite{box1987empirical}{{5}{1987}{{Box et~al.}}{{}}}
\bibcite{duane1987hybrid}{{6}{1987}{{Duane et~al.}}{{}}}
\bibcite{epanechnikov1969non}{{7}{1969}{{Epanechnikov}}{{}}}
\bibcite{fearnhead2003line}{{8}{2003}{{Fearnhead and Clifford}}{{}}}
\bibcite{ferguson1973bayesian}{{9}{1973}{{Ferguson}}{{}}}
\bibcite{fox2012tutorial}{{10}{2012}{{Fox and Roberts}}{{}}}
\bibcite{fox2008hdp}{{11}{2008}{{Fox et~al.}}{{}}}
\bibcite{geman1984stochastic}{{12}{1984}{{Geman and Geman}}{{}}}
\bibcite{gilks1992adaptive}{{13}{1992}{{Gilks and Wild}}{{}}}
\bibcite{ishwaran2000markov}{{14}{2000}{{Ishwaran and Zarepour}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{96}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\bibcite{johnson2014stochastic}{{15}{2014}{{Johnson and Willsky}}{{}}}
\bibcite{johnson2013bayesian}{{16}{2013}{{Johnson and Willsky}}{{}}}
\bibcite{kolter2011redd}{{17}{2011}{{Kolter and Johnson}}{{}}}
\bibcite{kurihara2007collapsed}{{18}{2007}{{Kurihara et~al.}}{{}}}
\bibcite{lindsay1995mixture}{{19}{1995}{{Lindsay}}{{}}}
\bibcite{mclachlan2004finite}{{20}{2004}{{McLachlan and Peel}}{{}}}
\bibcite{neal2003slice}{{21}{2003}{{Neal}}{{}}}
\bibcite{neal2011mcmc}{{22}{2011}{{Neal et~al.}}{{}}}
\bibcite{parzen1962estimation}{{23}{1962}{{Parzen}}{{}}}
\bibcite{rabiner1986introduction}{{24}{1986}{{Rabiner and Juang}}{{}}}
\bibcite{rasmussen2000infinite}{{25}{2000}{{Rasmussen}}{{}}}
\bibcite{rosenblatt1956remarks}{{26}{1956}{{Rosenblatt et~al.}}{{}}}
\bibcite{sethuraman1994constructive}{{27}{1994}{{Sethuraman}}{{}}}
\bibcite{teh2011dirichlet}{{28}{2011}{{Teh}}{{}}}
\bibcite{teh2006hierarchical}{{29}{2006}{{Teh et~al.}}{{}}}
\bibcite{tripuraneni2015particle}{{30}{2015}{{Tripuraneni et~al.}}{{}}}
\bibcite{vangael2008beam}{{31}{2008}{{Van~Gael et~al.}}{{}}}
\bibcite{wasserman2007nonparametric}{{32}{2007}{{Wasserman}}{{}}}
