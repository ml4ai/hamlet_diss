\relax 
\@writefile{toc}{\contentsline {chapter}{List of Tables}{9}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{kolter2011redd}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{10}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{rasmussen2000infinite}
\citation{teh2006hierarchical}
\citation{paisley2011discrete}
\@writefile{toc}{\contentsline {chapter}{Abstract}{11}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\citation{rasmussen2000infinite}
\citation{teh2006hierarchical}
\citation{paisley2011discrete}
\citation{beal2001infinite}
\citation{teh2006hierarchical}
\citation{box1987empirical}
\citation{rosenblatt1956remarks,parzen1962estimation}
\citation{wasserman2007nonparametric}
\@writefile{toc}{\contentsline {chapter}{\chapterline {1} \MakeUppercase  {Statistical Models and Model Selection} }{14}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:intro}{{1}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{14}}
\citation{lindsay1995mixture,mclachlan2004finite}
\citation{geman1984stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}An overview of the dissertation}{16}}
\newlabel{sec:an-overv-diss}{{1.1.1}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Model Selection}{17}}
\newlabel{sec:model-selection}{{1.2}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}The Likelihood Function and the Conservation of Explanatory Power}{17}}
\newlabel{sec:bayes-occams-razor}{{1.2.1}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}The Bias-Variance Tradeoff}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Example: Polynomial Regression}{20}}
\newlabel{sec:exampl-polyn-regr}{{1.2.3}{20}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:poly-linear}{{1.1a}{21}}
\newlabel{sub@fig:poly-linear}{{a}{21}}
\newlabel{fig:poly-quad}{{1.1b}{21}}
\newlabel{sub@fig:poly-quad}{{b}{21}}
\newlabel{fig:poly10}{{1.1c}{21}}
\newlabel{sub@fig:poly10}{{c}{21}}
\newlabel{fig:poly-errors}{{1.1d}{21}}
\newlabel{sub@fig:poly-errors}{{d}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Polynomials of varying degrees fit using ordinary least squares (equivalently, maximum likelihood estimation) to a set of eleven points generated from a fifth order polynomial with independent Normal residuals. 1.1a\hbox {}--1.1c\hbox {}: fits of polynomial order 1, 2 and 10. Filled circles are training data, unfilled circles are data not used during fitting. 1.1d\hbox {} squared prediction error in and out of the training sample for polynomial orders 1 through 10.\relax }}{21}}
\newlabel{fig:polynomials}{{1.1}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Balancing Fit and Complexity: Bayesian Occam's Razor}{22}}
\newlabel{sec:balanc-fit-compl}{{1.2.4}{22}}
\newlabel{eq:2}{{1.3}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Clustering via a Mixture of Gaussians Model}{24}}
\newlabel{sec:clustering}{{1.3}{24}}
\newlabel{eq:gmm}{{1.4}{24}}
\citation{geman1984stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Clustering Sequential Data: The Hidden Markov Model}{25}}
\citation{baum1966statistical}
\citation{rabiner1986introduction}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Model Selection in HMMs and the Mixture of Gaussians Model}{27}}
\newlabel{sec:model-select-mixt}{{1.5}{27}}
\citation{rosenblatt1956remarks,parzen1962estimation}
\@writefile{toc}{\contentsline {chapter}{\chapterline {2} \MakeUppercase  {Bayesian Nonparametric Models and the Infinite Hidden Markov Model} }{29}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:HMM-NPBayes}{{2}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Parametric Vs. Nonparametric Models}{29}}
\newlabel{sec:param-vs.-nonp}{{2.1}{29}}
\citation{epanechnikov1969non}
\citation{ferguson1973bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Dirichlet Process}{30}}
\newlabel{sec:dirichlet-process}{{2.2}{30}}
\citation{ferguson1973bayesian}
\newlabel{eq:4}{{2.2}{31}}
\citation{ferguson1973bayesian}
\citation{sethuraman1994constructive}
\citation{sethuraman1994constructive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Normalized Gamma Process representation of the DP}{32}}
\newlabel{sec:norm-gamma-proc}{{2.2.1}{32}}
\newlabel{eq:8}{{2.3}{32}}
\newlabel{eq:3}{{2.4}{32}}
\newlabel{eq:9}{{2.5}{32}}
\newlabel{eq:1}{{2.6}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The Stick-Breaking Process construction of the DP}{33}}
\newlabel{sec:stick-break-proc}{{2.2.2}{33}}
\newlabel{stick-step-1}{{1}{33}}
\newlabel{stick-step-2}{{2}{33}}
\newlabel{stick-step-3}{{3}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}The Chinese Restaurant Process}{33}}
\newlabel{sec:chin-rest-proc}{{2.2.3}{33}}
\citation{teh2011dirichlet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}The Dirichlet Process Mixture Model}{34}}
\newlabel{eq:gmm-2}{{2.8}{34}}
\newlabel{eq:DP-prior}{{2.9}{34}}
\citation{fox2012tutorial}
\citation{blei2006variational}
\citation{kurihara2007collapsed}
\newlabel{eq:5}{{2.10}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}A Gibbs Sampler for DP Mixture Models}{35}}
\newlabel{sec:gibbs-sampler-dp}{{2.2.5}{35}}
\newlabel{sec:dirichl-proc-gauss}{{2.2.5}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}An Infinite State HMM}{36}}
\newlabel{sec:an-infinite-state}{{2.3}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The Hierarchical Dirichlet Process}{36}}
\newlabel{sec:hier-dirichl-proc}{{2.3.1}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}The HDP-HMM}{36}}
\newlabel{sec:hdp-hmm}{{2.3.2}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Inference in Finite and Infinite State HMMs}{36}}
\newlabel{sec:inference-hdp-hmm}{{2.4}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}The Forward Backward Algorithm}{36}}
\newlabel{sec:forw-backw-algor-1}{{2.4.1}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Two Gibbs Samplers for the HDP-HMM}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Beam Sampling in the HDP-HMM}{36}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {chapter}{\chapterline {3}\MakeUppercase  {HaMMLeT: An infinite Hidden Markov Model with Local Transitions}}{37}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:HaMMLeT}{{3}{37}}
\citation{teh2006hierarchical}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Transition Dynamics in the HDP-HMM}{38}}
\newlabel{sec:transition-dynamics}{{3.1}{38}}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\newlabel{eq:1}{{3.3}{39}}
\newlabel{eq:4}{{3.4}{39}}
\newlabel{eq:5}{{3.5}{39}}
\newlabel{eq:6}{{3.6}{39}}
\citation{ferguson1973bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}An HDP-HMM With Local Transitions}{40}}
\@writefile{toc}{\contentsline {paragraph}{Notational Conventions}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}A Normalized Gamma Process representation of the HDP-HMM}{41}}
\newlabel{sec:normalized-gamma}{{3.2.1}{41}}
\newlabel{eq:17}{{3.7}{41}}
\newlabel{eq:18}{{3.8}{41}}
\newlabel{eq:16}{{3.9}{41}}
\newlabel{eq:19}{{3.10}{41}}
\newlabel{eq:20}{{3.11}{41}}
\newlabel{eq:21}{{3.12}{41}}
\newlabel{eq:22}{{3.13}{41}}
\newlabel{eq:23}{{3.15}{41}}
\newlabel{eq:50}{{3.16}{41}}
\newlabel{eq:24}{{3.17}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Promoting ``Local'' Transitions}{42}}
\newlabel{sec:prom-local-trans}{{3.2.2}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The HDP-HMM-LT as a continuous-time Markov Jump Process with ``failed'' jumps}{44}}
\newlabel{sec:dist-based-filt}{{3.3}{44}}
\newlabel{eq:beta}{{3.24}{44}}
\newlabel{eq:51}{{3.27}{45}}
\newlabel{eq:52}{{3.28}{45}}
\newlabel{eq:53}{{3.29}{45}}
\newlabel{eq:56}{{3.30}{46}}
\newlabel{eq:60}{{3.34}{46}}
\citation{fox2008hdp}
\citation{johnson2013bayesian}
\newlabel{eq:54}{{3.35}{47}}
\newlabel{eq:joint-likelihood}{{3.40}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}An HDP-HSMM-LT modification}{47}}
\newlabel{sec:an-hsmm-modification}{{3.3.1}{47}}
\citation{johnson2013bayesian}
\citation{johnson2013bayesian}
\newlabel{eq:95}{{3.41}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Summary}{49}}
\newlabel{sec:model-summary}{{3.3.2}{49}}
\newlabel{eq:96}{{3.43}{49}}
\newlabel{eq:likelihood}{{3.49}{49}}
\newlabel{eq:97}{{3.50}{49}}
\newlabel{eq:98}{{3.52}{49}}
\newlabel{eq:likelihood-hsmm}{{3.53}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}MCMC Inference in the ``Failed Jumps'' Representation}{49}}
\newlabel{sec:inference}{{3.4}{49}}
\citation{ishwaran2000markov}
\newlabel{eq:28}{{3.54}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Sampling $\pi $, $\beta $, $\alpha $ and $\gamma $}{50}}
\newlabel{sec:sampling-pi}{{3.4.1}{50}}
\newlabel{eq:46}{{3.55}{50}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\pi $}{51}}
\newlabel{eq:47}{{3.56}{51}}
\newlabel{eq:61}{{3.57}{51}}
\newlabel{eq:24}{{3.59}{51}}
\newlabel{eq:25}{{3.60}{51}}
\newlabel{sec:sampling-bbeta}{{3.4.1}{51}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\beta $}{51}}
\newlabel{eq:62}{{3.61}{51}}
\citation{teh2006hierarchical}
\newlabel{eq:31}{{3.67}{52}}
\newlabel{eq:32}{{3.68}{52}}
\newlabel{eq:33}{{3.69}{53}}
\newlabel{eq:34}{{3.70}{53}}
\newlabel{eq:38}{{3.71}{53}}
\newlabel{sec:sampling-alpha}{{3.4.1}{53}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\alpha $ and $\gamma $}{53}}
\newlabel{eq:42}{{3.72}{53}}
\newlabel{eq:43}{{3.79}{54}}
\newlabel{eq:8}{{3.80}{54}}
\newlabel{eq:44}{{3.81}{54}}
\newlabel{eq:9}{{3.82}{54}}
\citation{johnson2013bayesian}
\newlabel{eq:10}{{3.84}{55}}
\newlabel{eq:11}{{3.85}{55}}
\newlabel{eq:18}{{3.86}{55}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{55}}
\newlabel{eq:100}{{3.87}{55}}
\newlabel{eq:46}{{3.88}{55}}
\newlabel{eq:64}{{3.89}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Sampling $z$ and the auxiliary variables}{55}}
\newlabel{sec:sampling-z_t}{{3.4.2}{55}}
\newlabel{eq:19}{{3.93}{56}}
\newlabel{eq:48}{{3.95}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Sampling state and emission parameters}{56}}
\newlabel{sec:sampling-eta}{{3.4.3}{56}}
\newlabel{eq:65}{{3.100}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Use Cases}{57}}
\@writefile{toc}{\contentsline {chapter}{\chapterline {4}\MakeUppercase  {Binary Vector States: Speaker Diarization}}{59}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:cocktail-party}{{4}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Binary State Vectors}{59}}
\newlabel{eq:39}{{4.1}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Additional Inference Steps}{60}}
\newlabel{sec:sampling-eta}{{4.1.1}{60}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\theta $}{60}}
\newlabel{eq:1}{{4.2}{60}}
\newlabel{eq:68}{{4.3}{60}}
\newlabel{eq:70}{{4.4}{60}}
\newlabel{eq:71}{{4.5}{60}}
\newlabel{eq:72}{{4.6}{61}}
\newlabel{eq:74}{{4.7}{61}}
\newlabel{eq:73}{{4.8}{61}}
\newlabel{eq:91}{{4.9}{61}}
\newlabel{eq:77}{{4.10}{62}}
\newlabel{sec:sampling-bmu}{{4.1.1}{62}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\mu $}{62}}
\newlabel{eq:92}{{4.15}{62}}
\newlabel{eq:93}{{4.16}{62}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $W$ and $\Sigma $}{62}}
\newlabel{eq:78}{{4.17}{62}}
\newlabel{eq:79}{{4.18}{62}}
\citation{gilks1992adaptive}
\newlabel{eq:80}{{4.19}{63}}
\newlabel{sec:sampling-lambda}{{4.1.1}{63}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\lambda $}{63}}
\newlabel{eq:88}{{4.20}{63}}
\newlabel{eq:88}{{4.21}{63}}
\newlabel{eq:90}{{4.22}{63}}
\newlabel{eq:94}{{4.23}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Summary}{63}}
\newlabel{sec:summary}{{4.1.2}{63}}
\newlabel{eq:101}{{4.30}{64}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}``Cocktail Party'' Data}{65}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Synthetic Data Without Local Transitions}{67}}
\newlabel{sec:synth-data-without}{{4.3}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces (a-b) Accuracy and F1 scores for the HDP-HMM-LT, standard HDP-HMM, and Binary Factorial HMM on the Cocktail Party Data. Metrics are averaged over 10 Gibbs runs on each model, with error bars representing a 99\% confidence interval for the mean per iteration. The first 100 iterations are not shown. (c) Learned similarity parameter, $\lambda $, for the LT model, (d) Number of distinct states used by HDP-HMM and HDP-HMM-LT. The first 100 iterations are excluded.\relax }}{68}}
\newlabel{fig:cocktail-results}{{4.1}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces (a-b) Accuracy and F1 for the three models on data generated from an HDP-HMM without local transitions, (c) Learned similarity parameter, $\lambda $, for the LT model, (d) Number of states used by the HDP-HMM and HDP-HMM-LT. The first 100 iterations are omitted.\relax }}{69}}
\newlabel{fig:synthetic-results}{{4.2}{69}}
\@writefile{toc}{\contentsline {chapter}{\chapterline {5}\MakeUppercase  {Categorical Vector States: Power Disaggregation}}{70}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:REDD}{{5}{70}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Generalizing to Categorical-Valued $\theta $}{70}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Priors and Representations in the Categorical State Variant}{71}}
\newlabel{sec:priors-repr-categ}{{5.2}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Adapting Posterior Inference for Categorical State Vectors}{72}}
\newlabel{sec:adapt-post-infer}{{5.3}{72}}
\@writefile{toc}{\contentsline {subsubsection}{Sampling $\theta $}{72}}
\citation{kolter2011redd}
\@writefile{toc}{\contentsline {paragraph}{Sampling $W$}{73}}
\@writefile{toc}{\contentsline {paragraph}{Sampling $\lambda $}{73}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Power Disaggregation}{73}}
\newlabel{sec:power-disaggregation}{{5.4}{73}}
\citation{kolter2011redd}
\citation{kolter2011redd}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A sample data interval from the REDD dataset \cite  {kolter2011redd}. The top channel contains the total measured power in watts consumed by a home during a period of approximately 24 hours. Each timestep represents a 20 second intervals, during which the amplitude recorded is a median of the amplitudes in the original higher resolution data.\relax }}{75}}
\newlabel{fig:redd-data-example}{{5.1}{75}}
\citation{duane1987hybrid}
\citation{neal2011mcmc}
\@writefile{toc}{\contentsline {chapter}{\chapterline {6}\MakeUppercase  {Separate Similarities and Emissions: Learning Tonal Grammar in Music}}{76}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:music}{{6}{76}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Separable Similarity and Emissions}{76}}
\newlabel{sec:separ-simil-emiss}{{6.1}{76}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}A Hamlitonian Monte Carlo step to sample $\eta $}{76}}
\newlabel{sec:haml-monte-carlo}{{6.2}{76}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Synthetic Data from an HMM with a Nearly Block Diagonal Transition Matrix}{79}}
\newlabel{sec:synthetic-data-from}{{6.3}{79}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Discovering Chord Equivalence Classes in Tonal Music}{80}}
\newlabel{sec:disc-chord-equiv}{{6.4}{80}}
\bibstyle{apalike}
\bibdata{../bib/supplement}
\@writefile{toc}{\contentsline {chapter}{\chapterline {7}\MakeUppercase  {Conclusions and Future Work}}{81}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lot}{\addvspace {\medskipamount }}
\newlabel{chapter:discussion}{{7}{81}}
\bibcite{baum1966statistical}{{1}{1966}{{Baum and Petrie}}{{}}}
\bibcite{beal2001infinite}{{2}{2001}{{Beal et~al.}}{{}}}
\bibcite{blei2006variational}{{3}{2006}{{Blei and Jordan}}{{}}}
\bibcite{box1987empirical}{{4}{1987}{{Box et~al.}}{{}}}
\bibcite{duane1987hybrid}{{5}{1987}{{Duane et~al.}}{{}}}
\bibcite{epanechnikov1969non}{{6}{1969}{{Epanechnikov}}{{}}}
\bibcite{ferguson1973bayesian}{{7}{1973}{{Ferguson}}{{}}}
\bibcite{fox2012tutorial}{{8}{2012}{{Fox and Roberts}}{{}}}
\bibcite{fox2008hdp}{{9}{2008}{{Fox et~al.}}{{}}}
\bibcite{geman1984stochastic}{{10}{1984}{{Geman and Geman}}{{}}}
\bibcite{gilks1992adaptive}{{11}{1992}{{Gilks and Wild}}{{}}}
\bibcite{ishwaran2000markov}{{12}{2000}{{Ishwaran and Zarepour}}{{}}}
\bibcite{johnson2013bayesian}{{13}{2013}{{Johnson and Willsky}}{{}}}
\bibcite{kolter2011redd}{{14}{2011}{{Kolter and Johnson}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{82}}
\@writefile{lof}{\addvspace {\medskipamount }}
\@writefile{lof}{\addvspace {\medskipamount }}
\bibcite{kurihara2007collapsed}{{15}{2007}{{Kurihara et~al.}}{{}}}
\bibcite{lindsay1995mixture}{{16}{1995}{{Lindsay}}{{}}}
\bibcite{mclachlan2004finite}{{17}{2004}{{McLachlan and Peel}}{{}}}
\bibcite{neal2011mcmc}{{18}{2011}{{Neal et~al.}}{{}}}
\bibcite{paisley2011discrete}{{19}{2011}{{Paisley et~al.}}{{}}}
\bibcite{parzen1962estimation}{{20}{1962}{{Parzen}}{{}}}
\bibcite{rabiner1986introduction}{{21}{1986}{{Rabiner and Juang}}{{}}}
\bibcite{rasmussen2000infinite}{{22}{2000}{{Rasmussen}}{{}}}
\bibcite{rosenblatt1956remarks}{{23}{1956}{{Rosenblatt et~al.}}{{}}}
\bibcite{sethuraman1994constructive}{{24}{1994}{{Sethuraman}}{{}}}
\bibcite{teh2011dirichlet}{{25}{2011}{{Teh}}{{}}}
\bibcite{teh2006hierarchical}{{26}{2006}{{Teh et~al.}}{{}}}
\bibcite{wasserman2007nonparametric}{{27}{2007}{{Wasserman}}{{}}}
