\section{Parametric Vs. Nonparametric Models}
\label{sec:param-vs.-nonp}

Models that can be identified using a finite set of values
(that is, parameters) are called {\bf parametric} models.  Their
complexity and expressivity is the same whether they are fit using 10
data points, $10^6$ data points, or $10^10$ data points.  Most
canonical statistical models are parametric: the parameters in a
regression model comprise the regression coefficients and the
parameters of the residual distribution.  The parameters of the
mixture of Gaussians model comprise vector, $\pi$, of $K$ mixing weights,
and the parameters of the individual component Normal distributions:
$\{\mu_k, \sigma^2_k\}, k = 1, \dots, K$.

In a parametric model, once the sample size is a few orders of
magnitude larger than the number of parameters, the gains made by
further increasing the sample size, which in a frequentist setting
comes in the form of narrower confidence sets, and in a Bayesian
setting comes in the form of reduced posterior variance, are typically
negligible: the errors in prediction due to inevitable model misspecification, and
practical concerns with generalizability to new data sets, overwhelm
the remaining decimal places of uncertainty.

The name {\bf nonparametric model} is something of a misnomer, in that
nonparametric models do not have {\em no} parameters (a model with no
parameters would by definition be unable to learn anything from data);
rather, they have a number of parameters which grows adaptively as the
sample size increases.

A common frequentist family of nonparametric models are kernel-smoothed density
estimators \citep{rosenblatt1956remarks, parzen1962estimation}, 
in which the probability density of a data-generating process is
estimated by taking the empirical distribution and ``smoothing'' it to
obtain the estimated density at $y$ by
averaging together the values nearby points using a weight kernel.  As
a result, the estimated density requires $n$ values to specify it,
where $n$ is the sample size.  As such, the complexity of the family
of distributions in the model space grows with the sample size, unlike
in a parametric model.  Here, the tradeoff between bias and variance
is controlled not by the complexity of the model space, but rather by
the choice of smoothing bandwith: how much of the density at each
point is ``borrowed'' from nearby locations, with one extreme
representing a bandwidth of zero, in which case the estimate is simply
the empirical distribution.

It is natural to interpret the kernel density estimator as a mixture
model, where there is a mixture component centered at each data point
whose distribution belongs to a family defined by the kernel function:
for example, the Gaussian kernel results in a mixture of Gaussians,
the Epanechnikov kernel \citep{epanechnikov1969non} 
results in a mixture of Epanechnikov distributions, and the Uniform
kernel results in a mixture of Uniform distributions.  On the
interpretation that each component in a mixture model represents a
qualitatively distinct class of data, this means that each point is
viewed as qualitatively distinct, which from a Bayesian perspective
is unsatisfying.  Instead, we would like a model whose complexity
grows more slowly than linear in the sample size, such that as we
collect additional data it is always possible to encounter something
qualitatively new, but where the chances of doing so diminish as we
have more and more data.  This can be accomplished by employing a {\bf
Dirichlet Process} as the prior on the set of mixture components.

\section{The Dirichlet Process}
\label{sec:dirichlet-process}

The Dirichlet Process (DP) was formally defined by
\citet{ferguson1973bayesian} as a random probability measure, $G$, over a
$\mathcal{X}$ equipped with the sigma-algebra $\mathcal{A}$, with the
defining property that, for any partition of $\mathcal{X}$ consisting
of measureable sets, $A_1, A_2 \dots, A_k$, the random distribution given
by the probabilities
\begin{equation*}
  \{P(A_1), P(A_2), \dots, P(A_k)\}
\end{equation*}
has a Dirichlet distribution.

A Dirichlet distribution is defined by a mean distribution, $\pi_1,
\pi_2, \dots, \pi_k; \sum_i \pi_i = 1$, and a concentration parameter,
$\alpha$, which acts as an inverse variance parameter: as $\alpha$
goes to infinity, draws from the Dirichlet distribution are
distributions close to the mean with increasingly high probability.

The Dirichlet Process is also defined by a mean distribution $G_0$, called a
{\bf base measure}, and a
concentration parameter $\alpha$, but since the DP induces a Dirichlet
distribution over {\em any} finite partition of $\mathcal{X}$, the
mean distribution must be defined on all measureable
sets in $\mathcal{A}$.  We will write
\begin{equation}
G \sim \DP{\alpha G_0}
\end{equation}
to indicate that the random measure $G$ is distributed according to a
Dirichlet Process with base measure $G_0$ and concentration parameter $\alpha$.
Then, concretely, we have
\begin{equation*}
    P(A_1), P(A_2), \dots, P(A_k) \sim \Dir{\alpha G_0(A_1), \dots,
      \alpha G_0(A_k)}
\end{equation*}
An important probability of the DP is that the resulting measure is
discrete almost surely, and hence it is a sensible choice as a prior
on mixture components.  An equally important property when it comes to
using a DP as a Bayesian prior is that it is a {\em conjugate prior}
to a discrete likelihood. That is, if $n$ observations, $Y_1, \dots,
Y_n$, are drawn from
some unkown discrete probability measure $G$, and the prior employed
for $G$ is a Dirichlet Process, then the posterior distribution on $G$
is also a DP, whose base measure is a weighted sum of the prior base
measure, $G_0$, and the empirical distribution, $\hat{F}_n$:
\begin{equation}
  \label{eq:4}
  G \given Y \sim \alpha G_0 + n \hat{F}_n
\end{equation}
and whose concentration parameter is simply $\alpha + n$.

\subsection{The Normalized Gamma Process representation of the DP}
\label{sec:norm-gamma-proc}

\citet{ferguson1973bayesian} also showed that the Dirichlet Process
arises by normalizing a {\bf Gamma Process}.    
A Gamma Process is a stochastic point process on $\mathbb{R}^{+} \times
  \Theta$ which is defined by a {\bf L\'evy intensity measure}:
  \begin{equation}
    \label{eq:8}
    \nu(d\pi, d\theta) = \alpha \pi^{-1} e^{-\pi} d\pi G_0(d\theta)
  \end{equation}

  A realization of a Gamma process is a collection of point masses
  $\{\pi_k, \theta_k\}$, where $\pi_k \in \mathbb{R}^{+}$ is the mass
  associated with the point at location $\theta_k \in \Theta$.  This
  collection can be used to define a measure, $\mu$, on $\Theta$,
  where
  \begin{equation}
    \label{eq:3}
    \mu(A) = \sum_{k: \theta_k \in A} \pi_k
  \end{equation}

  The number $n(A)$ of point masses in a region $A \subset \mathbb{R}^+ \times \Theta$ is
  distributed as
  \begin{equation}
    \label{eq:9}
    n(A) \sim \Pois{\int_A \nu(d\pi, d\theta)}
  \end{equation}

  The L\'evy intensity measure of the Gamma process satisfies
  conditions to guarantee that the sum of all of the $\pi_k$
  weights is finite with probability 1, and therefore the measure $G$
  defined by
  \begin{equation}
    \label{eq:1}
    G = \frac{\mu}{\sum_k \pi_k}
  \end{equation}
  is a valid probability measure.  \citet{ferguson1973bayesian} showed
  that this probability measure is a DP with base measure $G_0$ and
  concentration $\alpha$, where
  these are the measure and parameter used in defining the L\'evy intensity of the
  Gamma process.

  Although the formal definition of the DP is well-defined, and
  although the Normalized Gamma Process representation guarantees
  existence of the DP, neither of these is terribly useful in {\em
    constructing} a DP, which limits the usefulness of the DP in
  applied modeling.  Fortunately, a constructive definition of the DP
  was discovered by \citet{sethuraman1994constructive}, using what is
  known as a {\bf Stick-Breaking Process}.

  \subsection{The Stick-Breaking Process construction of the DP}
  \label{sec:stick-break-proc}

  As shown by \citet{sethuraman1994constructive}, we can generate a
  draw from a Dirichlet Process by iteratively sampling the $\pi_k$
  weights, and then placing a point mass with weight $\pi_k$ at a
  location in $\Theta$ independently drawn from $G_0$.  This process
  is called a {\bf Stick-Breaking Process}.  By using the following
  algorithm to select the stick weights, the resulting collection of
  point masses has a Dirichlet Process.

  Having drawn $k-1$ point masses $(\pi_1, theta_1), \dots, (\pi_{k-1}, \theta_{k-1})$,
  \begin{enumerate}
  \item \label{stick-step-1} Draw $\tilde{\pi}_{k} \sim \Beta{1}{\alpha}$.
  \item \label{stick-step-2} Set $\pi_k = \tilde{\pi}_k \prod_{k'=1}^{K-1}
    (1 - \pi_{k'})$.
  \item \label{stick-step-3} Draw $\theta_k \sim G_0$.
  \end{enumerate}
  where, when $k = 1$, the null product in step \ref{stick-step-2} is
  1.

  The choice of $G_0$ and $\alpha$ determine the resulting DP.

  When describing the stick-breaking part of this process by itself
  --- that is, the process that produces the weights, $\pi_1, \pi_2,
  \dots$, it is common to write
  \begin{equation}
    \label{eq:11}
    \pi \sim \GEM{\alpha}
  \end{equation}
  where GEM stands for Griffiths-Engen-McCloskey, the names of three
  authors who did early work on stick-breaking processes and laid the foundation
  for the connection to Dirichlet Processes later formalized by
  \citet{sethuraman1994constructive}.
  
  \subsection{The Chinese Restaurant Process}
  \label{sec:chin-rest-proc}

  A useful construction for doing inference in a Dirichlet
  Process-based model is based on the metaphor of customers sharing
  food at a Chinese restaurant, which is normally described as
  follows.

  One by one, customers enter a Chinese restaurant and either sit at
  an unoccupied table and order a dish for the table, or join a table
  with other customers and share whatever dish is at the table.  The first
  customer necessarily starts a new table.  Subsequent customers join
  table $k$ with probability proportional to $n_k$, where $n_k$ is the
  number of customers currently seated at that table, and sit at an
  unoccupied table with probability proportional to a parameter
  $\alpha$.  

  More formally, the $n+1$th customer to enter the restaurant
  is assigned to table $k$ according to the distribution
  \begin{align}
    P(t_{n+1} = k) =
    \begin{cases}
      \frac{n_k}{n + \alpha} & k = 1, \dots, K \\
      \frac{\alpha}{n + \alpha} & k = K + 1
    \end{cases}
  \end{align}
  For each new table, a dish, $\theta_k$ is sampled from a base
  distribution, $G_0$.

  It turns out that the distribution of the collection of 
  assignments of dishes to customers is the same as the {\em marginal}
  distribution of draws from a random measure $G$ which is distributed
  $\DP{\alpha G_0}$ (see \citet{teh2011dirichlet} for a proof of this
  fact as well as a review of the theory of DPs in general).

  Notice that the Chinese Restaurant Process (CRP) has the property
  that the more often a dish has already been selected, the more
  likely it is to be selected again: that is, it has a ``rich get
  richer'' quality.  This makes sense in terms of the marginal
  distribution of draws from a DP-distributed random measure since,
  the more observations there are at a particular location, the
  stronger the evidence that the mass under $G$ at that location is
  large, and hence, the larger the posterior predictive probability at
  that location (marginalizing over $G$).

  \subsection{The Dirichlet Process Mixture Model}

  We are now ready to define a nonparametric version of a Bayesian
  Gaussian Mixture Model which replaces the Dirichlet distribution
  prior on the collection of mixture components from the fixed $K$ mixture model 
  with a Dirichlet Process prior.

  Reiterating the model initially defined in \eqref{eq:gmm}, suppose
  we have a model of the form
  \begin{equation}
    \label{eq:gmm-2}
    f(y \given \pi, \theta) = \sum_{k=1}^\infty \pi_k f(y \given \theta_k)
  \end{equation}
  where $\theta = (\theta_1, \theta_2, \dots)$ are some parameters
  governing the mixture components, and $\pi = (\pi_1, \pi_2, \dots)$ is
  the vector of mixing weights that sum to 1.  We now drop the
  assumption that all but finitely many of the $\pi_k$ are zero, and
  instead adopt as a prior:
  \begin{align}
    \label{eq:DP-prior}
    \{(\pi_k, \theta_k)_{k=1}^\infty\} \sim \DP{\alpha G_0}
  \end{align}
  As before, we introduce indicator variables, $\{z_i\}_{i=1}^n$ so
  that we can write
  \begin{align}
    \label{eq:5}
    P(z_i = k) = \pi_k, \qquad i = 1, \dots, n; k = 1, 2, \dots \\
    y_i \given z_i \sim F(\theta_{z_i})
  \end{align}
  where $F$ is a parametric family parameterized by $\theta$. For
  example, if $F$ is Normal, then $\theta$ might represent the mean
  vector and covariance matrix, and we might choose $G_0$ to be a
  Normal-Inverse Wishart distribution (or a non-conjugate choice as
  the application suggests).

  \subsection{Two Gibbs Samplers for DP Mixture Models}
  \label{sec:gibbs-sampler-dp}

  Given data, $y = (y_1, \dots, y_n)$ modeled using the mixture
  model in \eqref{eq:gmm-2} with the prior in \eqref{eq:DP-prior}, 
  we want to be able to make inferences about the parameters, $(\pi_k,
  \theta_k)$.  As with all but the simplest models, the full posterior
  is not amenable to exact analysis, and so we must resort to
  approximation to compute quantities of interest.  There are two
  dominant approximation methods in Bayesian inference.  One is variational
  inference, in which the true posterior is replaced by a distribution
  which is more amenable to analysis and which is in some sense as
  close as possible to the true distribution (usually the objective is
  to minimize the KL divergence from the target to the
  approximation).  The second popular approximation technique is
  Markov Chain Monte Carlo (MCMC), in which integrals involving the posterior
  are computed based on a sample from the posterior drawn using a
  Markov Chain whose transition kernel is chosen so as to yield the
  true posterior as the unique stationary distribution.  In a
  nutshell, variational Bayes provides an exact calculations based on an
  approximate distribution, whereas MCMC provides approximate
  calculations based on the true posterior.  I will focus on MCMC in
  this dissertation \citet{fox2012tutorial} for a tutorial on
  variational Bayes generally and \citet{blei2006variational} and
  \citet{kurihara2007collapsed} for work on variational methods for
  Dirichlet Process mixture models in particular.
  
  As with the finite mixture model, MCMC inference is greatly
  simplified by sampling over the $z_i$ indicators as well as over the
  mixture component parameters themselves (indeed, in some
  applications, these indicators may be the variables of primary
  interest).

  % The standard Gibbs sampling algorithm proceeds as follows
  % \begin{algorithm}
  %   \caption{Gibbs sampler for a Generic DP Mixture Model}
  %   \begin{algorithmic}[1]
  %     \State{Initialize the state labels, $z_1, \dots, z_n$ (e.g. from a
  %       a CRP)}
  %     \State{Define $K = \max_{i}\{z_i\}$}
  %     \For{Iteration $m = 1, \dots, M$}
  %         \For{$k = 1, \dots, K$}
  %         \State{Sample $\theta_k \given \{y_i: z_i = k\}$}
  %         \EndFor
  %     \EndFor
  %   \end{algorithmic}
  % \end{algorithm}
  
  We are interested in the joint posterior over the emission parameters,
  $\{\theta_k\}$, the component weights, $\{\pi_k\}$, and the component
  indicators, $\{z_i\}$, where $k$ indexes components and $i$ indexes
  observations.

  \paragraph{Method 1: A Collapsed Sampler Based on the CRP}
  One option is to sample only $\theta$ and $z$, integrating out
  $\pi$.  This is possible since the marginal distribution of $z
  \given \theta$ is a Chinese Restaurant Process.

  In this approach, we sample each entry of $z$ one at a time conditioned on the
  others.  Let $z^{(-i)}$ be the vector of component indicators
  excluding $z_i$, let $n_k^{(-i)} = \sum_{i' \neq i} \mathbb{I}(z_i = k)$ be the count
  of the number of $z_{i'}$ currently assigned to component $k$, not
  counting $z_i$, and let $K$ count the number of distinct values in
  $z$ among the other $n-1$ indicators (we will assume by permuting
  the labels that the distinct values are numbered $1$ through $K$).  Then we have
  \begin{align}
    p(z_i  = k \given z^{(-i)}) =
    \begin{cases}
      \frac{n_k}{n + \alpha}& \quad k = 1, \dots, K \\
      \frac{\alpha}{n + \alpha} & \quad k = K + 1
    \end{cases}
  \end{align}
  where if $z_i$ takes on a distinct value from any of the other
  entries in $z$, we call this value $K + 1$.

  In order to sample $z_i$ conditioned on both $z^{(-i)}$ and the
  data, we also need to compute the likelihood $p(y_i \given z_i, \theta)$ for
  each value of $z_i$.  Generically, we simply have
  \begin{align}
    p(y_i \given z_i, \theta) = f(y_i \given \theta_{z_i})
  \end{align}
  where $f(\cdot \given \theta_{z_i})$ is the density (or mass)
  function corresponding to the distribution $F(\theta_{z_i})$.

  Thus we sample $z_i$ with probabilities
  \begin{align}
    p(z_i  = k \given z^{(-i)}, y_i, \theta) \propto
    \begin{cases}
      \frac{n_k}{n + \alpha} f(y_i \given \theta_k) & \quad k = 1,
      \dots, K \\
      \frac{\alpha}{n + \alpha} \int_{\Theta} f(y_i \given \theta_*)
      p(\theta_*)\ d\theta_* & \qquad k = K+1
    \end{cases}
  \end{align}
  where depending on the form of $f$, it may be able to compute the
  integral for the marginal likelihood of $y_i$ analytically; but if
  not, we can approximate it by sampling a value of $\theta_*$ from
  the prior and using
  \begin{equation}
    \label{eq:1}
    p(z_i = K + 1 \given \theta, y) \propto \frac{\alpha}{n + \alpha} f(y_i \given theta_*)
  \end{equation}
  If $z_i$ is set to $K + 1$, we increment $K$.  If $z_i$ was
  previously the only indicator assigned to some value $k$, relabel
  the indicators to occupy consecutive natural numbers.

  Conditioned on $z$, sampling $\theta$ amounts to $K$ independent
  updates of the parameters of the $K$ separate components, each using
  the likelihood of the observations currently assigned to that
  component.  If the prior on $\theta$ is conjugate to the likelihood,
  this involves $K$ samples from an exponential family.

  \paragraph{Method 2: An uncollapsed sampler based on the
    Stick-Breaking Process}

  Alternatively we might choose to sample a finite subset of the
  entries in $\pi$ directly.  

  Conditioned on a current set of
  assignments to the $z$ indicators, we again let $K$ represent the
  number of distinct values taken on by the $z$, again assuming that
  the labels have been re-numbered as needed to take the values $1$
  through $K$, and again let $n_k, k = 1, \dots, K$ be the counts of
  the $z_i$ assigned to each $k$.  
  We can then sample the weights associated with each currently
  represented component, given by $\pi_1, \dots, \pi_K$, as well as
  the total weight associated with all unrepresented components
  combined, $\pi_{new}$.  Given the $n_k$, we have
  \begin{equation}
    \label{eq:2}
    (\pi_1, \dots, \pi_K, \pi_{new}) \sim \Dir{n_1, n_2, \dots, n_K, \alpha}
  \end{equation}
  We may then sample each $z_i$ according to
  \begin{align}
    p(z_i = k \given \pi, \theta, y) \propto
    \begin{cases}
      \pi_k f(y_i \given \theta_k) & \quad k = 1, \dots, K \\
      \pi_{new} \int_\Theta f(y_i \given \theta_*) p(\theta_*)\
      d\theta_* & k > K
    \end{cases}
  \end{align}
  where each time some $z_i$ is assigned to a new component, we must
  instantiate a new $\pi_{K+1}$ using the stick-breaking process, by sampling
  \begin{equation}
    \label{eq:6}
    \tilde{\pi}_{K+1} \sim \Beta{1}{\alpha},
  \end{equation}
  set $\pi_{K+1} := \pi_{new} \tilde{\pi}_{K+1}$, and set $\pi_{new}
  := 1 - \sum_{k=1}^{K+1} \pi_k$, and then increment $K$ before
  sampling the next $z_i$.

  Sampling $\theta$ is exactly as in the collapsed sampler, since
  the distribution depends only on $z$ and $y$.

\section{An Infinite State HMM}
\label{sec:an-infinite-state}

We would like to adapt the nonparametric DP mixture model to the
sequential setting to define an infinite state HMM 
in an analogous way to that in which the finite
mixture model was adapted to create a finite state HMM.  There, the
link between the mixture model and the $K$-state HMM was that the HMM
consisted of $K$ separate $K$-state mixture models: one associated
with each state.  That is, after visiting state $k$, the next
observation is drawn from mixture model $k$.

Naively, then, we might then try to define a countably infinite set of
mixture models, each with a DP prior, and after visiting component
$k$, draw the next observation from mixture model $k$.  However, if
the prior on the emission parameters $\theta$ is continuous, then with
probability 1 the set of mixture components will be non-overlapping,
and hence we will never revisit the same component twice.

% Perhaps instead we should draw only the mixing weights (that is, the
% transition probabilities) separately for
% each preceding state, and draw $\theta_j$ only once per
% column.  However, this will not do what we want either.  By drawing 
% probabilities from a Stick-Breaking
% Process we will tend to get probabilities that decrease as we go, and
% so by sampling the entries in each row of the transition matrix in
% order, we are introducing a bias for every state to transition toward
% states with low indices.  

\subsection{The Hierarchical Dirichlet Process}
\label{sec:hier-dirichl-proc}

The key property that we want in an Infinite State HMM needs to have
is that the mixture distributions need to be coupled --- that is, they
need to share a set of components.  In order to accomplish this using
a Dirichlet Process prior on the mixture parameters, the base measure
needs to be discrete.

\subsection{The HDP-HMM}
\label{sec:hdp-hmm}

\citet{teh2006hierarchical} defined the {\bf Hierarchical Dirichlet
  Process} (HDP), in which a collection of Dirichlet Processes take as
a common base measure a distribution itself drawn from a DP.  This
model is then hierarchical in the same sense as a Bayesian hierarchical regression
model, in which data sets from a collection of sources are assumed to
have similar distributions, and hence are given a common prior whose
parameters are informed by all of the data across sources.  

Formally, we define
\begin{align}
  G_0 \sim \DP{\gamma H}
\end{align}
and
\begin{align}
  G_j \sim \DP{\alpha G_0}, \quad j = 1, 2, \dots, J
\end{align}
where $\gamma$ is a concentration parameter that governs the
distribution of the weights of the atoms in the shared base measure
$G_0$, with larger $\gamma$ leading to probability mass being
dispersed among a large number of atoms, and smaller $\gamma$ leading
to a concentration of mass in just a few components.  The
concentration parameter $\alpha$ for the individual measures $G_j$
governs how tightly clustered the individual $G_j$ are around the mean
base measure $G_0$, with large $\alpha$ leading to the $G_j$ being
highly similar to $G_0$, and small $\alpha$ leading to each $G_j$
placing a lot of mass in a small number of components drawn from
$G_0$, such that the average across $G_j$s still looks like $G_0$, 
but where each $G_j$ may be quite different.

While the effect of $\gamma$ is clear enough from the Stick-Breaking
Process representation of DPs, it is worth examining how it is that
the second-level concentration $\alpha$ governs the behavior of the
atoms in the individual $G_j$.  We can see this using the
Stick-Breaking Process as well.

The effect of $\alpha$ is clearest when $\gamma$ is large, so that
$G_0$ has little bits of mass at lots of different places, with
location having much mass.  To draw a $G_j$ with this discrete $G_0$ 
as a base measure, we begin with a stick of unit mass, and break off a
mass by sampling from a
$\Beta{1}{\alpha}$ distribution.  We place a mass with this weight at
a random location drawn from the countable set of possibilities given
by $G_0$.  We then draw a random breaking point for remaining mass 
from another $\Beta{1}{\alpha}$ distribution, choose another location independently
from $G_0$, and so on.  First, imagine $\alpha$ is quite small.  Then
the first mass is likely to be quite near 1, the secon is likely to
take a large share of what's left, and so on.  So $G_j$ is likely to
place most of its mass at a few locations, though {\em which}
locations those are is uncertain due to the high dispersion in $G_0$.
At the other extreme, suppose $\alpha$ is quite large.  Then each time
we break off a mass, it is likely to be quite small, and it will take
many breaks and samples from $G_0$ before we accumulate significant
probability.  As a result, by the law of large numbers, 
the distribution of this large number of small masses is likely to
mimic closely the distribution in $G_0$.

TODO: add a figure illustrating combinations of alpha and gamma

\subsection{A Stick-Breaking Representation of the Aggregate Weights
  in an HDP}

The second-level Stick-Breaking Process describes the weights of the
atoms drawn from the base measure to form each $G_j$; however, since the base measure is
discrete, we will assign more than one stick to the same location;
thus if we want to describe the total mass that $G_j$ assigns to the
location $\theta_k$, we need to account for the fact that this mass is
the accumulation of infinitely many ``sticks''.  We can get a better
handle on the distribution of these masses by considering the defining
property of a Dirichlet Process random measure: namely, that the
distribution of the values of the measure over any finite partition of
$\Theta$ is a Dirichlet distribution.

Denote by $\theta_1, \theta_2, \dots$ the locations of the atoms in
$G_0$, with associated probabilities $\beta_1, \beta_2, \dots$.
Consider the finite partition $\{\{\theta_1\}, \Theta \setminus
\{\theta_1\}\}$, and define $\pi_{jk} := G_j(\theta_k), k = 1, 2,
\dots$.  Then,
\begin{align}
  (\pi_{j1}, 1 - \pi_{j1}) \sim \Dir{\alpha
    G_0(\theta_1), \alpha G_0(\Theta \setminus \theta_1)},
\end{align}
that is,
\begin{align}
  (\pi_{j1}, 1 - \pi_{j1}) \sim \Dir{\alpha
    \beta_1, \alpha (1 - \beta_1)},
\end{align}
Since the first component of a two-component Dirichlet distribution
has a Beta distribution with the same parameters, 
this means that $\pi_{j1} \sim \Beta{\alpha \beta_1}{\alpha(1 -
  \beta_1)}$.

More generally, for any $K$, we have
\begin{align}
  (\pi_{j1}, \dots, \pi_{jK}, \sum_{k=K+1}^{\infty} \pi_{jk}) \sim
  \Dir{\alpha \beta_1, \dots, \alpha \beta_K, \alpha
    \sum_{k=K+1}^{\infty} \beta_k}
\end{align}

Thus we can construct the $\pi_{jk}$ weights directly via the
following Stick-Breaking Process.  For $k = 1, 2, \dots$,
  \begin{enumerate}
  \item \label{stick-step-1} Draw $\tilde{\pi}_{jk} \sim \Beta{\alpha
      \beta_k}{\alpha (1 - \sum_{k'=1}^k \beta_{k'})}$.
  \item \label{stick-step-2} Set $\pi_{jk} = \tilde{\pi}_k \prod_{k'=1}^{K-1}
    (1 - \pi_{k'})$.
  \end{enumerate}

\subsection{A Normalized Gamma Process Representation of the Weights
  in the HDP}
\label{sec:norm-gamma-proc-1}

A Dirichlet distribution can be constructed by normalizing a set
of Gamma random variables, where the shape parameters are equal to the
parameters of the Dirichlet, and the rate parameters are a constant
(what constant does not matter, since it will be normalized out
anyway).  So we can write
\begin{align}
  \beta &\sim \GEM{\gamma} \\
  \tilde{\pi}_{jk} &\stackrel{ind}{\sim} \Gamm{\alpha \beta_k}{1} \\
  T_j &:= \sum_{k'} \tilde{\pi}_{jk'} \\
  \pi_{jk} &:= T_j^{-1} \tilde{\pi}_{jk}
\end{align}

Since the sum of independent Gamma variates with a common rate
parameter is a Gamma variate with the shared rate and whose shape is
the sum of the shapes, we have $T_j \sim \Gamm{\alpha}{1}$, and
conditioned on $T_j$ the $\pi_{jk}$ are independent.

\subsection{Adapting the HDP for an Infinite State HMM}

We could use the HDP to define a coupled collection of mixture models,
to model clustered data in several known contexts, as
\citet{teh2006hierarchical} did to define coupled infinite mixtures of
topics in various documents.  We can also use it to define a infinite
collection of infinite mixtures in the form of an HMM with infinitely
many states.  \citet{beal2001infinite} first described an infinite HMM
without explicitly making the connection to a Hierarchical Dirichlet
Process; \citet{teh2006hierarchical} showed that, with a few
differences, the model developed by \citeauthor{beal2001infinite}
could be derived using an HDP.

In the HMM setting, the ``data sources'' indexed by $j$ in the notation in the
previous section correspond to different previous states in the hidden
Markov chain, $\theta_j$ represents the emission parameters
associated with state $j$ each hidden state, and $G_j$ represents the
transition distribution from state $j$ to all other states (which are
identified by their respective $\theta_{j'}$, and due to discreteness
of $G_0$n, are the same countably infinite set for each source
state).  We will denote by $\pi_{jj'}$ the transition probability from
state $j$ to state $j'$, where we have replaced the $k$ subscript by
$j'$ to emphasize the fact that the set of source states and the set
of destination states are the same.

Then we can define the following prior on the elements of the transition matrix of the
HDP-HMM:
\begin{align}
  \beta &\sim \GEM{\gamma} \\
  \tilde{\pi}_{jj'} &\sim \Gamm{\alpha \beta_{j'}}{1} \\
  T_j &:= \sum_{j'} \tilde{\pi}_{jj'} \\
  \pi_{jj'} &:= T_j^{-1} \tilde{\pi}_{jj'} 
\end{align}
To complete the model, define a Markov chain over state indicators,
$\{z_t\}_{t=1}^T$, with infinite transition matrix $\pi =
(\pi_{jj'})$, a prior measure, $H$, on the collection of emission parameters
$\{\theta_j\}_{j=1}^\infty$, and the likelihood $F$:
\begin{align}
  z_t \given z_{t-1} &\sim \Discrete{\pi_{z_{t-1}1},\pi_{z_{t-1}2},
    \dots} \\
  \theta_j &\stackrel{i.i.d.}{\sim} H \\
  y_t \given z_t &\sim F(\theta_j)
\end{align}

\section{Inference in Finite and Infinite State HMMs}
\label{sec:inference-hdp-hmm}

A variety of inference algorithms have been developed for both finite
state and infinite state Bayesian Hidden
Markov Models, including variational methods (see \cite{beal2003variational} for
a review of the finite-state case, and \cite{johnson2014stochastic}
for the infinite-state case), and particle filters 
\citep{fearnhead2003line, tripuraneni2015particle}, as well as a number
of different Gibbs-sampling algorithms \citep{teh2006hierarchical,
  vangael2008beam, fox2008hdp, johnson2013bayesian}.  
The key difference among the Gibbs samplers is
the treatment of the latent state sequence, $\{z_t\}$.  One
distinction is whether each $z_t$ is put in its own block and sampled conditioned on
all of the others (as in \citeauthor{teh2006hierarchical}), or whether
the full $z_t$ sequence is sampled at once, as in
\citeauthor{vangael2008beam}, \citeauthor{fox2008hdp} and
\citeauthor{johnson2013bayesian}.  I focus here on the latter case,
since the Gibbs sampler developed in Chapter \ref{chapter:HaMMLeT} for
the new HaMMLeT model is of this type.

\subsection{The Forward Backward Algorithm}
\label{sec:forw-backw-algor-1}

In a non-dynamic mixture model, the component labels are mutually
independent given the component parameters, and hence a Gibbs sampler
can trivially sample all of them simultaneously.  In the dynamic case,
however, the distribution of each state indicator depends on the
previous indicator, which depends on the previous one, etc.
Moreoever, as a result of this propagation of dependence, the data at
time $t$ is indirect evidence for the indicators not just at time $t$,
but at {\em all other} times as well.  As a result,
sampling the $\{z_t\}$ sequence jointly requires care to appropriately
account for all of these dependencies.

Because of the Markov assumption, the set $\{z_1, \dots,
z_{t-1}\}$ is conditionally independent of the set $\{z_{t+1}, \dots,
z_{T}\}$, as well as of the data $\{y_{t+1}, \dots, y_{T}\}$, given
the indicator $z_t$.

\subsection{A Gibbs Sampler for the HDP-HMM}

\subsection{Beam Sampling in the HDP-HMM}
