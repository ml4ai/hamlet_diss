\contentsline {chapter}{List of Tables}{7}
\contentsline {chapter}{List of Figures}{8}
\contentsline {chapter}{Abstract}{9}
\contentsline {chapter}{\chapterline {1} \MakeUppercase {Statistical Models and Model Selection} }{12}
\contentsline {section}{\numberline {1.1}Statistical Models}{12}
\contentsline {section}{\numberline {1.2}Model Selection}{12}
\contentsline {subsection}{\numberline {1.2.1}The Likelihood Function and the Conservation of Explanatory Power}{12}
\contentsline {subsection}{\numberline {1.2.2}The Bias-Variance Tradeoff}{14}
\contentsline {subsection}{\numberline {1.2.3}Example: Polynomial Regression}{16}
\contentsline {subsection}{\numberline {1.2.4}Balancing Fit and Complexity: Bayesian Occam's Razor}{16}
\contentsline {section}{\numberline {1.3}Clustering via a Mixture of Gaussians Model}{19}
\contentsline {section}{\numberline {1.4}Clustering Sequential Data: The Hidden Markov Model}{21}
\contentsline {section}{\numberline {1.5}Model Selection in HMMs and the Mixture of Gaussians Model}{23}
\contentsline {section}{\numberline {1.6}The Structure of This Dissertation}{23}
\contentsline {chapter}{\chapterline {2} \MakeUppercase {Bayesian Nonparametric Models and the Infinite Hidden Markov Model} }{25}
\contentsline {section}{\numberline {2.1}Parametric Vs. Nonparametric Models}{25}
\contentsline {section}{\numberline {2.2}The Dirichlet Process}{26}
\contentsline {subsection}{\numberline {2.2.1}The Normalized Gamma Process representation of the DP}{28}
\contentsline {subsection}{\numberline {2.2.2}The Stick-Breaking Process construction of the DP}{29}
\contentsline {subsection}{\numberline {2.2.3}The Chinese Restaurant Process}{29}
\contentsline {subsection}{\numberline {2.2.4}The Dirichlet Process Mixture Model}{30}
\contentsline {subsection}{\numberline {2.2.5}A Gibbs Sampler for DP Mixture Models}{31}
\contentsline {section}{\numberline {2.3}An Infinite State HMM}{32}
\contentsline {subsection}{\numberline {2.3.1}The Hierarchical Dirichlet Process}{32}
\contentsline {subsection}{\numberline {2.3.2}The HDP-HMM}{32}
\contentsline {section}{\numberline {2.4}Inference in Finite and Infinite State HMMs}{32}
\contentsline {subsection}{\numberline {2.4.1}The Forward Backward Algorithm}{32}
\contentsline {subsection}{\numberline {2.4.2}Two Gibbs Samplers for the HDP-HMM}{32}
\contentsline {subsection}{\numberline {2.4.3}Beam Sampling in the HDP-HMM}{32}
\contentsline {chapter}{\chapterline {3}\MakeUppercase {HaMMLeT: An infinite Hidden Markov Model with Local Transitions}}{33}
\contentsline {section}{\numberline {3.1}Transition Dynamics in the HDP-HMM}{34}
\contentsline {section}{\numberline {3.2}An HDP-HMM With Local Transitions}{36}
\contentsline {paragraph}{Notational Conventions}{36}
\contentsline {subsection}{\numberline {3.2.1}A Normalized Gamma Process representation of the HDP-HMM}{37}
\contentsline {subsection}{\numberline {3.2.2}Promoting ``Local'' Transitions}{38}
\contentsline {section}{\numberline {3.3}The HDP-HMM-LT as a continuous-time Markov Jump Process with ``failed'' jumps}{40}
\contentsline {subsection}{\numberline {3.3.1}An HDP-HSMM-LT modification}{43}
\contentsline {subsection}{\numberline {3.3.2}Summary}{45}
\contentsline {section}{\numberline {3.4}MCMC Inference in the ``Failed Jumps'' Representation}{45}
\contentsline {subsection}{\numberline {3.4.1}Sampling $\pi $, $\beta $, $\alpha $ and $\gamma $}{46}
\contentsline {paragraph}{Sampling $\pi $}{47}
\contentsline {paragraph}{Sampling $\beta $}{47}
\contentsline {paragraph}{Sampling $\alpha $ and $\gamma $}{49}
\contentsline {subsubsection}{Summary}{51}
\contentsline {subsection}{\numberline {3.4.2}Sampling $z$ and the auxiliary variables}{51}
\contentsline {subsection}{\numberline {3.4.3}Sampling state and emission parameters}{52}
\contentsline {section}{\numberline {3.5}Use Cases}{53}
\contentsline {chapter}{\chapterline {4}\MakeUppercase {Binary Vector States: Speaker Diarization}}{55}
\contentsline {section}{\numberline {4.1}Binary State Vectors}{55}
\contentsline {subsection}{\numberline {4.1.1}Additional Inference Steps}{56}
\contentsline {paragraph}{Sampling $\theta $}{56}
\contentsline {paragraph}{Sampling $\mu $}{58}
\contentsline {paragraph}{Sampling $W$ and $\Sigma $}{58}
\contentsline {paragraph}{Sampling $\lambda $}{59}
\contentsline {subsection}{\numberline {4.1.2}Summary}{59}
\contentsline {section}{\numberline {4.2}``Cocktail Party'' Data}{61}
\contentsline {section}{\numberline {4.3}Synthetic Data Without Local Transitions}{63}
\contentsline {chapter}{\chapterline {5}\MakeUppercase {Categorical Vector States: Power Disaggregation}}{66}
\contentsline {section}{\numberline {5.1}Generalizing to Categorical-Valued $\theta $}{66}
\contentsline {section}{\numberline {5.2}Priors and Representations in the Categorical State Variant}{67}
\contentsline {section}{\numberline {5.3}Adapting Posterior Inference for Categorical State Vectors}{68}
\contentsline {subsubsection}{Sampling $\theta $}{68}
\contentsline {paragraph}{Sampling $W$}{69}
\contentsline {paragraph}{Sampling $\lambda $}{69}
\contentsline {section}{\numberline {5.4}Power Disaggregation}{69}
\contentsline {chapter}{\chapterline {6}\MakeUppercase {Separate Similarities and Emissions: Learning Tonal Grammar in Music}}{72}
\contentsline {section}{\numberline {6.1}Separable Similarity and Emissions}{72}
\contentsline {section}{\numberline {6.2}A Hamlitonian Monte Carlo step to sample $\eta $}{72}
\contentsline {section}{\numberline {6.3}Synthetic Data from an HMM with a Nearly Block Diagonal Transition Matrix}{75}
\contentsline {section}{\numberline {6.4}Discovering Chord Equivalence Classes in Tonal Music}{76}
\contentsline {chapter}{\chapterline {7}\MakeUppercase {Conclusions and Future Work}}{77}
\contentsline {chapter}{References}{78}
