\contentsline {chapter}{List of Tables}{9}
\contentsline {chapter}{List of Figures}{10}
\contentsline {chapter}{Abstract}{11}
\contentsline {chapter}{\chapterline {1} \MakeUppercase {Statistical Models and Model Selection} }{14}
\contentsline {section}{\numberline {1.1}Introduction}{14}
\contentsline {subsection}{\numberline {1.1.1}An overview of the dissertation}{16}
\contentsline {section}{\numberline {1.2}Model Selection}{17}
\contentsline {subsection}{\numberline {1.2.1}The Likelihood Function and the Conservation of Explanatory Power}{17}
\contentsline {subsection}{\numberline {1.2.2}The Bias-Variance Tradeoff}{19}
\contentsline {subsection}{\numberline {1.2.3}Example: Polynomial Regression}{20}
\contentsline {subsection}{\numberline {1.2.4}Balancing Fit and Complexity: Bayesian Occam's Razor}{22}
\contentsline {section}{\numberline {1.3}Clustering via a Mixture of Gaussians Model}{24}
\contentsline {section}{\numberline {1.4}Clustering Sequential Data: The Hidden Markov Model}{25}
\contentsline {section}{\numberline {1.5}Model Selection in HMMs and the Mixture of Gaussians Model}{27}
\contentsline {chapter}{\chapterline {2} \MakeUppercase {Bayesian Nonparametric Models and the Infinite Hidden Markov Model} }{29}
\contentsline {section}{\numberline {2.1}Parametric Vs. Nonparametric Models}{29}
\contentsline {section}{\numberline {2.2}The Dirichlet Process}{30}
\contentsline {subsection}{\numberline {2.2.1}The Normalized Gamma Process representation of the DP}{32}
\contentsline {subsection}{\numberline {2.2.2}The Stick-Breaking Process construction of the DP}{33}
\contentsline {subsection}{\numberline {2.2.3}The Chinese Restaurant Process}{33}
\contentsline {subsection}{\numberline {2.2.4}The Dirichlet Process Mixture Model}{34}
\contentsline {subsection}{\numberline {2.2.5}A Gibbs Sampler for DP Mixture Models}{35}
\contentsline {section}{\numberline {2.3}An Infinite State HMM}{36}
\contentsline {subsection}{\numberline {2.3.1}The Hierarchical Dirichlet Process}{36}
\contentsline {subsection}{\numberline {2.3.2}The HDP-HMM}{36}
\contentsline {section}{\numberline {2.4}Inference in Finite and Infinite State HMMs}{36}
\contentsline {subsection}{\numberline {2.4.1}The Forward Backward Algorithm}{36}
\contentsline {subsection}{\numberline {2.4.2}Two Gibbs Samplers for the HDP-HMM}{36}
\contentsline {subsection}{\numberline {2.4.3}Beam Sampling in the HDP-HMM}{36}
\contentsline {chapter}{\chapterline {3}\MakeUppercase {HaMMLeT: An infinite Hidden Markov Model with Local Transitions}}{37}
\contentsline {section}{\numberline {3.1}Transition Dynamics in the HDP-HMM}{38}
\contentsline {section}{\numberline {3.2}An HDP-HMM With Local Transitions}{40}
\contentsline {paragraph}{Notational Conventions}{40}
\contentsline {subsection}{\numberline {3.2.1}A Normalized Gamma Process representation of the HDP-HMM}{41}
\contentsline {subsection}{\numberline {3.2.2}Promoting ``Local'' Transitions}{42}
\contentsline {section}{\numberline {3.3}The HDP-HMM-LT as a continuous-time Markov Jump Process with ``failed'' jumps}{44}
\contentsline {subsection}{\numberline {3.3.1}An HDP-HSMM-LT modification}{47}
\contentsline {subsection}{\numberline {3.3.2}Summary}{49}
\contentsline {section}{\numberline {3.4}MCMC Inference in the ``Failed Jumps'' Representation}{49}
\contentsline {subsection}{\numberline {3.4.1}Sampling $\pi $, $\beta $, $\alpha $ and $\gamma $}{50}
\contentsline {paragraph}{Sampling $\pi $}{51}
\contentsline {paragraph}{Sampling $\beta $}{51}
\contentsline {paragraph}{Sampling $\alpha $ and $\gamma $}{53}
\contentsline {subsubsection}{Summary}{55}
\contentsline {subsection}{\numberline {3.4.2}Sampling $z$ and the auxiliary variables}{55}
\contentsline {subsection}{\numberline {3.4.3}Sampling state and emission parameters}{56}
\contentsline {section}{\numberline {3.5}Use Cases}{57}
\contentsline {chapter}{\chapterline {4}\MakeUppercase {Binary Vector States: Speaker Diarization}}{59}
\contentsline {section}{\numberline {4.1}Binary State Vectors}{59}
\contentsline {subsection}{\numberline {4.1.1}Additional Inference Steps}{60}
\contentsline {paragraph}{Sampling $\theta $}{60}
\contentsline {paragraph}{Sampling $\mu $}{62}
\contentsline {paragraph}{Sampling $W$ and $\Sigma $}{62}
\contentsline {paragraph}{Sampling $\lambda $}{63}
\contentsline {subsection}{\numberline {4.1.2}Summary}{63}
\contentsline {section}{\numberline {4.2}``Cocktail Party'' Data}{65}
\contentsline {section}{\numberline {4.3}Synthetic Data Without Local Transitions}{67}
\contentsline {chapter}{\chapterline {5}\MakeUppercase {Categorical Vector States: Power Disaggregation}}{70}
\contentsline {section}{\numberline {5.1}Generalizing to Categorical-Valued $\theta $}{70}
\contentsline {section}{\numberline {5.2}Priors and Representations in the Categorical State Variant}{71}
\contentsline {section}{\numberline {5.3}Adapting Posterior Inference for Categorical State Vectors}{72}
\contentsline {subsubsection}{Sampling $\theta $}{72}
\contentsline {paragraph}{Sampling $W$}{73}
\contentsline {paragraph}{Sampling $\lambda $}{73}
\contentsline {section}{\numberline {5.4}Power Disaggregation}{73}
\contentsline {chapter}{\chapterline {6}\MakeUppercase {Separate Similarities and Emissions: Learning Tonal Grammar in Music}}{76}
\contentsline {section}{\numberline {6.1}Separable Similarity and Emissions}{76}
\contentsline {section}{\numberline {6.2}A Hamlitonian Monte Carlo step to sample $\eta $}{76}
\contentsline {section}{\numberline {6.3}Synthetic Data from an HMM with a Nearly Block Diagonal Transition Matrix}{79}
\contentsline {section}{\numberline {6.4}Discovering Chord Equivalence Classes in Tonal Music}{80}
\contentsline {chapter}{\chapterline {7}\MakeUppercase {Conclusions and Future Work}}{81}
\contentsline {chapter}{References}{82}
